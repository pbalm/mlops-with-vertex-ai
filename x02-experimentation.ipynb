{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae86f7",
   "metadata": {},
   "source": [
    "# 02 - ML Experimentation with Custom Model\n",
    "\n",
    "The purpose of this notebook is to use [custom training](https://cloud.google.com/ai-platform-unified/docs/training/custom-training) to train a keras classifier to predict whether a given trip will result in a tip > 20%. The notebook covers the following tasks:\n",
    "1. Preprocess the data locally using Apache Beam.\n",
    "2. Train and test custom model locally using a Keras implementation.\n",
    "3. Submit a Dataflow job to preprocess the data at scale.\n",
    "4. Submit a custom training job to Vertex AI using a [pre-built container](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "5. Upload the trained model to Vertex AI.\n",
    "6. Track experiment parameters from [Vertex AI Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction).\n",
    "7. Submit a [hyperparameter tuning job](https://cloud.google.com/vertex-ai/docs/training/hyperparameter-tuning-overview) to Vertex AI.\n",
    "\n",
    "We use [Vertex TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) \n",
    "and [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) to  track, visualize, and compare ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5205917",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c6cfe",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4171d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.8.1\n",
      "TensorFlow Transform: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hp_tuning\n",
    "\n",
    "from src.common import features, datasource_utils\n",
    "from src.model_training import data, model, defaults, trainer, exporter\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde361cf",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fce242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: pbalm-cxb-aa\n",
      "Region: europe-west4\n",
      "Bucket name: pbalm-cxb-aa-eu\n",
      "Service Account: 188940921537-compute@developer.gserviceaccount.com\n",
      "Vertex API Parent URI: projects/pbalm-cxb-aa/locations/europe-west4\n"
     ]
    }
   ],
   "source": [
    "#PROJECT = 'cxb1-prj-test-no-vpcsc'\n",
    "#REGION = 'europe-west1'\n",
    "\n",
    "PROJECT = 'pbalm-cxb-aa'\n",
    "REGION = 'europe-west4'\n",
    "\n",
    "BUCKET = PROJECT + '-eu'\n",
    "\n",
    "#SERVICE_ACCOUNT = \"1031952735253-compute@developer.gserviceaccount.com\" # GCE default SA\n",
    "#SERVICE_ACCOUNT = \"sa-mlops@cxb1-prj-test-no-vpcsc.iam.gserviceaccount.com\"\n",
    "\n",
    "SERVICE_ACCOUNT = \"\" # This will trigger use of user account\n",
    "\n",
    "\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "PARENT = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "print(\"Vertex API Parent URI:\", PARENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfd33f",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b363e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v01'\n",
    "VERTEX_DATASET_NAME = 'creditcards'\n",
    "\n",
    "MODEL_DISPLAY_NAME = f'{VERTEX_DATASET_NAME}-classifier-{VERSION}'\n",
    "\n",
    "WORKSPACE = f'gs://{BUCKET}/{VERTEX_DATASET_NAME}'\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "RAW_SCHEMA_LOCATION = 'src/raw_schema/schema.pbtxt'\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{VERTEX_DATASET_NAME}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}'\n",
    "DATAFLOW_REGION = f'{REGION}'\n",
    "DATAFLOW_SERVICE_ACCOUNT = SERVICE_ACCOUNT\n",
    "#DATAFLOW_SUBNETWORK = 'https://www.googleapis.com/compute/v1/projects/cxb1-prj-test-no-vpcsc/regions/europe-west1/subnetworks/default'\n",
    "DATAFLOW_SUBNETWORK = 'https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default'\n",
    "\n",
    "if not REGION in DATAFLOW_SUBNETWORK:\n",
    "    raise Exception(f'{DATAFLOW_SUBNETWORK} is not a valid subnet for {REGION}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf63d9",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf8ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/188940921537/locations/europe-west4/tensorboards/5807145428889632768/operations/79341308816130048\n",
      "Tensorboard created. Resource name: projects/188940921537/locations/europe-west4/tensorboards/5807145428889632768\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/188940921537/locations/europe-west4/tensorboards/5807145428889632768')\n",
      "TensorBoard resource name: projects/188940921537/locations/europe-west4/tensorboards/5807145428889632768\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(location=REGION)\n",
    "tensorboard_resource = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "tensorboard_resource_name = tensorboard_resource.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a3859",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d03fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace is ready.\n",
      "Experiment directory: gs://pbalm-cxb-aa-eu/creditcards/experiments\n"
     ]
    }
   ],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")\n",
    "print(\"Experiment directory:\", EXPERIMENT_ARTIFACTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08e503",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3aefc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Resource creditcards-classifier-v01-run-local-20220620093413 not found.\n",
      "INFO:root:Creating Resource creditcards-classifier-v01-run-local-20220620093413\n",
      "INFO:root:Resource creditcards-classifier-v01-run-local-20220620093413-metrics not found.\n",
      "INFO:root:Creating Resource creditcards-classifier-v01-run-local-20220620093413-metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment run directory: gs://pbalm-cxb-aa-eu/creditcards/experiments/creditcards-classifier-v01/run-local-20220620093413\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8cee2",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data using Apache Beam\n",
    "\n",
    "The Apache Beam pipeline of data preprocessing is implemented in the [preprocessing](src/preprocessing) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a613a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b81f3",
   "metadata": {},
   "source": [
    "### Get Source Query from Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_USE = 'UNASSIGNED'\n",
    "LIMIT = 5120\n",
    "\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=VERTEX_DATASET_NAME, \n",
    "    ml_use=ML_USE, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047eac1",
   "metadata": {},
   "source": [
    "### Test Data Preprocessing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': f'gs://{BUCKET}/bq_tmp',\n",
    "    'project': PROJECT,\n",
    "    'region': REGION\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c225f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ece730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10911e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d252ab",
   "metadata": {},
   "source": [
    "## 2. Train a custom model locally using a Keras\n",
    "\n",
    "The `Keras` implementation of the custom model is in the [model_training](src/model_training) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a212f14",
   "metadata": {},
   "source": [
    "### Read transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918a86e-4210-4919-b79b-22b997a9fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'train/data-*.gz')\n",
    "eval_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'eval/data-*.gz')\n",
    "\n",
    "train_data_file_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_features, target in data.get_dataset(train_data_file_pattern, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f363d4",
   "metadata": {},
   "source": [
    "### Create hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fced82",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"hidden_units\": [64, 32]\n",
    "}\n",
    "\n",
    "hyperparams = defaults.update_hyperparams(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c17b65e",
   "metadata": {},
   "source": [
    "### Create and test model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model.create_model(transform_feature_spec.keys(), hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    classifier, \n",
    "    show_shapes=True, \n",
    "    show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22116478-ed67-486c-b6d0-e1f98745c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d451ef7",
   "metadata": {},
   "source": [
    "### Train the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153efc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams[\"learning_rate\"] = 0.001\n",
    "hyperparams[\"num_epochs\"] = 5\n",
    "hyperparams[\"batch_size\"] = 512\n",
    "\n",
    "vertex_ai.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = trainer.train(\n",
    "    train_data_dir=train_data_file_pattern,\n",
    "    eval_data_dir=eval_data_file_pattern,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbce8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = trainer.evaluate(\n",
    "    model=classifier,\n",
    "    data_dir=eval_data_file_pattern,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_metrics(\n",
    "    {\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tb-gcp-uploader --tensorboard_resource_name={tensorboard_resource_name} \\\n",
    "  --logdir={LOG_DIR} \\\n",
    "  --experiment_name={EXPERIMENT_NAME} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44268d04",
   "metadata": {},
   "source": [
    "### Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "exporter.export_serving_model(\n",
    "    classifier=classifier,\n",
    "    serving_model_dir=saved_model_dir,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1ba3e",
   "metadata": {},
   "source": [
    "### Inspect model serving signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8989c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68ecd0",
   "metadata": {},
   "source": [
    "### Test the exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)\n",
    "print(\"Saved model is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serving_tf_example with TF Examples\n",
    "\n",
    "file_names = tf.data.TFRecordDataset.list_files(EXPORTED_DATA_PREFIX + '/data-*.tfrecord')\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures['serving_tf_example'](batch)\n",
    "    for key in predictions:\n",
    "        print(f\"{key}: {predictions[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f041d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the serving_default with feature dictionary\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3ee32-6941-4428-bfb4-3a5c611e79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = { 'Amount': 50.0 }\n",
    "\n",
    "for i in range(1,29):\n",
    "    instance[f'V{i}'] = 1\n",
    "    \n",
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cec2080",
   "metadata": {},
   "source": [
    "## Start a new Vertex AI experiment run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d87d0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment run directory: gs://pbalm-cxb-aa-eu/creditcards/experiments/creditcards-classifier-v01/run-gcp-20220620121956\n"
     ]
    }
   ],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME)\n",
    "\n",
    "run_id = f\"run-gcp-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da802846-d5ff-4c0f-8faa-5eef3d680c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pbalm-cxb-aa\n",
      "pbalm-cxb-aa-eu\n",
      "creditcards-classifier-v01\n"
     ]
    }
   ],
   "source": [
    "print(f'{PROJECT}')\n",
    "print(f'{BUCKET}')\n",
    "print(f'{EXPERIMENT_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade27b7",
   "metadata": {},
   "source": [
    "## 3. Submit a Data Processing Job to Dataflow\n",
    "\n",
    "To rebuild the custom container:\n",
    "\n",
    "`gcloud builds submit . --tag europe-west4-docker.pkg.dev/pbalm-cxb-aa/dataflow/creditcards:latest --timeout=30m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "174b1fb2-4c5b-46dc-a5e6-fc2719a7ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 68 file(s) totalling 1.4 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://pbalm-cxb-aa_cloudbuild/source/1655742309.221514-740f4453db5947e3acb7e4ebe281d700.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/pbalm-cxb-aa/locations/global/builds/82a1b8c8-9319-4651-8a48-17534efcab3b].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/82a1b8c8-9319-4651-8a48-17534efcab3b?project=188940921537].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"82a1b8c8-9319-4651-8a48-17534efcab3b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://pbalm-cxb-aa_cloudbuild/source/1655742309.221514-740f4453db5947e3acb7e4ebe281d700.tgz#1655742310526107\n",
      "Copying gs://pbalm-cxb-aa_cloudbuild/source/1655742309.221514-740f4453db5947e3acb7e4ebe281d700.tgz#1655742310526107...\n",
      "/ [1 files][265.1 KiB/265.1 KiB]                                                \n",
      "Operation completed over 1 objects/265.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  1.501MB\n",
      "Step 1/5 : FROM apache/beam_python3.7_sdk:2.39.0\n",
      "2.39.0: Pulling from apache/beam_python3.7_sdk\n",
      "67e8aa6c8bbc: Pulling fs layer\n",
      "627e6c1e1055: Pulling fs layer\n",
      "0670968926f6: Pulling fs layer\n",
      "5a8b0e20be4b: Pulling fs layer\n",
      "b0b10a3a2784: Pulling fs layer\n",
      "e16cd24209e8: Pulling fs layer\n",
      "3d88b16bb65a: Pulling fs layer\n",
      "84d1e3bb48af: Pulling fs layer\n",
      "90b15c3c4eec: Pulling fs layer\n",
      "5a8b0e20be4b: Waiting\n",
      "2e0503779a57: Pulling fs layer\n",
      "b0b10a3a2784: Waiting\n",
      "9f9c11484bb1: Pulling fs layer\n",
      "2df57cb54e4d: Pulling fs layer\n",
      "4bd93539abda: Pulling fs layer\n",
      "79c59d6f0377: Pulling fs layer\n",
      "d2aabfdcaac0: Pulling fs layer\n",
      "9b57857091c3: Pulling fs layer\n",
      "8b9888fec7f1: Pulling fs layer\n",
      "feb65321cdf4: Pulling fs layer\n",
      "e0786fd257d1: Pulling fs layer\n",
      "1904d44a9360: Pulling fs layer\n",
      "e16cd24209e8: Waiting\n",
      "e3f57d0327cf: Pulling fs layer\n",
      "e204a3932b86: Pulling fs layer\n",
      "3d88b16bb65a: Waiting\n",
      "b1bb4d4f1d7b: Pulling fs layer\n",
      "84d1e3bb48af: Waiting\n",
      "248d84f0406d: Pulling fs layer\n",
      "90b15c3c4eec: Waiting\n",
      "2df57cb54e4d: Waiting\n",
      "4bd93539abda: Waiting\n",
      "79c59d6f0377: Waiting\n",
      "d2aabfdcaac0: Waiting\n",
      "9b57857091c3: Waiting\n",
      "8b9888fec7f1: Waiting\n",
      "e204a3932b86: Waiting\n",
      "feb65321cdf4: Waiting\n",
      "b1bb4d4f1d7b: Waiting\n",
      "2e0503779a57: Waiting\n",
      "248d84f0406d: Waiting\n",
      "e0786fd257d1: Waiting\n",
      "1904d44a9360: Waiting\n",
      "9f9c11484bb1: Waiting\n",
      "e3f57d0327cf: Waiting\n",
      "627e6c1e1055: Download complete\n",
      "0670968926f6: Download complete\n",
      "67e8aa6c8bbc: Verifying Checksum\n",
      "67e8aa6c8bbc: Download complete\n",
      "e16cd24209e8: Download complete\n",
      "5a8b0e20be4b: Verifying Checksum\n",
      "5a8b0e20be4b: Download complete\n",
      "3d88b16bb65a: Download complete\n",
      "84d1e3bb48af: Verifying Checksum\n",
      "84d1e3bb48af: Download complete\n",
      "2e0503779a57: Download complete\n",
      "90b15c3c4eec: Verifying Checksum\n",
      "90b15c3c4eec: Download complete\n",
      "9f9c11484bb1: Verifying Checksum\n",
      "9f9c11484bb1: Download complete\n",
      "67e8aa6c8bbc: Pull complete\n",
      "627e6c1e1055: Pull complete\n",
      "0670968926f6: Pull complete\n",
      "b0b10a3a2784: Verifying Checksum\n",
      "b0b10a3a2784: Download complete\n",
      "79c59d6f0377: Verifying Checksum\n",
      "79c59d6f0377: Download complete\n",
      "d2aabfdcaac0: Verifying Checksum\n",
      "d2aabfdcaac0: Download complete\n",
      "9b57857091c3: Verifying Checksum\n",
      "9b57857091c3: Download complete\n",
      "8b9888fec7f1: Verifying Checksum\n",
      "8b9888fec7f1: Download complete\n",
      "4bd93539abda: Verifying Checksum\n",
      "4bd93539abda: Download complete\n",
      "feb65321cdf4: Verifying Checksum\n",
      "feb65321cdf4: Download complete\n",
      "e0786fd257d1: Download complete\n",
      "1904d44a9360: Verifying Checksum\n",
      "1904d44a9360: Download complete\n",
      "5a8b0e20be4b: Pull complete\n",
      "e3f57d0327cf: Verifying Checksum\n",
      "e3f57d0327cf: Download complete\n",
      "e204a3932b86: Verifying Checksum\n",
      "e204a3932b86: Download complete\n",
      "b1bb4d4f1d7b: Verifying Checksum\n",
      "b1bb4d4f1d7b: Download complete\n",
      "248d84f0406d: Verifying Checksum\n",
      "248d84f0406d: Download complete\n",
      "b0b10a3a2784: Pull complete\n",
      "e16cd24209e8: Pull complete\n",
      "3d88b16bb65a: Pull complete\n",
      "84d1e3bb48af: Pull complete\n",
      "90b15c3c4eec: Pull complete\n",
      "2e0503779a57: Pull complete\n",
      "9f9c11484bb1: Pull complete\n",
      "2df57cb54e4d: Verifying Checksum\n",
      "2df57cb54e4d: Download complete\n",
      "2df57cb54e4d: Pull complete\n",
      "4bd93539abda: Pull complete\n",
      "79c59d6f0377: Pull complete\n",
      "d2aabfdcaac0: Pull complete\n",
      "9b57857091c3: Pull complete\n",
      "8b9888fec7f1: Pull complete\n",
      "feb65321cdf4: Pull complete\n",
      "e0786fd257d1: Pull complete\n",
      "1904d44a9360: Pull complete\n",
      "e3f57d0327cf: Pull complete\n",
      "e204a3932b86: Pull complete\n",
      "b1bb4d4f1d7b: Pull complete\n",
      "248d84f0406d: Pull complete\n",
      "Digest: sha256:05cd9456928258316847d3535c2d42a681f0ee7af2c1e9d6e663929c3a7f8c56\n",
      "Status: Downloaded newer image for apache/beam_python3.7_sdk:2.39.0\n",
      " ---> 28a2c07c5ac2\n",
      "Step 2/5 : COPY requirements.txt requirements.txt\n",
      " ---> 85da0260efdc\n",
      "Step 3/5 : RUN pip install -r requirements.txt\n",
      " ---> Running in 956e81e67a20\n",
      "Collecting kfp==1.8.12\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.2/301.2 KB 8.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-cloud-bigquery==2.34.3\n",
      "  Downloading google_cloud_bigquery-2.34.3-py2.py3-none-any.whl (206 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.6/206.6 KB 27.4 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery-storage==2.13.2\n",
      "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180.2/180.2 KB 4.8 MB/s eta 0:00:00\n",
      "Collecting google-cloud-aiplatform==1.14.0\n",
      "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 37.0 MB/s eta 0:00:00\n",
      "Collecting cloudml-hypertune==0.1.0.dev6\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pytest==7.1.2\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 KB 33.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow==2.8.2\n",
      "  Downloading tensorflow-2.8.2-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 497.9/497.9 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-data-validation==1.8.0\n",
      "  Downloading tensorflow_data_validation-1.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 68.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-transform==1.8.0\n",
      "  Downloading tensorflow_transform-1.8.0-py3-none-any.whl (435 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 435.3/435.3 KB 41.5 MB/s eta 0:00:00\n",
      "Collecting tfx==1.8.0\n",
      "  Downloading tfx-1.8.0-py3-none-any.whl (2.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 80.6 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io==0.26.0\n",
      "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/25.9 MB 74.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.0.0)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 636.6/636.6 KB 51.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.31.5)\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 KB 15.5 MB/s eta 0:00:00\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 77.8 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 KB 10.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 KB 8.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.0/58.0 KB 10.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 KB 10.4 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (8.0.4)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 12.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (3.19.4)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/11.1 MB 104.6 MB/s eta 0:00:00\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.44.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.20.3)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.5.1-py2.py3-none-any.whl (230 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 230.2/230.2 KB 27.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 6)) (1.11.0)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 6)) (0.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 6)) (4.11.3)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 6)) (21.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (13.0.0)\n",
      "Collecting tensorflow-estimator<2.9,>=2.8\n",
      "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 462.3/462.3 KB 43.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (2.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (0.24.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (57.5.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (1.21.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.8.2->-r requirements.txt (line 7)) (2.8.0)\n",
      "Collecting tensorflow-metadata<1.9,>=1.8.0\n",
      "  Downloading tensorflow_metadata-1.8.0-py3-none-any.whl (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 KB 7.0 MB/s eta 0:00:00\n",
      "Collecting joblib<0.15,>=0.12\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 KB 29.4 MB/s eta 0:00:00\n",
      "Collecting pyarrow<6,>=1\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.6/23.6 MB 81.9 MB/s eta 0:00:00\n",
      "Collecting pyfarmhash<0.4,>=0.2\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.9/99.9 KB 15.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tfx-bsl<1.9,>=1.8.0\n",
      "  Downloading tfx_bsl-1.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 85.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: apache-beam[gcp]<3,>=2.38 in /usr/local/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (2.39.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.3.5)\n",
      "Requirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.7/site-packages (from tensorflow-transform==1.8.0->-r requirements.txt (line 9)) (1.4.2)\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 74.4 MB/s eta 0:00:00\n",
      "Collecting packaging>=14.3\n",
      "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 KB 6.3 MB/s eta 0:00:00\n",
      "Collecting keras-tuner<2,>=1.0.4\n",
      "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 KB 18.8 MB/s eta 0:00:00\n",
      "Collecting tensorflow-model-analysis<0.40,>=0.39.0\n",
      "  Downloading tensorflow_model_analysis-0.39.0-py3-none-any.whl (1.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 79.2 MB/s eta 0:00:00\n",
      "Collecting ml-metadata<1.9.0,>=1.8.0\n",
      "  Downloading ml_metadata-1.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 96.3 MB/s eta 0:00:00\n",
      "Collecting ml-pipelines-sdk==1.8.0\n",
      "  Downloading ml_pipelines_sdk-1.8.0-py3-none-any.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 72.5 MB/s eta 0:00:00\n",
      "Collecting docker<5,>=4.1\n",
      "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.0/147.0 KB 21.9 MB/s eta 0:00:00\n",
      "Collecting tensorflow-hub<0.13,>=0.9.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.8/108.8 KB 15.3 MB/s eta 0:00:00\n",
      "Collecting click<9,>=7.1.2\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.8/82.8 KB 13.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-apitools<1,>=0.5 in /usr/local/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 10)) (0.5.31)\n",
      "Collecting jinja2<4,>=2.7.3\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 18.9 MB/s eta 0:00:00\n",
      "Collecting portpicker<2,>=1.3.1\n",
      "  Downloading portpicker-1.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting attrs>=19.2.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.3/49.3 KB 6.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.9.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 86.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (3.12.3)\n",
      "Requirement already satisfied: orjson<4.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (3.6.7)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.7)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (2.6.0)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (0.19.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (2021.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (0.3.1.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.4.10)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.19.1)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (3.6.2)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.4.1)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.16.1)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.15.3)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (2.11.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (0.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.8.2->-r requirements.txt (line 7)) (0.37.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/site-packages (from docker<5,>=4.1->tfx==1.8.0->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12->-r requirements.txt (line 1)) (1.56.0)\n",
      "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 10)) (0.17.3)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 10)) (4.1.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (0.2.8)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow==2.8.2->-r requirements.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest==7.1.2->-r requirements.txt (line 6)) (3.7.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.18.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.1/117.1 KB 15.4 MB/s eta 0:00:00\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Collecting ipython\n",
      "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 793.8/793.8 KB 52.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (1.26.9)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.4.7)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 281.4/281.4 KB 28.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2->-r requirements.txt (line 7)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2->-r requirements.txt (line 7)) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2->-r requirements.txt (line 7)) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2->-r requirements.txt (line 7)) (0.6.1)\n",
      "Collecting ipywidgets<8,>=7\n",
      "  Downloading ipywidgets-7.7.0-py2.py3-none-any.whl (123 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 KB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy<2,>=1.4.1 in /usr/local/lib/python3.7/site-packages (from tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 10)) (1.7.3)\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.8.2-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: grpcio-status>=1.16.0 in /usr/local/lib/python3.7/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (1.44.0)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /usr/local/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (0.6.2)\n",
      "Collecting pexpect>4.3\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.0/59.0 KB 9.3 MB/s eta 0:00:00\n",
      "Collecting backcall\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 KB 16.7 MB/s eta 0:00:00\n",
      "Collecting matplotlib-inline\n",
      "  Downloading matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\n",
      "Collecting jedi>=0.16\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 78.1 MB/s eta 0:00:00\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.12.0-py3-none-any.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 63.3 MB/s eta 0:00:00\n",
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 381.5/381.5 KB 32.1 MB/s eta 0:00:00\n",
      "Collecting ipython-genutils~=0.2.0\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting ipykernel>=4.5.1\n",
      "  Downloading ipykernel-6.15.0-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 19.6 MB/s eta 0:00:00\n",
      "Collecting nbformat>=4.2.0\n",
      "  Downloading nbformat-5.4.0-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 KB 10.3 MB/s eta 0:00:00\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.1.0-py3-none-any.whl (245 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.1/245.1 KB 30.7 MB/s eta 0:00:00\n",
      "Collecting widgetsnbextension~=3.6.0\n",
      "  Downloading widgetsnbextension-3.6.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 85.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/site-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 10)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (3.2.0)\n",
      "Collecting pyzmq>=17\n",
      "  Downloading pyzmq-23.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 62.2 MB/s eta 0:00:00\n",
      "Collecting nest-asyncio\n",
      "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
      "Collecting jupyter-client>=6.1.12\n",
      "  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.1/132.1 KB 22.1 MB/s eta 0:00:00\n",
      "Collecting debugpy>=1.0\n",
      "  Downloading debugpy-1.6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 86.2 MB/s eta 0:00:00\n",
      "Collecting tornado>=6.1\n",
      "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 428.5/428.5 KB 45.2 MB/s eta 0:00:00\n",
      "Collecting parso<0.9.0,>=0.8.0\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.8/100.8 KB 17.2 MB/s eta 0:00:00\n",
      "Collecting jupyter-core\n",
      "  Downloading jupyter_core-4.10.0-py3-none-any.whl (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.3/87.3 KB 13.6 MB/s eta 0:00:00\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.15.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /usr/local/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation==1.8.0->-r requirements.txt (line 8)) (0.1.0)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 10)) (0.2.5)\n",
      "Collecting notebook>=4.4.1\n",
      "  Downloading notebook-6.4.12-py3-none-any.whl (9.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 103.0 MB/s eta 0:00:00\n",
      "Collecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting nbconvert>=5\n",
      "  Downloading nbconvert-6.5.0-py3-none-any.whl (561 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 561.6/561.6 KB 43.2 MB/s eta 0:00:00\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.15.0-py3-none-any.whl (16 kB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 KB 10.2 MB/s eta 0:00:00\n",
      "Collecting bleach\n",
      "  Downloading bleach-5.0.0-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.3/160.3 KB 23.4 MB/s eta 0:00:00\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting tinycss2\n",
      "  Downloading tinycss2-1.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.6.4-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.8/71.8 KB 12.3 MB/s eta 0:00:00\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 10)) (4.10.0)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 KB 12.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 10)) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 10)) (2.3.1)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 10)) (2.21)\n",
      "Building wheels for collected packages: kfp, cloudml-hypertune, fire, kfp-server-api, pyfarmhash, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=f40a21c7000218345a166b1630343d102109066c75eb6c77998e0ff1cce8a796\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=44744341f7ed40cb934d95cb603205844e8d10cbe144ae808a6af1756cff160c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=46ff229efbfde8e2ac5bcb6396b474b7d3343bddca6dcf2f938819a38bc606d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=d3eeb863a131a0d5c9383f617d97452e3ea3dbad518f4253a905b02145f389e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "  Building wheel for pyfarmhash (setup.py): started\n",
      "  Building wheel for pyfarmhash (setup.py): finished with status 'done'\n",
      "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp37-cp37m-linux_x86_64.whl size=93059 sha256=d01bb06c65e9d5e7eccb5736879c200a89c2e0aedddbbaeef1e5a5ea07051cef\n",
      "  Stored in directory: /root/.cache/pip/wheels/53/58/7a/3b040f3a2ee31908e3be916e32660db6db53621ce6eba838dc\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=b3db5e1d187a45d434e764876c5b8d8568a174c0fc981bfb13362284a1c6e79b\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp cloudml-hypertune fire kfp-server-api pyfarmhash strip-hints\n",
      "Installing collected packages: webencodings, typing-extensions, tensorflow-estimator, tabulate, Send2Trash, pyfarmhash, ptyprocess, pickleshare, mistune, kt-legacy, joblib, ipython-genutils, iniconfig, fastjsonschema, cloudml-hypertune, backcall, uritemplate, traitlets, tornado, tomli, tinycss2, tensorflow-io-gcs-filesystem, tensorflow-hub, strip-hints, pyzmq, PyYAML, pyrsistent, pygments, pydantic, pyarrow, psutil, prompt-toolkit, prometheus-client, pexpect, parso, pandocfilters, packaging, nest-asyncio, MarkupSafe, kfp-pipeline-spec, jupyterlab-widgets, jupyterlab-pygments, fire, entrypoints, docstring-parser, Deprecated, defusedxml, decorator, debugpy, click, bleach, attrs, typer, terminado, tensorflow-metadata, tensorflow-io, requests-toolbelt, portpicker, ml-metadata, matplotlib-inline, kfp-server-api, jupyter-core, jsonschema, jinja2, jedi, docker, argon2-cffi-bindings, pytest, nbformat, kubernetes, jupyter-client, ipython, grpc-google-iam-v1, argon2-cffi, nbclient, ipykernel, google-api-python-client, tensorflow, nbconvert, ml-pipelines-sdk, keras-tuner, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery-storage, google-cloud-bigquery, tensorflow-serving-api, notebook, kfp, google-cloud-aiplatform, widgetsnbextension, tfx-bsl, ipywidgets, tensorflow-transform, tensorflow-model-analysis, tensorflow-data-validation, tfx\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.1.1\n",
      "    Uninstalling typing_extensions-4.1.1:\n",
      "      Successfully uninstalled typing_extensions-4.1.1\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: uritemplate\n",
      "    Found existing installation: uritemplate 4.1.1\n",
      "    Uninstalling uritemplate-4.1.1:\n",
      "      Successfully uninstalled uritemplate-4.1.1\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.24.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.24.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.24.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 6.0.1\n",
      "    Uninstalling pyarrow-6.0.1:\n",
      "      Successfully uninstalled pyarrow-6.0.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Uninstalling click-8.0.4:\n",
      "      Successfully uninstalled click-8.0.4\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "  Attempting uninstall: docker\n",
      "    Found existing installation: docker 5.0.3\n",
      "    Uninstalling docker-5.0.3:\n",
      "      Successfully uninstalled docker-5.0.3\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 4.6.11\n",
      "    Uninstalling pytest-4.6.11:\n",
      "      Successfully uninstalled pytest-4.6.11\n",
      "  Attempting uninstall: grpc-google-iam-v1\n",
      "    Found existing installation: grpc-google-iam-v1 0.12.3\n",
      "    Uninstalling grpc-google-iam-v1-0.12.3:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.12.3\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.41.0\n",
      "    Uninstalling google-api-python-client-2.41.0:\n",
      "      Successfully uninstalled google-api-python-client-2.41.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.8.0\n",
      "    Uninstalling tensorflow-2.8.0:\n",
      "      Successfully uninstalled tensorflow-2.8.0\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.13.0\n",
      "    Uninstalling google-cloud-bigquery-storage-2.13.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.13.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.34.2\n",
      "    Uninstalling google-cloud-bigquery-2.34.2:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.34.2\n",
      "Successfully installed Deprecated-1.2.13 MarkupSafe-2.1.1 PyYAML-5.4.1 Send2Trash-1.8.0 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 attrs-20.3.0 backcall-0.2.0 bleach-5.0.0 click-7.1.2 cloudml-hypertune-0.1.0.dev6 debugpy-1.6.0 decorator-5.1.1 defusedxml-0.7.1 docker-4.4.4 docstring-parser-0.14.1 entrypoints-0.4 fastjsonschema-2.15.3 fire-0.4.0 google-api-python-client-1.12.11 google-cloud-aiplatform-1.14.0 google-cloud-bigquery-2.34.3 google-cloud-bigquery-storage-2.13.2 google-cloud-resource-manager-1.5.1 google-cloud-storage-1.44.0 grpc-google-iam-v1-0.12.4 iniconfig-1.1.1 ipykernel-6.15.0 ipython-7.34.0 ipython-genutils-0.2.0 ipywidgets-7.7.0 jedi-0.18.1 jinja2-3.1.2 joblib-0.14.1 jsonschema-3.2.0 jupyter-client-7.3.4 jupyter-core-4.10.0 jupyterlab-pygments-0.2.2 jupyterlab-widgets-1.1.0 keras-tuner-1.1.2 kfp-1.8.12 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.2 kt-legacy-1.0.4 kubernetes-12.0.1 matplotlib-inline-0.1.3 mistune-0.8.4 ml-metadata-1.8.0 ml-pipelines-sdk-1.8.0 nbclient-0.6.4 nbconvert-6.5.0 nbformat-5.4.0 nest-asyncio-1.5.5 notebook-6.4.12 packaging-20.9 pandocfilters-1.5.0 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 portpicker-1.5.2 prometheus-client-0.14.1 prompt-toolkit-3.0.29 psutil-5.9.1 ptyprocess-0.7.0 pyarrow-5.0.0 pydantic-1.9.1 pyfarmhash-0.3.2 pygments-2.12.0 pyrsistent-0.18.1 pytest-7.1.2 pyzmq-23.2.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.9 tensorflow-2.8.2 tensorflow-data-validation-1.8.0 tensorflow-estimator-2.8.0 tensorflow-hub-0.12.0 tensorflow-io-0.26.0 tensorflow-io-gcs-filesystem-0.26.0 tensorflow-metadata-1.8.0 tensorflow-model-analysis-0.39.0 tensorflow-serving-api-2.8.2 tensorflow-transform-1.8.0 terminado-0.15.0 tfx-1.8.0 tfx-bsl-1.8.0 tinycss2-1.1.1 tomli-2.0.1 tornado-6.1 traitlets-5.3.0 typer-0.4.1 typing-extensions-3.10.0.2 uritemplate-3.0.1 webencodings-0.5.1 widgetsnbextension-3.6.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 956e81e67a20\n",
      " ---> f9d678cc2b20\n",
      "Step 4/5 : COPY src/raw_schema/schema.pbtxt raw_schema/\n",
      " ---> 88e702796703\n",
      "Step 5/5 : COPY src/ src/\n",
      " ---> 0dd4b217ade6\n",
      "Successfully built 0dd4b217ade6\n",
      "Successfully tagged europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/dataflow:latest\n",
      "PUSH\n",
      "Pushing europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/dataflow:latest\n",
      "The push refers to repository [europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/dataflow]\n",
      "6bb0864a2f47: Preparing\n",
      "726722db64e1: Preparing\n",
      "eeaba6f3673e: Preparing\n",
      "248ef6c17c61: Preparing\n",
      "5576fa104899: Preparing\n",
      "aac977877286: Preparing\n",
      "8916d7e373aa: Preparing\n",
      "b633383b8cbd: Preparing\n",
      "bc8b0343917a: Preparing\n",
      "b4f679ce27a6: Preparing\n",
      "e5bdb549ab2a: Preparing\n",
      "1b5dc3f18d1b: Preparing\n",
      "ec8826c456e8: Preparing\n",
      "606400ec6569: Preparing\n",
      "13818eb3c7f9: Preparing\n",
      "a536757dd580: Preparing\n",
      "0ec1a320476f: Preparing\n",
      "59de27832196: Preparing\n",
      "8bfd580decfc: Preparing\n",
      "d2f91fc4c31e: Preparing\n",
      "3f5d38b4936d: Preparing\n",
      "7be8268e2fb0: Preparing\n",
      "b889a93a79dd: Preparing\n",
      "9d4550089a93: Preparing\n",
      "a7934564e6b9: Preparing\n",
      "1b7cceb6a07c: Preparing\n",
      "b274e8788e0c: Preparing\n",
      "78658088978a: Preparing\n",
      "13818eb3c7f9: Waiting\n",
      "a536757dd580: Waiting\n",
      "0ec1a320476f: Waiting\n",
      "59de27832196: Waiting\n",
      "8bfd580decfc: Waiting\n",
      "d2f91fc4c31e: Waiting\n",
      "3f5d38b4936d: Waiting\n",
      "7be8268e2fb0: Waiting\n",
      "b889a93a79dd: Waiting\n",
      "9d4550089a93: Waiting\n",
      "a7934564e6b9: Waiting\n",
      "1b7cceb6a07c: Waiting\n",
      "b274e8788e0c: Waiting\n",
      "78658088978a: Waiting\n",
      "b4f679ce27a6: Waiting\n",
      "e5bdb549ab2a: Waiting\n",
      "1b5dc3f18d1b: Waiting\n",
      "606400ec6569: Waiting\n",
      "ec8826c456e8: Waiting\n",
      "aac977877286: Waiting\n",
      "8916d7e373aa: Waiting\n",
      "b633383b8cbd: Waiting\n",
      "bc8b0343917a: Waiting\n",
      "5576fa104899: Layer already exists\n",
      "aac977877286: Layer already exists\n",
      "8916d7e373aa: Layer already exists\n",
      "b633383b8cbd: Layer already exists\n",
      "bc8b0343917a: Layer already exists\n",
      "726722db64e1: Pushed\n",
      "b4f679ce27a6: Layer already exists\n",
      "6bb0864a2f47: Pushed\n",
      "e5bdb549ab2a: Layer already exists\n",
      "1b5dc3f18d1b: Layer already exists\n",
      "ec8826c456e8: Layer already exists\n",
      "606400ec6569: Layer already exists\n",
      "13818eb3c7f9: Layer already exists\n",
      "a536757dd580: Layer already exists\n",
      "0ec1a320476f: Layer already exists\n",
      "248ef6c17c61: Pushed\n",
      "59de27832196: Layer already exists\n",
      "8bfd580decfc: Layer already exists\n",
      "d2f91fc4c31e: Layer already exists\n",
      "3f5d38b4936d: Layer already exists\n",
      "7be8268e2fb0: Layer already exists\n",
      "b889a93a79dd: Layer already exists\n",
      "9d4550089a93: Layer already exists\n",
      "a7934564e6b9: Layer already exists\n",
      "1b7cceb6a07c: Layer already exists\n",
      "b274e8788e0c: Layer already exists\n",
      "78658088978a: Layer already exists\n",
      "eeaba6f3673e: Pushed\n",
      "latest: digest: sha256:5adbfabff5e25515efbecfc1e5c8a270fdcb00aca9d72f05eeb94567abe26bd4 size: 6198\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                                                   STATUS\n",
      "82a1b8c8-9319-4651-8a48-17534efcab3b  2022-06-20T16:25:11+00:00  4M44S     gs://pbalm-cxb-aa_cloudbuild/source/1655742309.221514-740f4453db5947e3acb7e4ebe281d700.tgz  europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/dataflow (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cp build/Dockerfile.dataflow Dockerfile\n",
    "!gcloud builds submit . --tag europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/dataflow:latest --timeout=30m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d903ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e772e2fb-8059-4199-a61b-ce7485b9e0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'188940921537-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAFLOW_SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fac3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_name: etl-creditcards-classifier-v01-run-gcp-20220620121956-with-sa-private-ips-nat-custom-container\n",
      "runner: DataflowRunner\n",
      "raw_data_query: \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM vertex_eu.creditcards_ml \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 1000000\n",
      "exported_data_prefix: gs://pbalm-cxb-aa-eu/creditcards/experiments/creditcards-classifier-v01/run-gcp-20220620121956/exported_data\n",
      "transformed_data_prefix: gs://pbalm-cxb-aa-eu/creditcards/experiments/creditcards-classifier-v01/run-gcp-20220620121956/transformed_data\n",
      "transform_artifact_dir: gs://pbalm-cxb-aa-eu/creditcards/experiments/creditcards-classifier-v01/run-gcp-20220620121956/transform_artifacts\n",
      "write_raw_data: False\n",
      "temporary_dir: gs://pbalm-cxb-aa-eu/creditcards/tmp\n",
      "gcs_location: gs://pbalm-cxb-aa-eu/creditcards/bq_tmp\n",
      "project: pbalm-cxb-aa\n",
      "region: europe-west4\n",
      "service_account_email: 188940921537-compute@developer.gserviceaccount.com\n",
      "use_public_ips: False\n",
      "subnetwork: https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default\n",
      "sdk_container_image: europe-west4-docker.pkg.dev/pbalm-cxb-aa/dataflow/creditcards:latest\n"
     ]
    }
   ],
   "source": [
    "ML_USE = 'UNASSIGNED'\n",
    "LIMIT = 1000000\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=VERTEX_DATASET_NAME, \n",
    "    ml_use=ML_USE, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "etl_job_name = f\"etl-{MODEL_DISPLAY_NAME}-{run_id}\"\n",
    "\n",
    "args = {\n",
    "    'job_name': etl_job_name + \"-with-sa-private-ips-nat-custom-container\",\n",
    "    'runner': 'DataflowRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'write_raw_data': False,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': DATAFLOW_REGION,\n",
    "    #'setup_file': './setup.py',\n",
    "    #'requirements_file': 'df_requirements.txt',\n",
    "    'service_account_email': DATAFLOW_SERVICE_ACCOUNT,\n",
    "    'use_public_ips': False,\n",
    "    'subnetwork': DATAFLOW_SUBNETWORK,\n",
    "    'sdk_container_image': 'europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/dataflow:latest'\n",
    "    #'experiments': ['disable_runner_v2']\n",
    "}\n",
    "\n",
    "for k in args:\n",
    "    print(f'{k}: {args[k]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c909d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "014e5512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing started...\n",
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a21c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4dfa8",
   "metadata": {},
   "source": [
    "## 4. Submit a Custom Training Job to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb5741-5c1a-4d66-8d34-828a49e44c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'''python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTIFACTS_DIR} \\\n",
    "    --num-epochs=3 \\\n",
    "    --hidden-units=32,32 \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e998139",
   "metadata": {},
   "source": [
    "### Test the training task locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTIFACTS_DIR} \\\n",
    "    --num-epochs=3 \\\n",
    "    --hidden-units=32,32 \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077e9d4",
   "metadata": {},
   "source": [
    "### Prepare training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(WORKSPACE, 'trainer_packages')\n",
    "TRAINER_PACKAGE_NAME = f'{MODEL_DISPLAY_NAME}_trainer'\n",
    "print(\"Trainer package upload location:\", TRAINER_PACKAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cc9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/__pycache__/\n",
    "!rm -r src/.ipynb_checkpoints/\n",
    "!rm -r src/raw_schema/.ipynb_checkpoints/\n",
    "!rm -f {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}.tar.gz\n",
    "\n",
    "!mkdir {TRAINER_PACKAGE_NAME}\n",
    "\n",
    "!cp setup.py {TRAINER_PACKAGE_NAME}/\n",
    "!cp -r src {TRAINER_PACKAGE_NAME}/\n",
    "!tar cvf {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}\n",
    "!gzip {TRAINER_PACKAGE_NAME}.tar\n",
    "!gsutil cp {TRAINER_PACKAGE_NAME}.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "!rm -r {TRAINER_PACKAGE_NAME}\n",
    "!rm -r {TRAINER_PACKAGE_NAME}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3af757",
   "metadata": {},
   "source": [
    "### Prepare the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bf43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RUNTIME = 'tf-cpu.2-5'\n",
    "TRAIN_IMAGE = f\"europe-docker.pkg.dev/vertex-ai/training/{TRAIN_RUNTIME}:latest\"\n",
    "print(\"Training image:\", TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "hidden_units = \"64,64\"\n",
    "\n",
    "trainer_args = [\n",
    "    f'--train-data-dir={TRANSFORMED_DATA_PREFIX + \"/train/*\"}',\n",
    "    f'--eval-data-dir={TRANSFORMED_DATA_PREFIX + \"/eval/*\"}',\n",
    "    f'--tft-output-dir={TRANSFORM_ARTIFACTS_DIR}',\n",
    "    f'--num-epochs={num_epochs}',\n",
    "    f'--learning-rate={learning_rate}',\n",
    "    f'--project={PROJECT}',\n",
    "    f'--region={REGION}',\n",
    "    f'--staging-bucket={BUCKET}',\n",
    "    f'--experiment-name={EXPERIMENT_NAME}'\n",
    "]\n",
    "\n",
    "for a in trainer_args:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a30d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_uri = os.path.join(TRAINER_PACKAGE_DIR, f'{TRAINER_PACKAGE_NAME}.tar.gz')\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": 'n1-standard-4',\n",
    "            \"accelerator_count\": 0\n",
    "    },\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": \"src.model_training.task\",\n",
    "            \"args\": trainer_args,\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for a in worker_pool_specs[0]:\n",
    "    print(f'{a}: {worker_pool_specs[0][a]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d6e99",
   "metadata": {},
   "source": [
    "### Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a custom training job...\")\n",
    "\n",
    "training_job_display_name = f\"{TRAINER_PACKAGE_NAME}_{run_id}\"\n",
    "\n",
    "training_job = vertex_ai.CustomJob(\n",
    "    display_name=training_job_display_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    base_output_dir=EXPERIMENT_RUN_DIR,\n",
    ")\n",
    "\n",
    "training_job.run(\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2896b59",
   "metadata": {},
   "source": [
    "## 5. Upload exported model to Vertex AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e0c39",
   "metadata": {},
   "source": [
    "### Generate the Explanation metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_config = features.generate_explanation_config(transform_feature_spec)\n",
    "explanation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd0587",
   "metadata": {},
   "source": [
    "### Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_RUNTIME='tf2-cpu.2-5'\n",
    "SERVING_IMAGE = f\"europe-docker.pkg.dev/vertex-ai/prediction/{SERVING_RUNTIME}:latest\"\n",
    "print(\"Serving image:\", SERVING_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_metadata = vertex_ai.explain.ExplanationMetadata(\n",
    "    inputs=explanation_config[\"inputs\"],\n",
    "    outputs=explanation_config[\"outputs\"],\n",
    ")\n",
    "explanation_parameters = vertex_ai.explain.ExplanationParameters(\n",
    "    explanation_config[\"params\"]\n",
    ")\n",
    "\n",
    "vertex_model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=SERVING_IMAGE,\n",
    "    parameters_schema_uri=None,\n",
    "    instance_schema_uri=None,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    explanation_parameters=explanation_parameters,\n",
    "    labels={\n",
    "        'dataset_name': VERTEX_DATASET_NAME,\n",
    "        'experiment': run_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51756dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model.gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa55220",
   "metadata": {},
   "source": [
    "## 6. Extract experiment run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07808f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = vertex_ai.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/experiments/{EXPERIMENT_NAME}/metrics?project={PROJECT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96618f6",
   "metadata": {},
   "source": [
    "## 7. Submit a Hyperparameter Tuning Job to Vertex AI\n",
    "\n",
    "For more information about configuring a hyperparameter study, refer to [Vertex AI Hyperparameter job configuration](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b87cd",
   "metadata": {},
   "source": [
    "### Configure a hyperparameter job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffa249",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_spec = {\n",
    "    'ACCURACY': 'maximize'\n",
    "}\n",
    "\n",
    "parameter_spec = {\n",
    "    'learning-rate': hp_tuning.DoubleParameterSpec(min=0.0001, max=0.01, scale='log'),\n",
    "    'hidden-units': hp_tuning.CategoricalParameterSpec(values=[\"32,32\", \"64,64\", \"128,128\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2454dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_display_name = f\"hpt_{TRAINER_PACKAGE_NAME}_{run_id}\"\n",
    "\n",
    "hp_tuning_job = vertex_ai.HyperparameterTuningJob(\n",
    "    display_name=tuning_job_display_name,\n",
    "    custom_job=training_job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=4,\n",
    "    parallel_trial_count=2,\n",
    "    search_algorithm=None # Bayesian optimization.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4ee22",
   "metadata": {},
   "source": [
    "### Submit the hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a hyperparameter tunning job...\")\n",
    "\n",
    "# FIXME: HP tuning job says it has \"no managed dataset\"\n",
    "\n",
    "hp_tuning_job.run(\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=tensorboard_resource_name,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd20da",
   "metadata": {},
   "source": [
    "### Retrieve trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8352e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_tuning_job.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311922be",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = sorted(\n",
    "    hp_tuning_job.trials, \n",
    "    key=lambda trial: trial.final_measurement.metrics[0].value, \n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "print(\"Best trial ID:\", best_trial.id)\n",
    "print(\"Validation Accuracy:\", best_trial.final_measurement.metrics[0].value)\n",
    "print(\"Hyperparameter Values:\")\n",
    "for parameter in best_trial.parameters:\n",
    "    print(f\" - {parameter.parameter_id}:{parameter.value}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
