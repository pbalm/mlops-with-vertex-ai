{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109707f2",
   "metadata": {},
   "source": [
    "# 04 - Test and Deploy Training Pipeline to Vertex Pipelines\n",
    "\n",
    "The purpose of this notebook is to test, deploy, and run the `TFX` pipeline on `Vertex Pipelines`. The notebook covers the following tasks:\n",
    "1. Run the tests locally.\n",
    "2. Run the pipeline using `Vertex Pipelines`\n",
    "3. Execute the pipeline deployment `CI/CD` steps using `Cloud Build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a51af1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e7d80",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3bb184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Tensorflow Version: 1.8.0\n",
      "KFP Version: 1.8.12\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "import tfx.v1 as tfx\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)\n",
    "print(\"KFP Version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bdded",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4b22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: pbalm-cxb-aa\n",
      "Region: europe-west4\n",
      "Bucket name: pbalm-cxb-aa-eu\n",
      "Service Account: 188940921537-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'pbalm-cxb-aa'\n",
    "REGION = 'europe-west4'\n",
    "CF_REGION = 'europe-west1' # No Cloud Functions in europe-west4\n",
    "BUCKET =  PROJECT + '-eu'\n",
    "SERVICE_ACCOUNT = \"188940921537-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP project id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a9dc9",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a75e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_LOCATION = 'EU'\n",
    "BQ_DATASET_NAME = 'vertex_eu' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'creditcards_ml'\n",
    "\n",
    "VERSION = 'v02'\n",
    "DATASET_DISPLAY_NAME = 'creditcards'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT}/creditcards/{CICD_IMAGE_NAME}\"\n",
    "\n",
    "DATAFLOW_REGION = 'europe-west4'\n",
    "DATAFLOW_SERVICE_ACCOUNT = SERVICE_ACCOUNT\n",
    "DATAFLOW_SUBNETWORK = f'https://www.googleapis.com/compute/v1/projects/{PROJECT}/regions/{REGION}/subnetworks/default'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06be3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'src/raw_schema/.ipynb_checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r src/raw_schema/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44678373",
   "metadata": {},
   "source": [
    "## 1. Run the CICD steps locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68204ed",
   "metadata": {},
   "source": [
    "### Set pipeline configurations for the local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] =  MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"BQ_TABLE_NAME\"] = BQ_TABLE_NAME\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"1000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"100\"\n",
    "os.environ[\"UPLOAD_MODEL\"] = \"1\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"-0.1\"    # NB Negative accuracy threshold makes no sense - allows everything\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d353e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: pbalm-cxb-aa\n",
      "REGION: europe-west4\n",
      "GCS_LOCATION: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests\n",
      "ARTIFACT_STORE_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: creditcards\n",
      "MODEL_DISPLAY_NAME: creditcards-classifier-v02\n",
      "PIPELINE_NAME: creditcards-classifier-v02-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 1000\n",
      "TEST_LIMIT: 100\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: -0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/pbalm-cxb-aa/tfx-creditcards:latest\n",
      "BEAM_RUNNER: DirectRunner\n",
      "SERVICE_ACCOUNT: \n",
      "SUBNETWORK: \n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', '--region=europe-west4', '--runner=DirectRunner', '--service_account_email=', '--no_use_public_ips', '--subnetwork=']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/pbalm-cxb-aa/tfx-creditcards:latest'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'europe-west4', 'ai_platform_training_args': {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/pbalm-cxb-aa/tfx-creditcards:latest'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', 'gcs_location': 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', 'project': 'pbalm-cxb-aa', 'region': 'europe-west4', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: creditcards-classifier-v02-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 1\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d51c12",
   "metadata": {},
   "source": [
    "### Run unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be84a8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/datasource_utils_tests.py BigQuery Source: pbalm-cxb-aa.vertex_eu.creditcards_ml\n",
      "\u001b[32m.\u001b[0mBigQuery Source: pbalm-cxb-aa.vertex_eu.creditcards_ml\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 6.58s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4358f955",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 3 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/model_tests.py \u001b[32m.\u001b[0m\u001b[33ms\u001b[0m2022-06-29 10:05:55.682642: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-29 10:05:55.684288: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['V1[0][0]',                     \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'Amount[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           1920        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,033\n",
      "Trainable params: 4,033\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19\n",
      "  /opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "    'nearest': pil_image.NEAREST,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "    'bilinear': pil_image.BILINEAR,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "    'bicubic': pil_image.BICUBIC,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    if hasattr(pil_image, 'HAMMING'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    if hasattr(pil_image, 'BOX'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    if hasattr(pil_image, 'LANCZOS'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m1 skipped\u001b[0m, \u001b[33m\u001b[1m10 warnings\u001b[0m\u001b[33m in 2.77s\u001b[0m\u001b[33m ===================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa00fd5",
   "metadata": {},
   "source": [
    "### Run e2e pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9aad70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py upload_model: 1\n",
      "Pipeline e2e test artifacts stored in: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests\n",
      "ML metadata store is ready.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/credi\"}\n",
      "Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'GcsModelPusher', 'VertexUploader']\n",
      "Beam pipeline args: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp']\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai/src/preprocessing/transformations.py' (including modules: ['etl', 'transformations']).\n",
      "User module package has hash fingerprint version 29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.\n",
      "Executing: ['/opt/conda/bin/python3.7', '/tmp/tmp9jf4mq2d/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpqgur8ht3', '--dist-dir', '/tmp/tmpv_d6bf5m']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmpqgur8ht3\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /tmp/tmpqgur8ht3\n",
      "copying build/lib/etl.py -> /tmp/tmpqgur8ht3\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmpqgur8ht3/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpqgur8ht3/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL\n",
      "creating '/tmp/tmpv_d6bf5m/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' and adding '/tmp/tmpqgur8ht3' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/RECORD'\n",
      "removing /tmp/tmpqgur8ht3\n",
      "Successfully built user code wheel distribution at 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'; target user module is 'transformations'.\n",
      "Full user module path is 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai/src/model_training/runner.py' (including modules: ['trainer', 'runner', 'model', 'defaults', 'exporter', 'data', 'task']).\n",
      "User module package has hash fingerprint version 5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775.\n",
      "Executing: ['/opt/conda/bin/python3.7', '/tmp/tmpb4pk7phj/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmp5g6855c8', '--dist-dir', '/tmp/tmph95qoe2g']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying trainer.py -> build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying task.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmp5g6855c8\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/trainer.py -> /tmp/tmp5g6855c8\n",
      "copying build/lib/model.py -> /tmp/tmp5g6855c8\n",
      "copying build/lib/runner.py -> /tmp/tmp5g6855c8\n",
      "copying build/lib/task.py -> /tmp/tmp5g6855c8\n",
      "copying build/lib/data.py -> /tmp/tmp5g6855c8\n",
      "copying build/lib/defaults.py -> /tmp/tmp5g6855c8\n",
      "copying build/lib/exporter.py -> /tmp/tmp5g6855c8\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmp5g6855c8/tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp5g6855c8/tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775.dist-info/WHEEL\n",
      "creating '/tmp/tmph95qoe2g/tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775-py3-none-any.whl' and adding '/tmp/tmp5g6855c8' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'task.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775.dist-info/RECORD'\n",
      "removing /tmp/tmp5g6855c8\n",
      "Successfully built user code wheel distribution at 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775-py3-none-any.whl'; target user module is 'runner'.\n",
      "Full user module path is 'runner@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775-py3-none-any.whl'\n",
      "Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"DataTransformer\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"GcsModelPusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"HyperparamsGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.hyperparameters_gen_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelEvaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelTrainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"VertexUploader\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.vertex_model_uploader_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"mlmd.sqllite\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"mlmd.sqllite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "Component BaselineModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"BaselineModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.BaselineModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"model\"\n",
      "      input_keys: \"model_blessing\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type ModelBlessing is not found in MLMD.\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component BaselineModelResolver is finished.\n",
      "Component HyperparamsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 2\n",
      "Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={}, output_dict=defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}), exec_properties={'batch_size': 512, 'hidden_units': '128,128', 'num_epochs': 1, 'learning_rate': 0.001}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir/2022-06-29T10:06:30.679874', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-29T10:06:30.679874')\n",
      "Hyperparameters: {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Hyperparameters are written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2/hyperparameters.json\n",
      "Cleaning up stateless execution info.\n",
      "Execution 2 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 2\n",
      "MetadataStore with DB connection initialized\n",
      "Component HyperparamsGen is finished.\n",
      "Component SchemaImporter is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"SchemaImporter\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/raw_schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an importer node.\n",
      "MetadataStore with DB connection initialized\n",
      "Processing source uri: src/raw_schema, properties: {}, custom_properties: {}\n",
      "Component SchemaImporter is finished.\n",
      "Component TestDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 4\n",
      "Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'output_file_format': 5, 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\"\\n    }\\n  ]\\n}', 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir/2022-06-29T10:06:30.679874', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-29T10:06:30.679874')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmppm1h85hg/build/tfx\n",
      "Generating a temp setup file at /tmp/tmppm1h85hg/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmppm1h85hg/build/tfx/setup.log\n",
      "E0629 10:06:37.108839935   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmppm1h85hg/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa4d1d66cb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa4d1d66dd0> ====================\n",
      "==================== <function pack_combiners at 0x7fa4d1d6c320> ====================\n",
      "==================== <function lift_combiners at 0x7fa4d1d6c3b0> ====================\n",
      "==================== <function expand_sdf at 0x7fa4d1d6c560> ====================\n",
      "==================== <function expand_gbk at 0x7fa4d1d6c5f0> ====================\n",
      "==================== <function sink_flattens at 0x7fa4d1d6c710> ====================\n",
      "==================== <function greedily_fuse at 0x7fa4d1d6c7a0> ====================\n",
      "==================== <function read_to_impulse at 0x7fa4d1d6c830> ====================\n",
      "==================== <function impulse_to_input at 0x7fa4d1d6c8c0> ====================\n",
      "==================== <function sort_stages at 0x7fa4d1d6cb00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa4d1d6cc20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa4d1d6ca70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa4d1d6cb90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa4ce450d50> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Setting socket default timeout to 60 seconds.\n",
      "socket default timeout is 60.0 seconds.\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Using location 'EU' from table <TableReference\n",
      " datasetId: 'vertex_eu'\n",
      " projectId: 'pbalm-cxb-aa'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM vertex_eu.creditcards_ml \n",
      "    WHERE ML_use = 'TEST'\n",
      "    LIMIT 100\n",
      "Dataset pbalm-cxb-aa:beam_temp_dataset_c134d86d0cd3420f985236277c649406 does not exist so we will create it as temporary with location=EU\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_2efcc14e-6_1656497201_445'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_2efcc14e-6_1656497201_445\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_2efcc14e-6_1656497207_76'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_2efcc14e-6_1656497207_76\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.044501543045043945 seconds.\n",
      "E0629 10:06:54.837773100   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03445935249328613 seconds.\n",
      "Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.032044172286987305 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03640174865722656 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03772711753845215 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 4 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 4\n",
      "MetadataStore with DB connection initialized\n",
      "Component TestDataGen is finished.\n",
      "Component TrainDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 5\n",
      "Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir/2022-06-29T10:06:30.679874', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-29T10:06:30.679874')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmp3dls6k_s/build/tfx\n",
      "Generating a temp setup file at /tmp/tmp3dls6k_s/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmp3dls6k_s/build/tfx/setup.log\n",
      "Added --extra_package=/tmp/tmp3dls6k_s/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa4d1d66cb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa4d1d66dd0> ====================\n",
      "==================== <function pack_combiners at 0x7fa4d1d6c320> ====================\n",
      "==================== <function lift_combiners at 0x7fa4d1d6c3b0> ====================\n",
      "==================== <function expand_sdf at 0x7fa4d1d6c560> ====================\n",
      "==================== <function expand_gbk at 0x7fa4d1d6c5f0> ====================\n",
      "==================== <function sink_flattens at 0x7fa4d1d6c710> ====================\n",
      "==================== <function greedily_fuse at 0x7fa4d1d6c7a0> ====================\n",
      "==================== <function read_to_impulse at 0x7fa4d1d6c830> ====================\n",
      "==================== <function impulse_to_input at 0x7fa4d1d6c8c0> ====================\n",
      "==================== <function sort_stages at 0x7fa4d1d6cb00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa4d1d6cc20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa4d1d6ca70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa4d1d6cb90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa4cca7a6d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Using location 'EU' from table <TableReference\n",
      " datasetId: 'vertex_eu'\n",
      " projectId: 'pbalm-cxb-aa'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM vertex_eu.creditcards_ml \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 1000\n",
      "Dataset pbalm-cxb-aa:beam_temp_dataset_0a01414b8a4842f3b07a7d72bba43cb5 does not exist so we will create it as temporary with location=EU\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_87d0a794-7_1656497224_702'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_87d0a794-7_1656497224_702\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_87d0a794-7_1656497230_435'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_87d0a794-7_1656497230_435\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.047075748443603516 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03565263748168945 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03969001770019531 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.032993316650390625 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03940725326538086 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.040094852447509766 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03748774528503418 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.037178754806518555 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 5 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 5\n",
      "MetadataStore with DB connection initialized\n",
      "Component TrainDataGen is finished.\n",
      "Component WarmstartModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"WarmstartModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.WarmstartModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"latest_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"latest_model\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component WarmstartModelResolver is finished.\n",
      "Component StatisticsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 7\n",
      "Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1656497240695\n",
      "last_update_time_since_epoch: 1656497240695\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir/2022-06-29T10:06:30.679874', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-29T10:06:30.679874')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmp3a378gxo/build/tfx\n",
      "Generating a temp setup file at /tmp/tmp3a378gxo/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmp3a378gxo/build/tfx/setup.log\n",
      "E0629 10:07:23.523277952   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmp3a378gxo/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Generating statistics for split train.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04723930358886719 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split train written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-train.\n",
      "Generating statistics for split eval.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04176044464111328 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split eval written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-eval.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa4d1d66cb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa4d1d66dd0> ====================\n",
      "==================== <function pack_combiners at 0x7fa4d1d6c320> ====================\n",
      "==================== <function lift_combiners at 0x7fa4d1d6c3b0> ====================\n",
      "==================== <function expand_sdf at 0x7fa4d1d6c560> ====================\n",
      "==================== <function expand_gbk at 0x7fa4d1d6c5f0> ====================\n",
      "==================== <function sink_flattens at 0x7fa4d1d6c710> ====================\n",
      "==================== <function greedily_fuse at 0x7fa4d1d6c7a0> ====================\n",
      "==================== <function read_to_impulse at 0x7fa4d1d6c830> ====================\n",
      "==================== <function impulse_to_input at 0x7fa4d1d6c8c0> ====================\n",
      "==================== <function sort_stages at 0x7fa4d1d6cb00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa4d1d6cc20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa4d1d6ca70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa4d1d6cb90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa4cd830e10> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033611297607421875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035961151123046875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04248929023742676 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03826451301574707 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.045197486877441406 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03633880615234375 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 7 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 7\n",
      "MetadataStore with DB connection initialized\n",
      "Component StatisticsGen is finished.\n",
      "Component ExampleValidator is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 8\n",
      "Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1656497193838\n",
      "last_update_time_since_epoch: 1656497193838\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")], 'statistics': [Artifact(artifact: id: 5\n",
      "type_id: 22\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:StatisticsGen:statistics:0\"\n",
      "create_time_since_epoch: 1656497250707\n",
      "last_update_time_since_epoch: 1656497250707\n",
      ", artifact_type: id: 22\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir/2022-06-29T10:06:30.679874', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-29T10:06:30.679874')\n",
      "Validating schema against the computed statistics for split train.\n",
      "Validation complete for split train. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-train.\n",
      "Validating schema against the computed statistics for split eval.\n",
      "Validation complete for split eval. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-eval.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 8 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 8\n",
      "MetadataStore with DB connection initialized\n",
      "Component ExampleValidator is finished.\n",
      "Component DataTransformer is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 9\n",
      "Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1656497193838\n",
      "last_update_time_since_epoch: 1656497193838\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")], 'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1656497240695\n",
      "last_update_time_since_epoch: 1656497240695\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'transformed_examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:transformed_examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:pre_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:pre_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:transform_graph:0\"\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_anomalies/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:post_transform_anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:post_transform_anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/updated_analyzer_cache/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:updated_analyzer_cache:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:updated_analyzer_cache:0\"\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:post_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:post_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'pre_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:pre_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:pre_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:post_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-29T10:06:30.679874:DataTransformer:post_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'force_tf_compat_v1': 0, 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}', 'disable_statistics': 0, 'custom_config': 'null'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir/2022-06-29T10:06:30.679874', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-29T10:06:30.679874\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-29T10:06:30.679874\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-29T10:06:30.679874')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpys90lh97/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpys90lh97/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpys90lh97/build/tfx/setup.log\n",
      "E0629 10:07:40.792517264   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmpys90lh97/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Installing '/tmp/tmpy377ev1j/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmpqs0hj8id', '/tmp/tmpy377ev1j/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "E0629 10:07:42.847660791   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmpy377ev1j/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Successfully installed '/tmp/tmpy377ev1j/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Installing '/tmp/tmp8o6enrar/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmpp25f9a0_', '/tmp/tmp8o6enrar/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "E0629 10:07:46.200680419   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmp8o6enrar/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Successfully installed '/tmp/tmp8o6enrar/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "Installing '/tmp/tmph6cjne0i/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmptmj4947j', '/tmp/tmph6cjne0i/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "E0629 10:07:49.551480974   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmph6cjne0i/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Successfully installed '/tmp/tmph6cjne0i/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "2022-06-29 10:07:53.945824: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-29 10:07:53.947528: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:326: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04428696632385254 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03676772117614746 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03308582305908203 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa4d1d66cb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa4d1d66dd0> ====================\n",
      "==================== <function pack_combiners at 0x7fa4d1d6c320> ====================\n",
      "==================== <function lift_combiners at 0x7fa4d1d6c3b0> ====================\n",
      "==================== <function expand_sdf at 0x7fa4d1d6c560> ====================\n",
      "==================== <function expand_gbk at 0x7fa4d1d6c5f0> ====================\n",
      "==================== <function sink_flattens at 0x7fa4d1d6c710> ====================\n",
      "==================== <function greedily_fuse at 0x7fa4d1d6c7a0> ====================\n",
      "==================== <function read_to_impulse at 0x7fa4d1d6c830> ====================\n",
      "==================== <function impulse_to_input at 0x7fa4d1d6c8c0> ====================\n",
      "==================== <function sort_stages at 0x7fa4d1d6cb00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa4d1d6cc20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa4d1d6ca70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa4d1d6cb90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa4cd7b10d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.042479515075683594 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03880906105041504 seconds.\n",
      "2022-06-29 10:08:21.867941: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/876b1e22f1ca47d3a6a352d36ece37c7/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03944683074951172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04124808311462402 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.042516231536865234 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03808021545410156 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.034461259841918945 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03742337226867676 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.035900115966796875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04183077812194824 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029165029525756836 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03337693214416504 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.034372806549072266 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03434491157531738 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.032878875732421875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.033540964126586914 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03544116020202637 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04300284385681152 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031636714935302734 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0342559814453125 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03723502159118652 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03351449966430664 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04143476486206055 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03786325454711914 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03874635696411133 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04018139839172363 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0337672233581543 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03837084770202637 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04065227508544922 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04430103302001953 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03723502159118652 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03645634651184082 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0396428108215332 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.038636207580566406 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03892374038696289 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031499385833740234 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.039563655853271484 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04167532920837402 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030931472778320312 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.036407470703125 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030397891998291016 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03979372978210449 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.035886287689208984 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03689146041870117 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03620100021362305 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04593014717102051 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03097677230834961 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03394055366516113 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031642913818359375 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03406548500061035 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02915191650390625 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03698921203613281 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031484127044677734 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.034485816955566406 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03026580810546875 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03506588935852051 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.036203622817993164 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035134315490722656 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03369331359863281 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03225517272949219 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03330039978027344 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03330349922180176 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03604435920715332 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0367588996887207 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029822826385498047 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.042090415954589844 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03683638572692871 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03420758247375488 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.035752296447753906 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03815317153930664 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03415513038635254 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.031609296798706055 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03910374641418457 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03390216827392578 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.033777713775634766 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0333399772644043 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04246783256530762 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.031571149826049805 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03365278244018555 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.038116455078125 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03637385368347168 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03343772888183594 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.037615060806274414 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04003024101257324 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03166604042053223 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03419756889343262 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03019857406616211 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04240918159484863 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031900644302368164 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.040949344635009766 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04205775260925293 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03328895568847656 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031861305236816406 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033635616302490234 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0345916748046875 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/82c8de1013db415db8666af9135bce7b/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04767322540283203 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02956247329711914 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03736734390258789 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03231954574584961 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03375101089477539 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.030804872512817383 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028522014617919922 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.047821760177612305 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03653144836425781 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Artifact type Model is not found in MLMD.\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "E0629 10:09:25.140109846   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmp375s246f/tfx_user_code_ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Successfully installed tfx-user-code-ModelTrainer-0.0+5b617b2a031159a2c04f7509f61fe28617f5e74365d1339d246e45ff40e64775\n",
      "Runner started...\n",
      "fn_args: FnArgs(working_dir=None, train_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-train/*'], eval_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-eval/*'], train_steps=None, eval_steps=None, schema_path='src/raw_schema/schema.pbtxt', schema_file='src/raw_schema/schema.pbtxt', transform_graph_path='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', transform_output='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7fa4cc0df320>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7fa4cc0df9e0>, data_view_decode_fn=None), serving_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving', eval_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-TFMA', model_run_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model_run/10', base_model=None, hyperparameters={'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}, custom_config=None)\n",
      "\n",
      "Hyperparameter:\n",
      "{'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "\n",
      "Runner executing trainer...\n",
      "Loading tft output from gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['Amount[0][0]',                 \n",
      "                                                                  'V1[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]']                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          3840        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            129         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,481\n",
      "Trainable params: 20,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model training started...\n",
      "      1/Unknown - 1s 910ms/step - loss: 0.7573 - accuracy: 0.0059Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7573 - accuracy: 0.0059\n",
      "Model training completed.\n",
      "Runner executing exporter...\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Model export started...\n",
      "Function `serve_features_fn` contains input name(s) Amount, V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26, V27, V28, V3, V4, V5, V6, V7, V8, V9 with unsupported characters which will be renamed to amount, v1, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v2, v20, v21, v22, v23, v24, v25, v26, v27, v28, v3, v4, v5, v6, v7, v8, v9 in the SavedModel.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving/assets\n",
      "Model export completed.\n",
      "Runner completed.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "E0629 10:09:45.413303686   19092 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa4acd26310> and <keras.engine.input_layer.InputLayer object at 0x7fa499087290>).\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.037001609802246094 seconds.\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa49b2d1e50> and <keras.engine.input_layer.InputLayer object at 0x7fa49abf9390>).\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa4d1d66cb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa4d1d66dd0> ====================\n",
      "==================== <function pack_combiners at 0x7fa4d1d6c320> ====================\n",
      "==================== <function lift_combiners at 0x7fa4d1d6c3b0> ====================\n",
      "==================== <function expand_sdf at 0x7fa4d1d6c560> ====================\n",
      "==================== <function expand_gbk at 0x7fa4d1d6c5f0> ====================\n",
      "==================== <function sink_flattens at 0x7fa4d1d6c710> ====================\n",
      "==================== <function greedily_fuse at 0x7fa4d1d6c7a0> ====================\n",
      "==================== <function read_to_impulse at 0x7fa4d1d6c830> ====================\n",
      "==================== <function impulse_to_input at 0x7fa4d1d6c8c0> ====================\n",
      "==================== <function sort_stages at 0x7fa4d1d6cb00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa4d1d6cc20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa4d1d6ca70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa4d1d6cb90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa49aedcf50> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.039079904556274414 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03391242027282715 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa498b70110> and <keras.engine.input_layer.InputLayer object at 0x7fa4acebc190>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa499963b90> and <keras.engine.input_layer.InputLayer object at 0x7fa499d64c90>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa4ad9903d0> and <keras.engine.input_layer.InputLayer object at 0x7fa4ad2dfdd0>).\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03845024108886719 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa4cd617c10> and <keras.engine.input_layer.InputLayer object at 0x7fa4cdadb850>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa49a059f10> and <keras.engine.input_layer.InputLayer object at 0x7fa4ad99e950>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa4998c9d10> and <keras.engine.input_layer.InputLayer object at 0x7fa499ad9ad0>).\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03277468681335449 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03773903846740723 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.033792734146118164 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.040325164794921875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04050946235656738 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.037758827209472656 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.035962820053100586 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03134894371032715 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03413844108581543 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03728890419006348 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.31 seconds.\n",
      "From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/GcsModelPusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Model registry location: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry/creditcards-classifier-v02/1656497415/\n",
      "Creating Model\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/188940921537/locations/europe-west4/models/4097272906302619648/operations/9040924025143951360\n",
      "Create Model backing LRO: projects/188940921537/locations/europe-west4/models/4097272906302619648/operations/9040924025143951360\n",
      "Model created. Resource name: projects/188940921537/locations/europe-west4/models/4097272906302619648\n",
      "Model created. Resource name: projects/188940921537/locations/europe-west4/models/4097272906302619648\n",
      "To use this Model in another session:\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/188940921537/locations/europe-west4/models/4097272906302619648')\n",
      "model = aiplatform.Model('projects/188940921537/locations/europe-west4/models/4097272906302619648')\n",
      "Model uploaded to Vertex AI: projects/188940921537/locations/europe-west4/models/4097272906302619648\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/VertexUploader/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Model output: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry/creditcards-classifier-v02\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19\n",
      "  /opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "    'nearest': pil_image.NEAREST,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "    'bilinear': pil_image.BILINEAR,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "    'bicubic': pil_image.BICUBIC,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    if hasattr(pil_image, 'HAMMING'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    if hasattr(pil_image, 'BOX'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    if hasattr(pil_image, 'LANCZOS'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485\n",
      "  /opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    from collections import MutableMapping\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318\n",
      "  /opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    from collections import Mapping\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2471: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    temp_location = pcoll.pipeline.options.view_as(\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2473: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2504: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    | _PassThroughThenCleanup(files_to_remove_pcoll))\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m19 warnings\u001b[0m\u001b[33m in 479.50s (0:07:59)\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5704bcb",
   "metadata": {},
   "source": [
    "## 2. Run the training pipeline using Vertex Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e4a2fdb-5b66-4548-ac3c-3cae00e37ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_VERSION='tfx-1.8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7db74",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = DATAFLOW_REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"85000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"15000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"{REGION}-docker.pkg.dev/{PROJECT}/{DATASET_DISPLAY_NAME}/vertex:{IMG_VERSION}\"\n",
    "os.environ[\"ENABLE_CACHE\"] = \"1\"\n",
    "os.environ[\"SUBNETWORK\"] = DATAFLOW_SUBNETWORK\n",
    "os.environ[\"SERVICE_ACCOUNT\"] = DATAFLOW_SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d83ef31a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: pbalm-cxb-aa\n",
      "REGION: europe-west4\n",
      "GCS_LOCATION: gs://pbalm-cxb-aa-eu/creditcards\n",
      "ARTIFACT_STORE_URI: gs://pbalm-cxb-aa-eu/creditcards/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: creditcards\n",
      "MODEL_DISPLAY_NAME: creditcards-classifier-v02\n",
      "PIPELINE_NAME: creditcards-classifier-v02-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 85000\n",
      "TEST_LIMIT: 15000\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: -0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n",
      "BEAM_RUNNER: DataflowRunner\n",
      "SERVICE_ACCOUNT: 188940921537-compute@developer.gserviceaccount.com\n",
      "SUBNETWORK: https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/temp', '--region=europe-west4', '--runner=DataflowRunner', '--service_account_email=188940921537-compute@developer.gserviceaccount.com', '--no_use_public_ips', '--subnetwork=https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default']\n",
      "TRAINING_RUNNER: vertex\n",
      "VERTEX_TRAINING_ARGS: {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'europe-west4', 'ai_platform_training_args': {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DataflowRunner', 'temporary_dir': 'gs://pbalm-cxb-aa-eu/creditcards/temp', 'gcs_location': 'gs://pbalm-cxb-aa-eu/creditcards/temp', 'project': 'pbalm-cxb-aa', 'region': 'europe-west4', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: creditcards-classifier-v02-predictions\n",
      "ENABLE_CACHE: 1\n",
      "UPLOAD_MODEL: 1\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3164f",
   "metadata": {},
   "source": [
    "### Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a0e729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n"
     ]
    }
   ],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3087da4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 73 file(s) totalling 2.0 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/pbalm-cxb-aa/locations/global/builds/1155e839-2aa0-4f21-924d-b6c9e9e0738a].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/1155e839-2aa0-4f21-924d-b6c9e9e0738a?project=188940921537].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"1155e839-2aa0-4f21-924d-b6c9e9e0738a\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz#1656506422752768\n",
      "Copying gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz#1656506422752768...\n",
      "/ [1 files][459.4 KiB/459.4 KiB]                                                \n",
      "Operation completed over 1 objects/459.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.114MB\n",
      "Step 1/6 : FROM gcr.io/tfx-oss-public/tfx:1.8.0\n",
      "1.8.0: Pulling from tfx-oss-public/tfx\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "086b79b77a03: Pulling fs layer\n",
      "4698168f5888: Pulling fs layer\n",
      "86de3d566666: Pulling fs layer\n",
      "30d00d530989: Pulling fs layer\n",
      "69a2bfee9a44: Pulling fs layer\n",
      "381964195b8b: Pulling fs layer\n",
      "fe1468e51d2b: Pulling fs layer\n",
      "86de3d566666: Waiting\n",
      "69a2bfee9a44: Waiting\n",
      "381964195b8b: Waiting\n",
      "30d00d530989: Waiting\n",
      "e807ad87032f: Pulling fs layer\n",
      "0c557f25f33e: Pulling fs layer\n",
      "67cab7d11474: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "999747c8e1ca: Pulling fs layer\n",
      "e92bc58784f1: Pulling fs layer\n",
      "a9d25440a572: Pulling fs layer\n",
      "ee75ae25ade1: Pulling fs layer\n",
      "b13c015c05f0: Pulling fs layer\n",
      "fe1468e51d2b: Waiting\n",
      "8f0d2639aefc: Pulling fs layer\n",
      "11646adc2850: Pulling fs layer\n",
      "14c7723c1bbe: Pulling fs layer\n",
      "6252b7e4a35a: Pulling fs layer\n",
      "ae96ea101185: Pulling fs layer\n",
      "8553e38f9d3b: Pulling fs layer\n",
      "e807ad87032f: Waiting\n",
      "67cab7d11474: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "999747c8e1ca: Waiting\n",
      "e92bc58784f1: Waiting\n",
      "a9d25440a572: Waiting\n",
      "ee75ae25ade1: Waiting\n",
      "b13c015c05f0: Waiting\n",
      "b4375d47e797: Pulling fs layer\n",
      "906cdf1c6b78: Pulling fs layer\n",
      "d70342317ce5: Pulling fs layer\n",
      "acea7e9af8f8: Pulling fs layer\n",
      "e9ec5ae321aa: Pulling fs layer\n",
      "32eed1f081f7: Pulling fs layer\n",
      "4b5c1c89bd3d: Pulling fs layer\n",
      "80c2cbe5e4a8: Pulling fs layer\n",
      "85c3c971789d: Pulling fs layer\n",
      "fa58d293bde3: Pulling fs layer\n",
      "8f0d2639aefc: Waiting\n",
      "a0efb95d3b56: Pulling fs layer\n",
      "1bb05b14fb7d: Pulling fs layer\n",
      "11646adc2850: Waiting\n",
      "3cebcf201134: Pulling fs layer\n",
      "14c7723c1bbe: Waiting\n",
      "6252b7e4a35a: Waiting\n",
      "32eed1f081f7: Waiting\n",
      "4b5c1c89bd3d: Waiting\n",
      "ae96ea101185: Waiting\n",
      "8553e38f9d3b: Waiting\n",
      "b4375d47e797: Waiting\n",
      "80c2cbe5e4a8: Waiting\n",
      "906cdf1c6b78: Waiting\n",
      "85c3c971789d: Waiting\n",
      "d70342317ce5: Waiting\n",
      "acea7e9af8f8: Waiting\n",
      "e9ec5ae321aa: Waiting\n",
      "fa58d293bde3: Waiting\n",
      "1bb05b14fb7d: Waiting\n",
      "a0efb95d3b56: Waiting\n",
      "3cebcf201134: Waiting\n",
      "086b79b77a03: Verifying Checksum\n",
      "086b79b77a03: Download complete\n",
      "4698168f5888: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "86de3d566666: Verifying Checksum\n",
      "86de3d566666: Download complete\n",
      "30d00d530989: Verifying Checksum\n",
      "30d00d530989: Download complete\n",
      "381964195b8b: Verifying Checksum\n",
      "381964195b8b: Download complete\n",
      "e807ad87032f: Verifying Checksum\n",
      "e807ad87032f: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "086b79b77a03: Pull complete\n",
      "4698168f5888: Pull complete\n",
      "86de3d566666: Pull complete\n",
      "30d00d530989: Pull complete\n",
      "69a2bfee9a44: Verifying Checksum\n",
      "69a2bfee9a44: Download complete\n",
      "67cab7d11474: Verifying Checksum\n",
      "67cab7d11474: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "fe1468e51d2b: Verifying Checksum\n",
      "fe1468e51d2b: Download complete\n",
      "999747c8e1ca: Verifying Checksum\n",
      "999747c8e1ca: Download complete\n",
      "a9d25440a572: Verifying Checksum\n",
      "a9d25440a572: Download complete\n",
      "e92bc58784f1: Download complete\n",
      "ee75ae25ade1: Verifying Checksum\n",
      "ee75ae25ade1: Download complete\n",
      "8f0d2639aefc: Verifying Checksum\n",
      "8f0d2639aefc: Download complete\n",
      "11646adc2850: Verifying Checksum\n",
      "11646adc2850: Download complete\n",
      "14c7723c1bbe: Verifying Checksum\n",
      "14c7723c1bbe: Download complete\n",
      "6252b7e4a35a: Verifying Checksum\n",
      "6252b7e4a35a: Download complete\n",
      "ae96ea101185: Download complete\n",
      "8553e38f9d3b: Verifying Checksum\n",
      "8553e38f9d3b: Download complete\n",
      "b4375d47e797: Download complete\n",
      "906cdf1c6b78: Verifying Checksum\n",
      "906cdf1c6b78: Download complete\n",
      "d70342317ce5: Verifying Checksum\n",
      "d70342317ce5: Download complete\n",
      "b13c015c05f0: Verifying Checksum\n",
      "b13c015c05f0: Download complete\n",
      "0c557f25f33e: Verifying Checksum\n",
      "0c557f25f33e: Download complete\n",
      "acea7e9af8f8: Verifying Checksum\n",
      "acea7e9af8f8: Download complete\n",
      "32eed1f081f7: Verifying Checksum\n",
      "32eed1f081f7: Download complete\n",
      "80c2cbe5e4a8: Verifying Checksum\n",
      "80c2cbe5e4a8: Download complete\n",
      "85c3c971789d: Verifying Checksum\n",
      "85c3c971789d: Download complete\n",
      "fa58d293bde3: Verifying Checksum\n",
      "fa58d293bde3: Download complete\n",
      "a0efb95d3b56: Download complete\n",
      "1bb05b14fb7d: Verifying Checksum\n",
      "1bb05b14fb7d: Download complete\n",
      "4b5c1c89bd3d: Verifying Checksum\n",
      "4b5c1c89bd3d: Download complete\n",
      "3cebcf201134: Verifying Checksum\n",
      "3cebcf201134: Download complete\n",
      "e9ec5ae321aa: Verifying Checksum\n",
      "e9ec5ae321aa: Download complete\n",
      "69a2bfee9a44: Pull complete\n",
      "381964195b8b: Pull complete\n",
      "fe1468e51d2b: Pull complete\n",
      "e807ad87032f: Pull complete\n",
      "0c557f25f33e: Pull complete\n",
      "67cab7d11474: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "999747c8e1ca: Pull complete\n",
      "e92bc58784f1: Pull complete\n",
      "a9d25440a572: Pull complete\n",
      "ee75ae25ade1: Pull complete\n",
      "b13c015c05f0: Pull complete\n",
      "8f0d2639aefc: Pull complete\n",
      "11646adc2850: Pull complete\n",
      "14c7723c1bbe: Pull complete\n",
      "6252b7e4a35a: Pull complete\n",
      "ae96ea101185: Pull complete\n",
      "8553e38f9d3b: Pull complete\n",
      "b4375d47e797: Pull complete\n",
      "906cdf1c6b78: Pull complete\n",
      "d70342317ce5: Pull complete\n",
      "acea7e9af8f8: Pull complete\n",
      "e9ec5ae321aa: Pull complete\n",
      "32eed1f081f7: Pull complete\n",
      "4b5c1c89bd3d: Pull complete\n",
      "80c2cbe5e4a8: Pull complete\n",
      "85c3c971789d: Pull complete\n",
      "fa58d293bde3: Pull complete\n",
      "a0efb95d3b56: Pull complete\n",
      "1bb05b14fb7d: Pull complete\n",
      "3cebcf201134: Pull complete\n",
      "Digest: sha256:5d99c562fcc484d1bd104abab75267f37586d23a97a5a6e354c7828a1d2dfb83\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.8.0\n",
      " ---> 864d5f66048d\n",
      "Step 2/6 : COPY requirements.txt requirements.txt\n",
      " ---> b1841bc20c02\n",
      "Step 3/6 : RUN pip install -r requirements.txt\n",
      " ---> Running in ee545fe99ac3\n",
      "Collecting kfp==1.8.12\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "      301.2/301.2 kB 10.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-cloud-bigquery==2.34.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (2.34.3)\n",
      "Collecting google-cloud-bigquery-storage==2.13.2\n",
      "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "      180.2/180.2 kB 26.2 MB/s eta 0:00:00\n",
      "Collecting google-cloud-aiplatform==1.14.0\n",
      "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
      "      1.9/1.9 MB 63.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-pubsub in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.12.1)\n",
      "Collecting cloudml-hypertune==0.1.0.dev6\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pytest==7.1.2\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "      297.0/297.0 kB 36.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow-data-validation==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: tensorflow-transform==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: tfx==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (1.8.0)\n",
      "Collecting tensorflow-io==0.26.0\n",
      "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
      "      25.9/25.9 MB 62.0 MB/s eta 0:00:00\n",
      "Collecting apache-beam[gcp]==2.39.0\n",
      "  Downloading apache_beam-2.39.0-cp37-cp37m-manylinux2010_x86_64.whl (10.3 MB)\n",
      "      10.3/10.3 MB 91.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.31.5)\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "      106.8/106.8 kB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.12.11)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "      54.3/54.3 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "      58.0/58.0 kB 11.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "      56.3/56.3 kB 11.1 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (0.1.15)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "      87.7/87.7 kB 14.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (3.20.1)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.9.0)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (20.9)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.46.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (4.11.3)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "      98.7/98.7 kB 17.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (20.3.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.3.5)\n",
      "Requirement already satisfied: tfx-bsl<1.9,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.1)\n",
      "Requirement already satisfied: numpy<2,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.21.6)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-metadata<1.9,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: joblib<0.15,>=0.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.14.1)\n",
      "Requirement already satisfied: pyarrow<6,>=1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (5.0.0)\n",
      "Requirement already satisfied: pydot<2,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.8.0->-r requirements.txt (line 10)) (1.4.2)\n",
      "Requirement already satisfied: ml-pipelines-sdk==1.8.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: google-apitools<1,>=0.5 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.5.31)\n",
      "Requirement already satisfied: portpicker<2,>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: docker<5,>=4.1 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (4.4.4)\n",
      "Requirement already satisfied: keras-tuner<2,>=1.0.4 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.1.2)\n",
      "Requirement already satisfied: ml-metadata<1.9.0,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: jinja2<4,>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (2.10)\n",
      "Requirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.40,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.39.0)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.26.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "      2.4/2.4 MB 83.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2022.1)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.6.8)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.7)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.3.1.1)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.12.3)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.19.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2.7.0)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.4.11)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.15.4)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.0.1)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.6.2)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.1.0)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.2.2)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Using cached google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.16.2)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.19.2)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.2.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub->-r requirements.txt (line 5)) (0.12.4)\n",
      "Requirement already satisfied: grpcio-status>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub->-r requirements.txt (line 5)) (1.46.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.12->-r requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<5,>=4.1->tfx==1.8.0->-r requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.12->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12->-r requirements.txt (line 1)) (59.8.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12->-r requirements.txt (line 1)) (1.56.1)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (0.17.3)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (4.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (4.8)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (6.1.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest==7.1.2->-r requirements.txt (line 7)) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2<4,>=2.7.3->tfx==1.8.0->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12->-r requirements.txt (line 1)) (0.18.1)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (1.0.4)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (7.33.0)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.8.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (1.26.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from portpicker<2,>=1.3.1->tfx==1.8.0->-r requirements.txt (line 11)) (5.9.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.12->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.5.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (14.0.1)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.0rc0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (7.7.0)\n",
      "Requirement already satisfied: scipy<2,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.7.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.5.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (5.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.12.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (3.0.29)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.13.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (5.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (3.6.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (0.4.8)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (3.3.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.6.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (7.3.1)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.5.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (4.10.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (2.15.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.4.11)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.4.5)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.13.3)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (21.3.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (4.11.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (5.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.5.13)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.5.1)\n",
      "Building wheels for collected packages: kfp, cloudml-hypertune, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=c71fb66e549af6b1b2c4f09fb1629c798579eb011edcfbc5be0abc5d4547616f\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=b25afd768c1a363c416f42b4629c7397f960a1e1033eb6951771ace7aeeed673\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=a1578731b2117df07628b2394e2c12aab57d15187287a73dc3b8956a6c65d2a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=536a4d4e26a17737a9b47ebd4b82d1121a2a71f139db1c5c3f3994b122b041c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=635bc5f16babf15eab13f21129c6d3482df54cd658e4c209f3459e141a61c5cc\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp cloudml-hypertune fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, iniconfig, cloudml-hypertune, typer, tensorflow-io-gcs-filesystem, tabulate, strip-hints, py, fire, docstring-parser, Deprecated, tensorflow-io, requests-toolbelt, kfp-server-api, jsonschema, pytest, apache-beam, google-cloud-core, google-cloud-storage, google-cloud-bigquery-storage, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.2.0\n",
      "    Uninstalling typing_extensions-4.2.0:\n",
      "      Successfully uninstalled typing_extensions-4.2.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.23.1\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.23.1:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.23.1\n",
      "  Attempting uninstall: tensorflow-io\n",
      "    Found existing installation: tensorflow-io 0.23.1\n",
      "    Uninstalling tensorflow-io-0.23.1:\n",
      "      Successfully uninstalled tensorflow-io-0.23.1\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.5.1\n",
      "    Uninstalling jsonschema-4.5.1:\n",
      "      Successfully uninstalled jsonschema-4.5.1\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.38.0\n",
      "    Uninstalling apache-beam-2.38.0:\n",
      "      Successfully uninstalled apache-beam-2.38.0\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.2.2\n",
      "    Uninstalling google-cloud-core-2.2.2:\n",
      "      Successfully uninstalled google-cloud-core-2.2.2\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.2.1\n",
      "    Uninstalling google-cloud-storage-2.2.1:\n",
      "      Successfully uninstalled google-cloud-storage-2.2.1\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.13.1\n",
      "    Uninstalling google-cloud-bigquery-storage-2.13.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.13.1\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.13.0\n",
      "    Uninstalling google-cloud-aiplatform-1.13.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.13.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 apache-beam-2.39.0 cloudml-hypertune-0.1.0.dev6 docstring-parser-0.14.1 fire-0.4.0 google-cloud-aiplatform-1.14.0 google-cloud-bigquery-storage-2.13.2 google-cloud-core-1.7.2 google-cloud-storage-2.1.0 iniconfig-1.1.1 jsonschema-3.2.0 kfp-1.8.12 kfp-server-api-1.8.2 py-1.11.0 pytest-7.1.2 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 tensorflow-io-0.26.0 tensorflow-io-gcs-filesystem-0.26.0 typer-0.4.1 typing-extensions-3.10.0.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: There was an error checking the latest version of pip.\n",
      "\u001b[0mRemoving intermediate container ee545fe99ac3\n",
      " ---> 89d9932b4d31\n",
      "Step 4/6 : RUN pip install -U numpy --ignore-installed\n",
      " ---> Running in a77de1a1ae97\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "      15.7/15.7 MB 79.8 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "statsmodels 0.13.2 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
      "\u001b[0mSuccessfully installed numpy-1.21.6\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: There was an error checking the latest version of pip.\n",
      "\u001b[0mRemoving intermediate container a77de1a1ae97\n",
      " ---> 068cee2879fc\n",
      "Step 5/6 : COPY src/ src/\n",
      " ---> 4f3ddd21e0b9\n",
      "Step 6/6 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in 783ae0dadcb8\n",
      "Removing intermediate container 783ae0dadcb8\n",
      " ---> c0a2aa4ee795\n",
      "Successfully built c0a2aa4ee795\n",
      "Successfully tagged europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n",
      "PUSH\n",
      "Pushing europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n",
      "The push refers to repository [europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex]\n",
      "91885e7876e8: Preparing\n",
      "2af584f9627d: Preparing\n",
      "3f4b01ba4a1a: Preparing\n",
      "683394350b8e: Preparing\n",
      "849f99ab0557: Preparing\n",
      "003ab0deb210: Preparing\n",
      "051b5111dbe3: Preparing\n",
      "105aac973237: Preparing\n",
      "6b279ee1dea4: Preparing\n",
      "0f5815af70ed: Preparing\n",
      "a439fe54d797: Preparing\n",
      "e5bb7384706a: Preparing\n",
      "529b51f6018a: Preparing\n",
      "4d5391a66f17: Preparing\n",
      "8776dda77d84: Preparing\n",
      "f7bf6100a736: Preparing\n",
      "2427ba19d9ab: Preparing\n",
      "c5d8ddc90738: Preparing\n",
      "f9d66d415903: Preparing\n",
      "003ab0deb210: Waiting\n",
      "051b5111dbe3: Waiting\n",
      "105aac973237: Waiting\n",
      "6b279ee1dea4: Waiting\n",
      "0f5815af70ed: Waiting\n",
      "a439fe54d797: Waiting\n",
      "e5bb7384706a: Waiting\n",
      "529b51f6018a: Waiting\n",
      "4d5391a66f17: Waiting\n",
      "8776dda77d84: Waiting\n",
      "f7bf6100a736: Waiting\n",
      "2427ba19d9ab: Waiting\n",
      "0a0b70a03299: Preparing\n",
      "01d285020d37: Preparing\n",
      "ad52cc5ce980: Preparing\n",
      "40d867f1633d: Preparing\n",
      "695cde20e218: Preparing\n",
      "eab9c045ef1b: Preparing\n",
      "ad7c511b31df: Preparing\n",
      "cd9f5c9bd89e: Preparing\n",
      "ca40136e604d: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "08e753b98db4: Preparing\n",
      "8f9243705224: Preparing\n",
      "ba42d5f65b46: Preparing\n",
      "51981f322139: Preparing\n",
      "cbe679dd18e3: Preparing\n",
      "ce07be50029c: Preparing\n",
      "7011392a3aa0: Preparing\n",
      "0cfddc66f231: Preparing\n",
      "a4a375cdde15: Preparing\n",
      "f9d66d415903: Waiting\n",
      "ac5045d5adeb: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "0a0b70a03299: Waiting\n",
      "ca40136e604d: Waiting\n",
      "01d285020d37: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "08e753b98db4: Waiting\n",
      "ad52cc5ce980: Waiting\n",
      "8f9243705224: Waiting\n",
      "ba42d5f65b46: Waiting\n",
      "40d867f1633d: Waiting\n",
      "51981f322139: Waiting\n",
      "695cde20e218: Waiting\n",
      "cbe679dd18e3: Waiting\n",
      "ce07be50029c: Waiting\n",
      "eab9c045ef1b: Waiting\n",
      "7011392a3aa0: Waiting\n",
      "0cfddc66f231: Waiting\n",
      "ad7c511b31df: Waiting\n",
      "a4a375cdde15: Waiting\n",
      "ac5045d5adeb: Waiting\n",
      "cd9f5c9bd89e: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "849f99ab0557: Layer already exists\n",
      "003ab0deb210: Layer already exists\n",
      "051b5111dbe3: Layer already exists\n",
      "105aac973237: Layer already exists\n",
      "6b279ee1dea4: Layer already exists\n",
      "683394350b8e: Pushed\n",
      "91885e7876e8: Pushed\n",
      "0f5815af70ed: Layer already exists\n",
      "529b51f6018a: Layer already exists\n",
      "a439fe54d797: Layer already exists\n",
      "e5bb7384706a: Layer already exists\n",
      "8776dda77d84: Layer already exists\n",
      "4d5391a66f17: Layer already exists\n",
      "f7bf6100a736: Layer already exists\n",
      "2427ba19d9ab: Layer already exists\n",
      "c5d8ddc90738: Layer already exists\n",
      "f9d66d415903: Layer already exists\n",
      "0a0b70a03299: Layer already exists\n",
      "2af584f9627d: Pushed\n",
      "01d285020d37: Layer already exists\n",
      "ad52cc5ce980: Layer already exists\n",
      "40d867f1633d: Layer already exists\n",
      "ad7c511b31df: Layer already exists\n",
      "eab9c045ef1b: Layer already exists\n",
      "695cde20e218: Layer already exists\n",
      "cd9f5c9bd89e: Layer already exists\n",
      "ca40136e604d: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "08e753b98db4: Layer already exists\n",
      "8f9243705224: Layer already exists\n",
      "ba42d5f65b46: Layer already exists\n",
      "51981f322139: Layer already exists\n",
      "cbe679dd18e3: Layer already exists\n",
      "ce07be50029c: Layer already exists\n",
      "7011392a3aa0: Layer already exists\n",
      "0cfddc66f231: Layer already exists\n",
      "a4a375cdde15: Layer already exists\n",
      "ac5045d5adeb: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "3f4b01ba4a1a: Pushed\n",
      "tfx-1.8: digest: sha256:7853e0b60c1320ce13714cc50d3f1d5c29faf27c7d2f9e1cbae7f02f97f77ebe size: 8726\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                                               STATUS\n",
      "1155e839-2aa0-4f21-924d-b6c9e9e0738a  2022-06-29T12:40:23+00:00  3M57S     gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz  europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cp build/Dockerfile.vertex Dockerfile\n",
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155568ca",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c1d5ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://pbalm-cxb-aa-eu/creditcards/tfx_artifacts/creditcards-cla\"}\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "installing to /tmp/tmpbwxcsesb\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /tmp/tmpbwxcsesb\n",
      "copying build/lib/etl.py -> /tmp/tmpbwxcsesb\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmpbwxcsesb/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpbwxcsesb/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL\n",
      "creating '/tmp/tmpucxrdsxa/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' and adding '/tmp/tmpbwxcsesb' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/RECORD'\n",
      "removing /tmp/tmpbwxcsesb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying trainer.py -> build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying task.py -> build/lib\n",
      "installing to /tmp/tmp0no4trhs\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/trainer.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/model.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/runner.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/task.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/data.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/defaults.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/exporter.py -> /tmp/tmp0no4trhs\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmp0no4trhs/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp0no4trhs/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/WHEEL\n",
      "creating '/tmp/tmpvztlac_0/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7-py3-none-any.whl' and adding '/tmp/tmp0no4trhs' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'task.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/RECORD'\n",
      "removing /tmp/tmp0no4trhs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "pipeline_definition = runner.compile_training_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5e506ad-e26d-4fd6-b235-e52302061ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "vertex_ai.init(project=PROJECT, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "855d0d6b-60cf-4bc4-8598-9a0f45acef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://creditcards-classifier-v02-train-pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 31.6 KiB/ 31.6 KiB]                                                \n",
      "Operation completed over 1 objects/31.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/compiled_pipelines/\"\n",
    "!gsutil cp {pipeline_definition_file} {PIPELINES_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb943e",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e625665-e0ac-4978-b820-561018e0adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/creditcards-classifier-v02-train-pipeline-20220630091659?project=188940921537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/creditcards-classifier-v02-train-pipeline-20220630091659?project=188940921537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "    \n",
    "job = pipeline_jobs.PipelineJob(template_path = pipeline_definition_file,\n",
    "                                display_name=DATASET_DISPLAY_NAME,\n",
    "                                #enable_caching=False,\n",
    "                                parameter_values={\n",
    "                                    'learning_rate': 0.003,\n",
    "                                    'batch_size': 512,\n",
    "                                    'hidden_units': '128,128',\n",
    "                                    'num_epochs': 30,\n",
    "                                })\n",
    "\n",
    "job.run(sync=False, service_account=DATAFLOW_SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be1fd",
   "metadata": {},
   "source": [
    "### Extracting pipeline runs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37ae4aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220629101840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220629101840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220629101840')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220629101840')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/creditcards-classifier-v02-train-pipeline-20220629101840?project=188940921537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/creditcards-classifier-v02-train-pipeline-20220629101840?project=188940921537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pipeline_name</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_name</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:num_epochs</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:hidden_units</th>\n",
       "      <td>128,128</td>\n",
       "      <td>256,126</td>\n",
       "      <td>256,126</td>\n",
       "      <td>256,126</td>\n",
       "      <td>128,128</td>\n",
       "      <td>128,128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128,128</td>\n",
       "      <td>128,128</td>\n",
       "      <td>128,128</td>\n",
       "      <td>128,128</td>\n",
       "      <td>128,128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:batch_size</th>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param.input:learning_rate</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          0   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                          1   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                     7   \n",
       "param.input:hidden_units                                             256,126   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                             0.0015   \n",
       "\n",
       "                                                                          2   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                     7   \n",
       "param.input:hidden_units                                             256,126   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                             0.0015   \n",
       "\n",
       "                                                                          3   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                     7   \n",
       "param.input:hidden_units                                             256,126   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                             0.0015   \n",
       "\n",
       "                                                                          4   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                          5   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                          6   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                   NaN   \n",
       "param.input:hidden_units                                                 NaN   \n",
       "param.input:batch_size                                                   NaN   \n",
       "param.input:learning_rate                                                NaN   \n",
       "\n",
       "                                                                          7   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                   NaN   \n",
       "param.input:hidden_units                                                 NaN   \n",
       "param.input:batch_size                                                   NaN   \n",
       "param.input:learning_rate                                                NaN   \n",
       "\n",
       "                                                                          8   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                   NaN   \n",
       "param.input:hidden_units                                                 NaN   \n",
       "param.input:batch_size                                                   NaN   \n",
       "param.input:learning_rate                                                NaN   \n",
       "\n",
       "                                                                          9   \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                          10  \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                          11  \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                          12  \\\n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline   \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "param.input:num_epochs                                                    30   \n",
       "param.input:hidden_units                                             128,128   \n",
       "param.input:batch_size                                                   512   \n",
       "param.input:learning_rate                                              0.003   \n",
       "\n",
       "                                                                          13  \n",
       "pipeline_name                      creditcards-classifier-v02-train-pipeline  \n",
       "run_name                   creditcards-classifier-v02-train-pipeline-2022...  \n",
       "param.input:num_epochs                                                    30  \n",
       "param.input:hidden_units                                             128,128  \n",
       "param.input:batch_size                                                   512  \n",
       "param.input:learning_rate                                              0.003  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
    "pipeline_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b454fe9",
   "metadata": {},
   "source": [
    "## 3. Execute the pipeline deployment CI/CD steps in Cloud Build\n",
    "\n",
    "The CI/CD routine is defined in the [pipeline-deployment.yaml](pipeline-deployment.yaml) file, and consists of the following steps:\n",
    "1. Clone the repository to the build environment.\n",
    "2. Run unit tests.\n",
    "3. Run a local e2e test of the pipeline.\n",
    "4. Build the ML container image for pipeline steps.\n",
    "5. Compile the pipeline.\n",
    "6. Upload the pipeline to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29688d4d",
   "metadata": {},
   "source": [
    "### Build CI/CD container Image for Cloud Build\n",
    "\n",
    "This is the runtime environment where the steps of testing and deploying the pipeline will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4759b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n"
     ]
    }
   ],
   "source": [
    "!echo $CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc09c3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 16 file(s) totalling 27.6 KiB before compression.\n",
      "Uploading tarball of [build/.] to [gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/pbalm-cxb-aa/locations/global/builds/fa254c17-768b-459e-b679-98c9ae4a9b3e].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/fa254c17-768b-459e-b679-98c9ae4a9b3e?project=188940921537].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"fa254c17-768b-459e-b679-98c9ae4a9b3e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz#1656498940963890\n",
      "Copying gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz#1656498940963890...\n",
      "/ [1 files][  4.2 KiB/  4.2 KiB]                                                \n",
      "Operation completed over 1 objects/4.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  41.98kB\n",
      "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:1.8.0\n",
      "1.8.0: Pulling from tfx-oss-public/tfx\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "086b79b77a03: Pulling fs layer\n",
      "4698168f5888: Pulling fs layer\n",
      "86de3d566666: Pulling fs layer\n",
      "30d00d530989: Pulling fs layer\n",
      "69a2bfee9a44: Pulling fs layer\n",
      "381964195b8b: Pulling fs layer\n",
      "fe1468e51d2b: Pulling fs layer\n",
      "e807ad87032f: Pulling fs layer\n",
      "0c557f25f33e: Pulling fs layer\n",
      "67cab7d11474: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "999747c8e1ca: Pulling fs layer\n",
      "e92bc58784f1: Pulling fs layer\n",
      "a9d25440a572: Pulling fs layer\n",
      "ee75ae25ade1: Pulling fs layer\n",
      "b13c015c05f0: Pulling fs layer\n",
      "69a2bfee9a44: Waiting\n",
      "8f0d2639aefc: Pulling fs layer\n",
      "11646adc2850: Pulling fs layer\n",
      "14c7723c1bbe: Pulling fs layer\n",
      "6252b7e4a35a: Pulling fs layer\n",
      "381964195b8b: Waiting\n",
      "ae96ea101185: Pulling fs layer\n",
      "8553e38f9d3b: Pulling fs layer\n",
      "b4375d47e797: Pulling fs layer\n",
      "fe1468e51d2b: Waiting\n",
      "906cdf1c6b78: Pulling fs layer\n",
      "d70342317ce5: Pulling fs layer\n",
      "acea7e9af8f8: Pulling fs layer\n",
      "e9ec5ae321aa: Pulling fs layer\n",
      "32eed1f081f7: Pulling fs layer\n",
      "4b5c1c89bd3d: Pulling fs layer\n",
      "80c2cbe5e4a8: Pulling fs layer\n",
      "85c3c971789d: Pulling fs layer\n",
      "fa58d293bde3: Pulling fs layer\n",
      "a0efb95d3b56: Pulling fs layer\n",
      "1bb05b14fb7d: Pulling fs layer\n",
      "3cebcf201134: Pulling fs layer\n",
      "e807ad87032f: Waiting\n",
      "0c557f25f33e: Waiting\n",
      "67cab7d11474: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "999747c8e1ca: Waiting\n",
      "e92bc58784f1: Waiting\n",
      "a9d25440a572: Waiting\n",
      "ee75ae25ade1: Waiting\n",
      "b13c015c05f0: Waiting\n",
      "8f0d2639aefc: Waiting\n",
      "11646adc2850: Waiting\n",
      "14c7723c1bbe: Waiting\n",
      "86de3d566666: Waiting\n",
      "30d00d530989: Waiting\n",
      "6252b7e4a35a: Waiting\n",
      "80c2cbe5e4a8: Waiting\n",
      "ae96ea101185: Waiting\n",
      "8553e38f9d3b: Waiting\n",
      "85c3c971789d: Waiting\n",
      "b4375d47e797: Waiting\n",
      "906cdf1c6b78: Waiting\n",
      "d70342317ce5: Waiting\n",
      "fa58d293bde3: Waiting\n",
      "acea7e9af8f8: Waiting\n",
      "e9ec5ae321aa: Waiting\n",
      "a0efb95d3b56: Waiting\n",
      "32eed1f081f7: Waiting\n",
      "4b5c1c89bd3d: Waiting\n",
      "1bb05b14fb7d: Waiting\n",
      "3cebcf201134: Waiting\n",
      "086b79b77a03: Download complete\n",
      "4698168f5888: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "30d00d530989: Verifying Checksum\n",
      "30d00d530989: Download complete\n",
      "86de3d566666: Verifying Checksum\n",
      "86de3d566666: Download complete\n",
      "381964195b8b: Verifying Checksum\n",
      "381964195b8b: Download complete\n",
      "e807ad87032f: Verifying Checksum\n",
      "e807ad87032f: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "086b79b77a03: Pull complete\n",
      "4698168f5888: Pull complete\n",
      "86de3d566666: Pull complete\n",
      "30d00d530989: Pull complete\n",
      "69a2bfee9a44: Verifying Checksum\n",
      "69a2bfee9a44: Download complete\n",
      "67cab7d11474: Verifying Checksum\n",
      "67cab7d11474: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "fe1468e51d2b: Download complete\n",
      "999747c8e1ca: Verifying Checksum\n",
      "999747c8e1ca: Download complete\n",
      "a9d25440a572: Verifying Checksum\n",
      "a9d25440a572: Download complete\n",
      "ee75ae25ade1: Verifying Checksum\n",
      "ee75ae25ade1: Download complete\n",
      "e92bc58784f1: Verifying Checksum\n",
      "e92bc58784f1: Download complete\n",
      "8f0d2639aefc: Verifying Checksum\n",
      "8f0d2639aefc: Download complete\n",
      "11646adc2850: Download complete\n",
      "14c7723c1bbe: Verifying Checksum\n",
      "14c7723c1bbe: Download complete\n",
      "6252b7e4a35a: Verifying Checksum\n",
      "6252b7e4a35a: Download complete\n",
      "ae96ea101185: Verifying Checksum\n",
      "ae96ea101185: Download complete\n",
      "8553e38f9d3b: Verifying Checksum\n",
      "8553e38f9d3b: Download complete\n",
      "b4375d47e797: Verifying Checksum\n",
      "b4375d47e797: Download complete\n",
      "b13c015c05f0: Verifying Checksum\n",
      "b13c015c05f0: Download complete\n",
      "906cdf1c6b78: Verifying Checksum\n",
      "906cdf1c6b78: Download complete\n",
      "d70342317ce5: Verifying Checksum\n",
      "d70342317ce5: Download complete\n",
      "acea7e9af8f8: Verifying Checksum\n",
      "acea7e9af8f8: Download complete\n",
      "0c557f25f33e: Download complete\n",
      "32eed1f081f7: Verifying Checksum\n",
      "32eed1f081f7: Download complete\n",
      "80c2cbe5e4a8: Verifying Checksum\n",
      "80c2cbe5e4a8: Download complete\n",
      "85c3c971789d: Verifying Checksum\n",
      "85c3c971789d: Download complete\n",
      "fa58d293bde3: Verifying Checksum\n",
      "fa58d293bde3: Download complete\n",
      "a0efb95d3b56: Verifying Checksum\n",
      "a0efb95d3b56: Download complete\n",
      "1bb05b14fb7d: Verifying Checksum\n",
      "1bb05b14fb7d: Download complete\n",
      "4b5c1c89bd3d: Verifying Checksum\n",
      "4b5c1c89bd3d: Download complete\n",
      "3cebcf201134: Verifying Checksum\n",
      "3cebcf201134: Download complete\n",
      "e9ec5ae321aa: Verifying Checksum\n",
      "e9ec5ae321aa: Download complete\n",
      "69a2bfee9a44: Pull complete\n",
      "381964195b8b: Pull complete\n",
      "fe1468e51d2b: Pull complete\n",
      "e807ad87032f: Pull complete\n",
      "0c557f25f33e: Pull complete\n",
      "67cab7d11474: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "999747c8e1ca: Pull complete\n",
      "e92bc58784f1: Pull complete\n",
      "a9d25440a572: Pull complete\n",
      "ee75ae25ade1: Pull complete\n",
      "b13c015c05f0: Pull complete\n",
      "8f0d2639aefc: Pull complete\n",
      "11646adc2850: Pull complete\n",
      "14c7723c1bbe: Pull complete\n",
      "6252b7e4a35a: Pull complete\n",
      "ae96ea101185: Pull complete\n",
      "8553e38f9d3b: Pull complete\n",
      "b4375d47e797: Pull complete\n",
      "906cdf1c6b78: Pull complete\n",
      "d70342317ce5: Pull complete\n",
      "acea7e9af8f8: Pull complete\n",
      "e9ec5ae321aa: Pull complete\n",
      "32eed1f081f7: Pull complete\n",
      "4b5c1c89bd3d: Pull complete\n",
      "80c2cbe5e4a8: Pull complete\n",
      "85c3c971789d: Pull complete\n",
      "fa58d293bde3: Pull complete\n",
      "a0efb95d3b56: Pull complete\n",
      "1bb05b14fb7d: Pull complete\n",
      "3cebcf201134: Pull complete\n",
      "Digest: sha256:5d99c562fcc484d1bd104abab75267f37586d23a97a5a6e354c7828a1d2dfb83\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.8.0\n",
      " ---> 864d5f66048d\n",
      "Step 2/4 : RUN pip install -U pip\n",
      " ---> Running in 0dfac0969e9e\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.1.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n",
      "      2.1/2.1 MB 39.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.1\n",
      "    Uninstalling pip-22.1.1:\n",
      "      Successfully uninstalled pip-22.1.1\n",
      "Successfully installed pip-22.1.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 0dfac0969e9e\n",
      " ---> 6626c72f06d3\n",
      "Step 3/4 : RUN pip install google-cloud-aiplatform==1.14.0 google-cloud-aiplatform[tensorboard]\n",
      " ---> Running in 57f7a24bd324\n",
      "Collecting google-cloud-aiplatform==1.14.0\n",
      "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
      "      1.9/1.9 MB 36.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-aiplatform[tensorboard] in /opt/conda/lib/python3.7/site-packages (1.13.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.31.5)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.4.1)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.19.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (3.20.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.20.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (2.2.1)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (20.9)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (2.34.3)\n",
      "Requirement already satisfied: tensorflow<3.0.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (2.8.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (59.8.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.16.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.35.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2022.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.56.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.46.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.2.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.14.0) (0.12.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform==1.14.0) (2.4.7)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.0.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.8.0rc0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.6.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.5.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (4.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.8.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.8.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (14.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.37.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (1.1.2)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.5.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.8.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.2.0)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.13.0\n",
      "    Uninstalling google-cloud-aiplatform-1.13.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.13.0\n",
      "Successfully installed google-cloud-aiplatform-1.14.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 57f7a24bd324\n",
      " ---> 0c01591c987b\n",
      "Step 4/4 : RUN pip install pytest kfp==1.8.12 google-cloud-bigquery==2.34.3 google-cloud-bigquery-storage==2.13.2 google-cloud-aiplatform==1.14.0\n",
      " ---> Running in 2ad06ed59bd7\n",
      "Collecting pytest\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "      297.0/297.0 kB 11.0 MB/s eta 0:00:00\n",
      "Collecting kfp==1.8.12\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "      301.2/301.2 kB 37.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-cloud-bigquery==2.34.3 in /opt/conda/lib/python3.7/site-packages (2.34.3)\n",
      "Collecting google-cloud-bigquery-storage==2.13.2\n",
      "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "      180.2/180.2 kB 26.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-aiplatform==1.14.0 in /opt/conda/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.0.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (5.4.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.31.5)\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "      106.8/106.8 kB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.12.11)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "      54.3/54.3 kB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "      58.0/58.0 kB 10.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "      56.3/56.3 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (0.1.15)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "      87.7/87.7 kB 15.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (3.20.1)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (3.0.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.9.0)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.27.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (1.46.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.3.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (20.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.2.2)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (1.20.3)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.4.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (20.3.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "      98.7/98.7 kB 17.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (4.11.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp==1.8.12) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.12) (1.14.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.12) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (59.8.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (1.56.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (2022.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.12) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.12) (0.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (4.8)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.14.0) (0.12.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3) (1.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.8.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12) (0.18.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (1.26.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery==2.34.3) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3) (2.0.12)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.12) (0.37.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.12) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12) (3.2.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3) (2.21)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=4ae57b0502ee076c6806094bd99ab61d90a5a62860814f0e4474df8d63cf36dd\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=0fbcd5ab32bce697650e1fc618a5d5bf2a770e8761649e6f21907ae690078f61\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=cb684bcc68e439caaf28a79d5ef3061624c8509c43fbd4af6cd8f8e1f9517c97\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=673f937a4c2b3775e0ff0de365fdfca886899c2bfd2d7b7ab082c07f2678445b\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, iniconfig, typer, tabulate, strip-hints, py, fire, docstring-parser, Deprecated, requests-toolbelt, kfp-server-api, jsonschema, pytest, google-cloud-storage, google-cloud-bigquery-storage, kfp\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.2.0\n",
      "    Uninstalling typing_extensions-4.2.0:\n",
      "      Successfully uninstalled typing_extensions-4.2.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.5.1\n",
      "    Uninstalling jsonschema-4.5.1:\n",
      "      Successfully uninstalled jsonschema-4.5.1\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.2.1\n",
      "    Uninstalling google-cloud-storage-2.2.1:\n",
      "      Successfully uninstalled google-cloud-storage-2.2.1\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.13.1\n",
      "    Uninstalling google-cloud-bigquery-storage-2.13.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.13.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 docstring-parser-0.14.1 fire-0.4.0 google-cloud-bigquery-storage-2.13.2 google-cloud-storage-2.1.0 iniconfig-1.1.1 jsonschema-3.2.0 kfp-1.8.12 kfp-server-api-1.8.2 py-1.11.0 pytest-7.1.2 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 typer-0.4.1 typing-extensions-3.10.0.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 2ad06ed59bd7\n",
      " ---> 06cdf6877b53\n",
      "Successfully built 06cdf6877b53\n",
      "Successfully tagged europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n",
      "PUSH\n",
      "Pushing europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n",
      "The push refers to repository [europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd]\n",
      "b0ce04659859: Preparing\n",
      "012d696bfe00: Preparing\n",
      "6d47b426a1e8: Preparing\n",
      "849f99ab0557: Preparing\n",
      "003ab0deb210: Preparing\n",
      "051b5111dbe3: Preparing\n",
      "105aac973237: Preparing\n",
      "6b279ee1dea4: Preparing\n",
      "0f5815af70ed: Preparing\n",
      "a439fe54d797: Preparing\n",
      "e5bb7384706a: Preparing\n",
      "529b51f6018a: Preparing\n",
      "4d5391a66f17: Preparing\n",
      "8776dda77d84: Preparing\n",
      "f7bf6100a736: Preparing\n",
      "2427ba19d9ab: Preparing\n",
      "c5d8ddc90738: Preparing\n",
      "f9d66d415903: Preparing\n",
      "0a0b70a03299: Preparing\n",
      "01d285020d37: Preparing\n",
      "ad52cc5ce980: Preparing\n",
      "40d867f1633d: Preparing\n",
      "695cde20e218: Preparing\n",
      "eab9c045ef1b: Preparing\n",
      "ad7c511b31df: Preparing\n",
      "cd9f5c9bd89e: Preparing\n",
      "ca40136e604d: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "08e753b98db4: Preparing\n",
      "8f9243705224: Preparing\n",
      "ba42d5f65b46: Preparing\n",
      "51981f322139: Preparing\n",
      "cbe679dd18e3: Preparing\n",
      "ce07be50029c: Preparing\n",
      "7011392a3aa0: Preparing\n",
      "0cfddc66f231: Preparing\n",
      "a4a375cdde15: Preparing\n",
      "ac5045d5adeb: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "ad52cc5ce980: Waiting\n",
      "40d867f1633d: Waiting\n",
      "695cde20e218: Waiting\n",
      "eab9c045ef1b: Waiting\n",
      "ad7c511b31df: Waiting\n",
      "051b5111dbe3: Waiting\n",
      "cd9f5c9bd89e: Waiting\n",
      "ca40136e604d: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "105aac973237: Waiting\n",
      "6b279ee1dea4: Waiting\n",
      "08e753b98db4: Waiting\n",
      "8f9243705224: Waiting\n",
      "0f5815af70ed: Waiting\n",
      "ba42d5f65b46: Waiting\n",
      "a439fe54d797: Waiting\n",
      "e5bb7384706a: Waiting\n",
      "51981f322139: Waiting\n",
      "cbe679dd18e3: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "ce07be50029c: Waiting\n",
      "7011392a3aa0: Waiting\n",
      "0cfddc66f231: Waiting\n",
      "a4a375cdde15: Waiting\n",
      "ac5045d5adeb: Waiting\n",
      "529b51f6018a: Waiting\n",
      "4d5391a66f17: Waiting\n",
      "c5d8ddc90738: Waiting\n",
      "8776dda77d84: Waiting\n",
      "f9d66d415903: Waiting\n",
      "f7bf6100a736: Waiting\n",
      "2427ba19d9ab: Waiting\n",
      "0a0b70a03299: Waiting\n",
      "01d285020d37: Waiting\n",
      "003ab0deb210: Layer already exists\n",
      "849f99ab0557: Layer already exists\n",
      "051b5111dbe3: Layer already exists\n",
      "105aac973237: Layer already exists\n",
      "6b279ee1dea4: Layer already exists\n",
      "0f5815af70ed: Layer already exists\n",
      "a439fe54d797: Layer already exists\n",
      "e5bb7384706a: Layer already exists\n",
      "529b51f6018a: Layer already exists\n",
      "4d5391a66f17: Layer already exists\n",
      "8776dda77d84: Layer already exists\n",
      "f7bf6100a736: Layer already exists\n",
      "2427ba19d9ab: Layer already exists\n",
      "c5d8ddc90738: Layer already exists\n",
      "b0ce04659859: Pushed\n",
      "012d696bfe00: Pushed\n",
      "6d47b426a1e8: Pushed\n",
      "f9d66d415903: Layer already exists\n",
      "0a0b70a03299: Layer already exists\n",
      "01d285020d37: Layer already exists\n",
      "ad52cc5ce980: Layer already exists\n",
      "40d867f1633d: Layer already exists\n",
      "ad7c511b31df: Layer already exists\n",
      "695cde20e218: Layer already exists\n",
      "eab9c045ef1b: Layer already exists\n",
      "cd9f5c9bd89e: Layer already exists\n",
      "ca40136e604d: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "cbe679dd18e3: Layer already exists\n",
      "08e753b98db4: Layer already exists\n",
      "8f9243705224: Layer already exists\n",
      "ba42d5f65b46: Layer already exists\n",
      "51981f322139: Layer already exists\n",
      "0cfddc66f231: Layer already exists\n",
      "ce07be50029c: Layer already exists\n",
      "7011392a3aa0: Layer already exists\n",
      "a4a375cdde15: Layer already exists\n",
      "ac5045d5adeb: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "latest: digest: sha256:276eb1acaebb72fc71f76a3a0549b0ceb4c1b911e0ee68a023f9e6d812f42bad size: 8519\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                                               STATUS\n",
      "fa254c17-768b-459e-b679-98c9ae4a9b3e  2022-06-29T10:35:41+00:00  3M39S     gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz  europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cp build/Dockerfile.cicd build/Dockerfile\n",
    "!gcloud builds submit --tag $CICD_IMAGE_URI build/. --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9b2af",
   "metadata": {},
   "source": [
    "### Run CI/CD from pipeline deployment using Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00b55593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_REPO_URL=https://github.com/pbalm/mlops-with-vertex-ai.git,_BRANCH=main,_CICD_IMAGE_URI=europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest,_PROJECT=pbalm-cxb-aa,_REGION=europe-west4,_GCS_LOCATION=gs://pbalm-cxb-aa-eu/creditcards/,_TEST_GCS_LOCATION=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests,_BQ_LOCATION=EU,_BQ_DATASET_NAME=vertex_eu,_BQ_TABLE_NAME=creditcards_ml,_DATASET_DISPLAY_NAME=creditcards,_MODEL_DISPLAY_NAME=creditcards-classifier-v02,_CI_TRAIN_LIMIT=1000,_CI_TEST_LIMIT=100,_CI_UPLOAD_MODEL=0,_CI_ACCURACY_THRESHOLD=-0.1,_BEAM_RUNNER=DataflowRunner,_TRAINING_RUNNER=vertex,_TFX_IMAGE_URI=europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8,_PIPELINE_NAME=creditcards-classifier-v02-train-pipeline,_PIPELINES_STORE=gs://pbalm-cxb-aa-eu/creditcards/compiled_pipelines,_SUBNETWORK=https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default,_GCS_BUCKET=pbalm-cxb-aa-eu/cloudbuild,_SERVICE_ACCOUNT=188940921537-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "#REPO_URL = \"https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai.git\" # Change to your github repo.\n",
    "REPO_URL=\"https://github.com/pbalm/mlops-with-vertex-ai.git\"\n",
    "\n",
    "BRANCH = \"main\"\n",
    "\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 1000\n",
    "CI_TEST_LIMIT = 100\n",
    "CI_UPLOAD_MODEL = 0\n",
    "CI_ACCURACY_THRESHOLD = -0.1 # again setting accuracy threshold to negative\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "VERSION = 'tfx-1.8'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINES_STORE = os.path.join(GCS_LOCATION, \"compiled_pipelines\")\n",
    "\n",
    "#TFX_IMAGE_URI = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "#europe-west4-docker.pkg.dev/pbalm-cxb-aa/dataflow/creditcards:latest\n",
    "TFX_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT}/{DATASET_DISPLAY_NAME}/vertex:{VERSION}\"\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={DATAFLOW_REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_BQ_TABLE_NAME={BQ_TABLE_NAME},\\\n",
    "_DATASET_DISPLAY_NAME={DATASET_DISPLAY_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_UPLOAD_MODEL={CI_UPLOAD_MODEL},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINES_STORE={PIPELINES_STORE},\\\n",
    "_SUBNETWORK={DATAFLOW_SUBNETWORK},\\\n",
    "_GCS_BUCKET={BUCKET}/cloudbuild,\\\n",
    "_SERVICE_ACCOUNT={DATAFLOW_SERVICE_ACCOUNT}\\\n",
    "\"\"\"\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ea6ba-0d59-439b-9117-e0fbffa1ca21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --no-source --timeout=60m --config build/pipeline-deployment.yaml --substitutions {SUBSTITUTIONS} --machine-type=e2-highcpu-8"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
