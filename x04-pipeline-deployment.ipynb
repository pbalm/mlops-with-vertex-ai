{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109707f2",
   "metadata": {},
   "source": [
    "# 04 - Test and Deploy Training Pipeline to Vertex Pipelines\n",
    "\n",
    "The purpose of this notebook is to test, deploy, and run the `TFX` pipeline on `Vertex Pipelines`. The notebook covers the following tasks:\n",
    "1. Run the tests locally.\n",
    "2. Run the pipeline using `Vertex Pipelines`\n",
    "3. Execute the pipeline deployment `CI/CD` steps using `Cloud Build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a51af1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e7d80",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3bb184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Tensorflow Version: 1.8.0\n",
      "KFP Version: 1.8.12\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "import tfx.v1 as tfx\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)\n",
    "print(\"KFP Version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bdded",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b4b22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: pbalm-cxb-aa\n",
      "Region: europe-west4\n",
      "Bucket name: pbalm-cxb-aa-eu\n",
      "Service Account: 188940921537-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'pbalm-cxb-aa'\n",
    "REGION = 'europe-west4'\n",
    "CF_REGION = 'europe-west1' # No Cloud Functions in europe-west4\n",
    "BUCKET =  PROJECT + '-eu'\n",
    "SERVICE_ACCOUNT = \"188940921537-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP project id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a9dc9",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a75e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_LOCATION = 'EU'\n",
    "BQ_DATASET_NAME = 'vertex_eu' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'creditcards_ml'\n",
    "\n",
    "VERSION = 'v02'\n",
    "DATASET_DISPLAY_NAME = 'creditcards'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT}/creditcards/{CICD_IMAGE_NAME}\"\n",
    "\n",
    "DATAFLOW_REGION = 'europe-west4'\n",
    "DATAFLOW_SERVICE_ACCOUNT = SERVICE_ACCOUNT\n",
    "DATAFLOW_SUBNETWORK = f'https://www.googleapis.com/compute/v1/projects/{PROJECT}/regions/{REGION}/subnetworks/default'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06be3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'src/raw_schema/.ipynb_checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r src/raw_schema/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44678373",
   "metadata": {},
   "source": [
    "## 1. Run the CICD steps locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68204ed",
   "metadata": {},
   "source": [
    "### Set pipeline configurations for the local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05a1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] =  MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"BQ_TABLE_NAME\"] = BQ_TABLE_NAME\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"1000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"100\"\n",
    "os.environ[\"UPLOAD_MODEL\"] = \"1\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"-0.1\"    # NB Negative accuracy threshold makes no sense - allows everything\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d353e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: pbalm-cxb-aa\n",
      "REGION: europe-west4\n",
      "GCS_LOCATION: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests\n",
      "ARTIFACT_STORE_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: creditcards\n",
      "MODEL_DISPLAY_NAME: creditcards-classifier-v02\n",
      "PIPELINE_NAME: creditcards-classifier-v02-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 1000\n",
      "TEST_LIMIT: 100\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: -0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n",
      "BEAM_RUNNER: DirectRunner\n",
      "SERVICE_ACCOUNT: 188940921537-compute@developer.gserviceaccount.com\n",
      "SUBNETWORK: https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', '--region=europe-west4', '--runner=DirectRunner', '--service_account_email=188940921537-compute@developer.gserviceaccount.com', '--no_use_public_ips', '--subnetwork=https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'europe-west4', 'ai_platform_training_args': {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', 'gcs_location': 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', 'project': 'pbalm-cxb-aa', 'region': 'europe-west4', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: creditcards-classifier-v02-predictions\n",
      "ENABLE_CACHE: 1\n",
      "UPLOAD_MODEL: 1\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d51c12",
   "metadata": {},
   "source": [
    "### Run unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9be84a8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/datasource_utils_tests.py BigQuery Source: pbalm-cxb-aa.vertex_eu.creditcards_ml\n",
      "\u001b[32m.\u001b[0mBigQuery Source: pbalm-cxb-aa.vertex_eu.creditcards_ml\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 6.20s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4358f955",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 3 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/model_tests.py \u001b[32m.\u001b[0m\u001b[33ms\u001b[0m2022-07-04 09:49:18.080678: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 09:49:18.082534: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['V1[0][0]',                     \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'Amount[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           1920        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,033\n",
      "Trainable params: 4,033\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19\n",
      "  /opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "    'nearest': pil_image.NEAREST,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "    'bilinear': pil_image.BILINEAR,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "    'bicubic': pil_image.BICUBIC,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    if hasattr(pil_image, 'HAMMING'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    if hasattr(pil_image, 'BOX'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    if hasattr(pil_image, 'LANCZOS'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m1 skipped\u001b[0m, \u001b[33m\u001b[1m10 warnings\u001b[0m\u001b[33m in 2.66s\u001b[0m\u001b[33m ===================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa00fd5",
   "metadata": {},
   "source": [
    "### Run e2e pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb9aad70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py upload_model: 1\n",
      "Pipeline e2e test artifacts stored in: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests\n",
      "ML metadata store is ready.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/credi\"}\n",
      "Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'GcsModelPusher', 'VertexUploader']\n",
      "Beam pipeline args: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp']\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai/src/preprocessing/transformations.py' (including modules: ['etl', 'transformations']).\n",
      "User module package has hash fingerprint version fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7.\n",
      "Executing: ['/opt/conda/bin/python3.7', '/tmp/tmpe5xjz0cb/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmp131xxmgf', '--dist-dir', '/tmp/tmpi_fnrskr']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmp131xxmgf\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /tmp/tmp131xxmgf\n",
      "copying build/lib/etl.py -> /tmp/tmp131xxmgf\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmp131xxmgf/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp131xxmgf/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7.dist-info/WHEEL\n",
      "creating '/tmp/tmpi_fnrskr/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl' and adding '/tmp/tmp131xxmgf' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7.dist-info/RECORD'\n",
      "removing /tmp/tmp131xxmgf\n",
      "Successfully built user code wheel distribution at 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl'; target user module is 'transformations'.\n",
      "Full user module path is 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl'\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai/src/model_training/runner.py' (including modules: ['trainer', 'runner', 'model', 'defaults', 'exporter', 'data', 'task']).\n",
      "User module package has hash fingerprint version 1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6.\n",
      "Executing: ['/opt/conda/bin/python3.7', '/tmp/tmpadmbc_vv/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpq6o2if3i', '--dist-dir', '/tmp/tmp0j_qmr_c']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying trainer.py -> build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying task.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmpq6o2if3i\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/trainer.py -> /tmp/tmpq6o2if3i\n",
      "copying build/lib/model.py -> /tmp/tmpq6o2if3i\n",
      "copying build/lib/runner.py -> /tmp/tmpq6o2if3i\n",
      "copying build/lib/task.py -> /tmp/tmpq6o2if3i\n",
      "copying build/lib/data.py -> /tmp/tmpq6o2if3i\n",
      "copying build/lib/defaults.py -> /tmp/tmpq6o2if3i\n",
      "copying build/lib/exporter.py -> /tmp/tmpq6o2if3i\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmpq6o2if3i/tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpq6o2if3i/tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6.dist-info/WHEEL\n",
      "creating '/tmp/tmp0j_qmr_c/tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6-py3-none-any.whl' and adding '/tmp/tmpq6o2if3i' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'task.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6.dist-info/RECORD'\n",
      "removing /tmp/tmpq6o2if3i\n",
      "Successfully built user code wheel distribution at 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6-py3-none-any.whl'; target user module is 'runner'.\n",
      "Full user module path is 'runner@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6-py3-none-any.whl'\n",
      "Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"DataTransformer\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"GcsModelPusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"HyperparamsGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.hyperparameters_gen_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelEvaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelTrainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"VertexUploader\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.vertex_model_uploader_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"mlmd.sqllite\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"mlmd.sqllite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "Component BaselineModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"BaselineModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.BaselineModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"model\"\n",
      "      input_keys: \"model_blessing\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type Model is not found in MLMD.\n",
      "Artifact type ModelBlessing is not found in MLMD.\n",
      "Component BaselineModelResolver is finished.\n",
      "Component HyperparamsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"steps_per_epoch\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 2\n",
      "Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={}, output_dict=defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}), exec_properties={'batch_size': 512, 'steps_per_epoch': 1, 'learning_rate': 0.001, 'num_epochs': 1, 'hidden_units': '128,128'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir/2022-07-04T09:53:14.614839', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"steps_per_epoch\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-07-04T09:53:14.614839')\n",
      "Hyperparameters: {'num_epochs': 1, 'steps_per_epoch': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Hyperparameters are written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2/hyperparameters.json\n",
      "Cleaning up stateless execution info.\n",
      "Execution 2 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 2\n",
      "MetadataStore with DB connection initialized\n",
      "Component HyperparamsGen is finished.\n",
      "Component SchemaImporter is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"SchemaImporter\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/raw_schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an importer node.\n",
      "MetadataStore with DB connection initialized\n",
      "Processing source uri: src/raw_schema, properties: {}, custom_properties: {}\n",
      "Component SchemaImporter is finished.\n",
      "Component TestDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 4\n",
      "Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\"\\n    }\\n  ]\\n}', 'output_file_format': 5, 'output_data_format': 6, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir/2022-07-04T09:53:14.614839', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-07-04T09:53:14.614839')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpg7wa595b/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpg7wa595b/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpg7wa595b/build/tfx/setup.log\n",
      "E0704 09:53:20.747302974   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmpg7wa595b/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa5d6e58d40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa5d6e58e60> ====================\n",
      "==================== <function pack_combiners at 0x7fa5d6e5c3b0> ====================\n",
      "==================== <function lift_combiners at 0x7fa5d6e5c440> ====================\n",
      "==================== <function expand_sdf at 0x7fa5d6e5c5f0> ====================\n",
      "==================== <function expand_gbk at 0x7fa5d6e5c680> ====================\n",
      "==================== <function sink_flattens at 0x7fa5d6e5c7a0> ====================\n",
      "==================== <function greedily_fuse at 0x7fa5d6e5c830> ====================\n",
      "==================== <function read_to_impulse at 0x7fa5d6e5c8c0> ====================\n",
      "==================== <function impulse_to_input at 0x7fa5d6e5c950> ====================\n",
      "==================== <function sort_stages at 0x7fa5d6e5cb90> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa5d6e5ccb0> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa5d6e5cb00> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa5d6e5cc20> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa5d4541310> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "E0704 09:53:23.220753672   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Setting socket default timeout to 60 seconds.\n",
      "socket default timeout is 60.0 seconds.\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Using location 'EU' from table <TableReference\n",
      " datasetId: 'vertex_eu'\n",
      " projectId: 'pbalm-cxb-aa'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM vertex_eu.creditcards_ml \n",
      "    WHERE ML_use = 'TEST'\n",
      "    LIMIT 100\n",
      "Dataset pbalm-cxb-aa:beam_temp_dataset_3b7f60533d1e49df91e7c93660ebf406 does not exist so we will create it as temporary with location=EU\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_f7db9cec-0_1656928405_21'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_f7db9cec-0_1656928405_21\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_f7db9cec-0_1656928411_700'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_f7db9cec-0_1656928411_700\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04146003723144531 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03401350975036621 seconds.\n",
      "Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03696393966674805 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03526425361633301 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031946659088134766 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 4 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 4\n",
      "MetadataStore with DB connection initialized\n",
      "Component TestDataGen is finished.\n",
      "Component TrainDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 5\n",
      "Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_file_format': 5, 'output_data_format': 6, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\"\\n    }\\n  ]\\n}', 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir/2022-07-04T09:53:14.614839', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-07-04T09:53:14.614839')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmp3x9cov_1/build/tfx\n",
      "Generating a temp setup file at /tmp/tmp3x9cov_1/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmp3x9cov_1/build/tfx/setup.log\n",
      "E0704 09:53:43.228138796   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmp3x9cov_1/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa5d6e58d40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa5d6e58e60> ====================\n",
      "==================== <function pack_combiners at 0x7fa5d6e5c3b0> ====================\n",
      "==================== <function lift_combiners at 0x7fa5d6e5c440> ====================\n",
      "==================== <function expand_sdf at 0x7fa5d6e5c5f0> ====================\n",
      "==================== <function expand_gbk at 0x7fa5d6e5c680> ====================\n",
      "==================== <function sink_flattens at 0x7fa5d6e5c7a0> ====================\n",
      "==================== <function greedily_fuse at 0x7fa5d6e5c830> ====================\n",
      "==================== <function read_to_impulse at 0x7fa5d6e5c8c0> ====================\n",
      "==================== <function impulse_to_input at 0x7fa5d6e5c950> ====================\n",
      "==================== <function sort_stages at 0x7fa5d6e5cb90> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa5d6e5ccb0> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa5d6e5cb00> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa5d6e5cc20> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa5d44187d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Using location 'EU' from table <TableReference\n",
      " datasetId: 'vertex_eu'\n",
      " projectId: 'pbalm-cxb-aa'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM vertex_eu.creditcards_ml \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 1000\n",
      "Dataset pbalm-cxb-aa:beam_temp_dataset_d2d2077838e24c1ba58f8a609e26e88f does not exist so we will create it as temporary with location=EU\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_ed561114-4_1656928428_722'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_ed561114-4_1656928428_722\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_ed561114-4_1656928433_103'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_ed561114-4_1656928433_103\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.032974958419799805 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03942275047302246 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.042806148529052734 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0377655029296875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.038805484771728516 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03695321083068848 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03322958946228027 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03633904457092285 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 5 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 5\n",
      "MetadataStore with DB connection initialized\n",
      "Component TrainDataGen is finished.\n",
      "Component WarmstartModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"WarmstartModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.WarmstartModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"latest_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"latest_model\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component WarmstartModelResolver is finished.\n",
      "Component StatisticsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 7\n",
      "Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1656928444059\n",
      "last_update_time_since_epoch: 1656928444059\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir/2022-07-04T09:53:14.614839', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-07-04T09:53:14.614839')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmp2_wfqkdf/build/tfx\n",
      "Generating a temp setup file at /tmp/tmp2_wfqkdf/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmp2_wfqkdf/build/tfx/setup.log\n",
      "E0704 09:54:06.814695405   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmp2_wfqkdf/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Generating statistics for split train.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03082728385925293 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split train written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-train.\n",
      "Generating statistics for split eval.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.034391164779663086 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split eval written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-eval.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa5d6e58d40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa5d6e58e60> ====================\n",
      "==================== <function pack_combiners at 0x7fa5d6e5c3b0> ====================\n",
      "==================== <function lift_combiners at 0x7fa5d6e5c440> ====================\n",
      "==================== <function expand_sdf at 0x7fa5d6e5c5f0> ====================\n",
      "==================== <function expand_gbk at 0x7fa5d6e5c680> ====================\n",
      "==================== <function sink_flattens at 0x7fa5d6e5c7a0> ====================\n",
      "==================== <function greedily_fuse at 0x7fa5d6e5c830> ====================\n",
      "==================== <function read_to_impulse at 0x7fa5d6e5c8c0> ====================\n",
      "==================== <function impulse_to_input at 0x7fa5d6e5c950> ====================\n",
      "==================== <function sort_stages at 0x7fa5d6e5cb90> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa5d6e5ccb0> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa5d6e5cb00> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa5d6e5cc20> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa5cdf936d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035559654235839844 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03250312805175781 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03939199447631836 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033376216888427734 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.038323163986206055 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03309440612792969 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 7 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 7\n",
      "MetadataStore with DB connection initialized\n",
      "Component StatisticsGen is finished.\n",
      "Component ExampleValidator is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 8\n",
      "Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'statistics': [Artifact(artifact: id: 5\n",
      "type_id: 22\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:StatisticsGen:statistics:0\"\n",
      "create_time_since_epoch: 1656928453535\n",
      "last_update_time_since_epoch: 1656928453535\n",
      ", artifact_type: id: 22\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1656928397661\n",
      "last_update_time_since_epoch: 1656928397661\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir/2022-07-04T09:53:14.614839', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-07-04T09:53:14.614839')\n",
      "Validating schema against the computed statistics for split train.\n",
      "Validation complete for split train. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-train.\n",
      "Validating schema against the computed statistics for split eval.\n",
      "Validation complete for split eval. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-eval.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 8 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 8\n",
      "MetadataStore with DB connection initialized\n",
      "Component ExampleValidator is finished.\n",
      "Component DataTransformer is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 9\n",
      "Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1656928397661\n",
      "last_update_time_since_epoch: 1656928397661\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")], 'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1656928444059\n",
      "last_update_time_since_epoch: 1656928444059\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/updated_analyzer_cache/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:updated_analyzer_cache:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:updated_analyzer_cache:0\"\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:post_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:post_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_anomalies/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:post_transform_anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:post_transform_anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'pre_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:pre_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:pre_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:post_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:post_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:pre_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:pre_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:transform_graph:0\"\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-07-04T09:53:14.614839:DataTransformer:transformed_examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'disable_statistics': 0, 'custom_config': 'null', 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}', 'force_tf_compat_v1': 0, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir/2022-07-04T09:53:14.614839', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-04T09:53:14.614839\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-04T09:53:14.614839\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-07-04T09:53:14.614839')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpgzsnmn87/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpgzsnmn87/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpgzsnmn87/build/tfx/setup.log\n",
      "Added --extra_package=/tmp/tmpgzsnmn87/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Installing '/tmp/tmp5syf7e9q/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmp5zlabieo', '/tmp/tmp5syf7e9q/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl']\n",
      "E0704 09:54:24.789995941   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmp5syf7e9q/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7\n",
      "Successfully installed '/tmp/tmp5syf7e9q/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl'.\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Installing '/tmp/tmpmsr_aiel/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmppieeqglg', '/tmp/tmpmsr_aiel/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl']\n",
      "E0704 09:54:28.016521022   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmpmsr_aiel/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7\n",
      "Successfully installed '/tmp/tmpmsr_aiel/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl'.\n",
      "Installing '/tmp/tmpa1z9m1s3/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmpi5uj2rqh', '/tmp/tmpa1z9m1s3/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl']\n",
      "Processing /tmp/tmpa1z9m1s3/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7\n",
      "Successfully installed '/tmp/tmpa1z9m1s3/tfx_user_code_DataTransformer-0.0+fbce0ce0afea29fb286acdf8cf924e3de88d6a004802c1a9bee126280975cad7-py3-none-any.whl'.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "2022-07-04 09:54:34.971012: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-04 09:54:34.973312: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:326: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03783369064331055 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035501718521118164 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02982354164123535 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa5d6e58d40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa5d6e58e60> ====================\n",
      "==================== <function pack_combiners at 0x7fa5d6e5c3b0> ====================\n",
      "==================== <function lift_combiners at 0x7fa5d6e5c440> ====================\n",
      "==================== <function expand_sdf at 0x7fa5d6e5c5f0> ====================\n",
      "==================== <function expand_gbk at 0x7fa5d6e5c680> ====================\n",
      "==================== <function sink_flattens at 0x7fa5d6e5c7a0> ====================\n",
      "==================== <function greedily_fuse at 0x7fa5d6e5c830> ====================\n",
      "==================== <function read_to_impulse at 0x7fa5d6e5c8c0> ====================\n",
      "==================== <function impulse_to_input at 0x7fa5d6e5c950> ====================\n",
      "==================== <function sort_stages at 0x7fa5d6e5cb90> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa5d6e5ccb0> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa5d6e5cb00> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa5d6e5cc20> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa5cdf20fd0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03899955749511719 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04079937934875488 seconds.\n",
      "2022-07-04 09:55:01.324764: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/35f87874ec2b4c1b9d2193c19a0f90cf/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04490160942077637 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.036179304122924805 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03371453285217285 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03819441795349121 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03500032424926758 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03591108322143555 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03027653694152832 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.046936750411987305 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030707836151123047 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03081512451171875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03688979148864746 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0370326042175293 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03231072425842285 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031586408615112305 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.033182621002197266 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028067350387573242 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03218364715576172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029483795166015625 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03229713439941406 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.036017656326293945 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03331756591796875 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.026042461395263672 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.05491232872009277 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028207063674926758 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.026080846786499023 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0426332950592041 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03181815147399902 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02989673614501953 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03525376319885254 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03584718704223633 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03354454040527344 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04004621505737305 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03691840171813965 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03586721420288086 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.036003828048706055 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035152435302734375 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029868125915527344 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03146529197692871 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030504226684570312 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.032161712646484375 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03180503845214844 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.029671430587768555 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031830787658691406 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.31 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.039968013763427734 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03856921195983887 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03629946708679199 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03345918655395508 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03404974937438965 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030430316925048828 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0335230827331543 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03302192687988281 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03414607048034668 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029385089874267578 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04550790786743164 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029547929763793945 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.036267995834350586 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03510904312133789 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02870464324951172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.023977994918823242 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033235788345336914 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.026489973068237305 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033104896545410156 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02608013153076172 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.039627790451049805 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03098750114440918 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03130626678466797 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.034281015396118164 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04032325744628906 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03154587745666504 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03109455108642578 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028378009796142578 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03087019920349121 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030591726303100586 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.029422283172607422 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03321576118469238 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.030414581298828125 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03295731544494629 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02658367156982422 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.025725364685058594 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035765647888183594 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029079437255859375 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03231072425842285 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02940082550048828 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.026721477508544922 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030410289764404297 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03374838829040527 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030028820037841797 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03449869155883789 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03412985801696777 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.036451101303100586 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02843165397644043 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.027965068817138672 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02858901023864746 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/531fecb0fe4d4c739980c0a671a9980a/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.043843984603881836 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03037118911743164 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03772139549255371 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03572511672973633 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028074264526367188 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.031116962432861328 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0327146053314209 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03619527816772461 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03056812286376953 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Artifact type Model is not found in MLMD.\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "E0704 09:56:01.485962569   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmpva5h6kvf/tfx_user_code_ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Successfully installed tfx-user-code-ModelTrainer-0.0+1a45c2d104bc6a0ca64df05601c8b2e36ace34b0932533e4e33691976d1340e6\n",
      "Runner started...\n",
      "fn_args: FnArgs(working_dir=None, train_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-train/*'], eval_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-eval/*'], train_steps=None, eval_steps=None, schema_path='src/raw_schema/schema.pbtxt', schema_file='src/raw_schema/schema.pbtxt', transform_graph_path='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', transform_output='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7fa5ae7a4ef0>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7fa5ae7a4830>, data_view_decode_fn=None), serving_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving', eval_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-TFMA', model_run_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model_run/10', base_model=None, hyperparameters={'num_epochs': 1, 'steps_per_epoch': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}, custom_config=None)\n",
      "\n",
      "Hyperparameter:\n",
      "{'num_epochs': 1, 'steps_per_epoch': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "\n",
      "Runner executing trainer...\n",
      "Loading tft output from gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['Amount[0][0]',                 \n",
      "                                                                  'V1[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]']                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          3840        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            129         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,481\n",
      "Trainable params: 20,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model training started... steps per epoch = 1\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8309 - accuracy: 0.1270 - auc: 0.1953\n",
      "Model training completed.\n",
      "Runner executing exporter...\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Model export started...\n",
      "Function `serve_features_fn` contains input name(s) Amount, V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26, V27, V28, V3, V4, V5, V6, V7, V8, V9 with unsupported characters which will be renamed to amount, v1, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v2, v20, v21, v22, v23, v24, v25, v26, v27, v28, v3, v4, v5, v6, v7, v8, v9 in the SavedModel.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving/assets\n",
      "Model export completed.\n",
      "Runner completed.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "E0704 09:56:38.114583306   10788 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa59bd03a90> and <keras.engine.input_layer.InputLayer object at 0x7fa5ac5ace90>).\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03759193420410156 seconds.\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa5ac3a5090> and <keras.engine.input_layer.InputLayer object at 0x7fa5ae346790>).\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7fa5d6e58d40> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7fa5d6e58e60> ====================\n",
      "==================== <function pack_combiners at 0x7fa5d6e5c3b0> ====================\n",
      "==================== <function lift_combiners at 0x7fa5d6e5c440> ====================\n",
      "==================== <function expand_sdf at 0x7fa5d6e5c5f0> ====================\n",
      "==================== <function expand_gbk at 0x7fa5d6e5c680> ====================\n",
      "==================== <function sink_flattens at 0x7fa5d6e5c7a0> ====================\n",
      "==================== <function greedily_fuse at 0x7fa5d6e5c830> ====================\n",
      "==================== <function read_to_impulse at 0x7fa5d6e5c8c0> ====================\n",
      "==================== <function impulse_to_input at 0x7fa5d6e5c950> ====================\n",
      "==================== <function sort_stages at 0x7fa5d6e5cb90> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7fa5d6e5ccb0> ====================\n",
      "==================== <function setup_timer_mapping at 0x7fa5d6e5cb00> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7fa5d6e5cc20> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fa59a33df90> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03131222724914551 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03321480751037598 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa5cca38b90> and <keras.engine.input_layer.InputLayer object at 0x7fa5ae17e3d0>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa59a316c90> and <keras.engine.input_layer.InputLayer object at 0x7fa59bed0c90>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa5cdb16d50> and <keras.engine.input_layer.InputLayer object at 0x7fa599fccc50>).\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035770416259765625 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa5ac476a50> and <keras.engine.input_layer.InputLayer object at 0x7fa5d41cbe90>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa59a9a1c10> and <keras.engine.input_layer.InputLayer object at 0x7fa59a4e3f50>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fa5ad289cd0> and <keras.engine.input_layer.InputLayer object at 0x7fa5ad2be810>).\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03412890434265137 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03088545799255371 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03918576240539551 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.030312299728393555 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.06752300262451172 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02752065658569336 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0614926815032959 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.030078887939453125 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03702068328857422 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0341343879699707 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/GcsModelPusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Model registry location: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry/creditcards-classifier-v02/1656928625/\n",
      "Creating Model\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/188940921537/locations/europe-west4/models/1160362999303634944/operations/2487799539226902528\n",
      "Create Model backing LRO: projects/188940921537/locations/europe-west4/models/1160362999303634944/operations/2487799539226902528\n",
      "Model created. Resource name: projects/188940921537/locations/europe-west4/models/1160362999303634944\n",
      "Model created. Resource name: projects/188940921537/locations/europe-west4/models/1160362999303634944\n",
      "To use this Model in another session:\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/188940921537/locations/europe-west4/models/1160362999303634944')\n",
      "model = aiplatform.Model('projects/188940921537/locations/europe-west4/models/1160362999303634944')\n",
      "Model uploaded to Vertex AI: projects/188940921537/locations/europe-west4/models/1160362999303634944\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/VertexUploader/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Model output: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry/creditcards-classifier-v02\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19\n",
      "  /opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "    'nearest': pil_image.NEAREST,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "    'bilinear': pil_image.BILINEAR,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "    'bicubic': pil_image.BICUBIC,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    if hasattr(pil_image, 'HAMMING'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    if hasattr(pil_image, 'BOX'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    if hasattr(pil_image, 'LANCZOS'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485\n",
      "  /opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    from collections import MutableMapping\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318\n",
      "  /opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    from collections import Mapping\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2471: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    temp_location = pcoll.pipeline.options.view_as(\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2473: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2504: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    | _PassThroughThenCleanup(files_to_remove_pcoll))\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m19 warnings\u001b[0m\u001b[33m in 419.38s (0:06:59)\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5704bcb",
   "metadata": {},
   "source": [
    "## 2. Run the training pipeline using Vertex Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e4a2fdb-5b66-4548-ac3c-3cae00e37ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_VERSION='tfx-1.8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7db74",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e2fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = DATAFLOW_REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"85000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"15000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"{REGION}-docker.pkg.dev/{PROJECT}/{DATASET_DISPLAY_NAME}/vertex:{IMG_VERSION}\"\n",
    "os.environ[\"ENABLE_CACHE\"] = \"1\"\n",
    "os.environ[\"SUBNETWORK\"] = DATAFLOW_SUBNETWORK\n",
    "os.environ[\"SERVICE_ACCOUNT\"] = DATAFLOW_SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d83ef31a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: pbalm-cxb-aa\n",
      "REGION: europe-west4\n",
      "GCS_LOCATION: gs://pbalm-cxb-aa-eu/creditcards\n",
      "ARTIFACT_STORE_URI: gs://pbalm-cxb-aa-eu/creditcards/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: creditcards\n",
      "MODEL_DISPLAY_NAME: creditcards-classifier-v02\n",
      "PIPELINE_NAME: creditcards-classifier-v02-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 85000\n",
      "TEST_LIMIT: 15000\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: -0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n",
      "BEAM_RUNNER: DataflowRunner\n",
      "SERVICE_ACCOUNT: 188940921537-compute@developer.gserviceaccount.com\n",
      "SUBNETWORK: https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/temp', '--region=europe-west4', '--runner=DataflowRunner', '--service_account_email=188940921537-compute@developer.gserviceaccount.com', '--no_use_public_ips', '--subnetwork=https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default']\n",
      "TRAINING_RUNNER: vertex\n",
      "VERTEX_TRAINING_ARGS: {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'europe-west4', 'ai_platform_training_args': {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DataflowRunner', 'temporary_dir': 'gs://pbalm-cxb-aa-eu/creditcards/temp', 'gcs_location': 'gs://pbalm-cxb-aa-eu/creditcards/temp', 'project': 'pbalm-cxb-aa', 'region': 'europe-west4', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: creditcards-classifier-v02-predictions\n",
      "ENABLE_CACHE: 1\n",
      "UPLOAD_MODEL: 1\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3164f",
   "metadata": {},
   "source": [
    "### Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a0e729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n"
     ]
    }
   ],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3087da4e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 73 file(s) totalling 2.0 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/pbalm-cxb-aa/locations/global/builds/1155e839-2aa0-4f21-924d-b6c9e9e0738a].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/1155e839-2aa0-4f21-924d-b6c9e9e0738a?project=188940921537].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"1155e839-2aa0-4f21-924d-b6c9e9e0738a\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz#1656506422752768\n",
      "Copying gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz#1656506422752768...\n",
      "/ [1 files][459.4 KiB/459.4 KiB]                                                \n",
      "Operation completed over 1 objects/459.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.114MB\n",
      "Step 1/6 : FROM gcr.io/tfx-oss-public/tfx:1.8.0\n",
      "1.8.0: Pulling from tfx-oss-public/tfx\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "086b79b77a03: Pulling fs layer\n",
      "4698168f5888: Pulling fs layer\n",
      "86de3d566666: Pulling fs layer\n",
      "30d00d530989: Pulling fs layer\n",
      "69a2bfee9a44: Pulling fs layer\n",
      "381964195b8b: Pulling fs layer\n",
      "fe1468e51d2b: Pulling fs layer\n",
      "86de3d566666: Waiting\n",
      "69a2bfee9a44: Waiting\n",
      "381964195b8b: Waiting\n",
      "30d00d530989: Waiting\n",
      "e807ad87032f: Pulling fs layer\n",
      "0c557f25f33e: Pulling fs layer\n",
      "67cab7d11474: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "999747c8e1ca: Pulling fs layer\n",
      "e92bc58784f1: Pulling fs layer\n",
      "a9d25440a572: Pulling fs layer\n",
      "ee75ae25ade1: Pulling fs layer\n",
      "b13c015c05f0: Pulling fs layer\n",
      "fe1468e51d2b: Waiting\n",
      "8f0d2639aefc: Pulling fs layer\n",
      "11646adc2850: Pulling fs layer\n",
      "14c7723c1bbe: Pulling fs layer\n",
      "6252b7e4a35a: Pulling fs layer\n",
      "ae96ea101185: Pulling fs layer\n",
      "8553e38f9d3b: Pulling fs layer\n",
      "e807ad87032f: Waiting\n",
      "67cab7d11474: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "999747c8e1ca: Waiting\n",
      "e92bc58784f1: Waiting\n",
      "a9d25440a572: Waiting\n",
      "ee75ae25ade1: Waiting\n",
      "b13c015c05f0: Waiting\n",
      "b4375d47e797: Pulling fs layer\n",
      "906cdf1c6b78: Pulling fs layer\n",
      "d70342317ce5: Pulling fs layer\n",
      "acea7e9af8f8: Pulling fs layer\n",
      "e9ec5ae321aa: Pulling fs layer\n",
      "32eed1f081f7: Pulling fs layer\n",
      "4b5c1c89bd3d: Pulling fs layer\n",
      "80c2cbe5e4a8: Pulling fs layer\n",
      "85c3c971789d: Pulling fs layer\n",
      "fa58d293bde3: Pulling fs layer\n",
      "8f0d2639aefc: Waiting\n",
      "a0efb95d3b56: Pulling fs layer\n",
      "1bb05b14fb7d: Pulling fs layer\n",
      "11646adc2850: Waiting\n",
      "3cebcf201134: Pulling fs layer\n",
      "14c7723c1bbe: Waiting\n",
      "6252b7e4a35a: Waiting\n",
      "32eed1f081f7: Waiting\n",
      "4b5c1c89bd3d: Waiting\n",
      "ae96ea101185: Waiting\n",
      "8553e38f9d3b: Waiting\n",
      "b4375d47e797: Waiting\n",
      "80c2cbe5e4a8: Waiting\n",
      "906cdf1c6b78: Waiting\n",
      "85c3c971789d: Waiting\n",
      "d70342317ce5: Waiting\n",
      "acea7e9af8f8: Waiting\n",
      "e9ec5ae321aa: Waiting\n",
      "fa58d293bde3: Waiting\n",
      "1bb05b14fb7d: Waiting\n",
      "a0efb95d3b56: Waiting\n",
      "3cebcf201134: Waiting\n",
      "086b79b77a03: Verifying Checksum\n",
      "086b79b77a03: Download complete\n",
      "4698168f5888: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "86de3d566666: Verifying Checksum\n",
      "86de3d566666: Download complete\n",
      "30d00d530989: Verifying Checksum\n",
      "30d00d530989: Download complete\n",
      "381964195b8b: Verifying Checksum\n",
      "381964195b8b: Download complete\n",
      "e807ad87032f: Verifying Checksum\n",
      "e807ad87032f: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "086b79b77a03: Pull complete\n",
      "4698168f5888: Pull complete\n",
      "86de3d566666: Pull complete\n",
      "30d00d530989: Pull complete\n",
      "69a2bfee9a44: Verifying Checksum\n",
      "69a2bfee9a44: Download complete\n",
      "67cab7d11474: Verifying Checksum\n",
      "67cab7d11474: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "fe1468e51d2b: Verifying Checksum\n",
      "fe1468e51d2b: Download complete\n",
      "999747c8e1ca: Verifying Checksum\n",
      "999747c8e1ca: Download complete\n",
      "a9d25440a572: Verifying Checksum\n",
      "a9d25440a572: Download complete\n",
      "e92bc58784f1: Download complete\n",
      "ee75ae25ade1: Verifying Checksum\n",
      "ee75ae25ade1: Download complete\n",
      "8f0d2639aefc: Verifying Checksum\n",
      "8f0d2639aefc: Download complete\n",
      "11646adc2850: Verifying Checksum\n",
      "11646adc2850: Download complete\n",
      "14c7723c1bbe: Verifying Checksum\n",
      "14c7723c1bbe: Download complete\n",
      "6252b7e4a35a: Verifying Checksum\n",
      "6252b7e4a35a: Download complete\n",
      "ae96ea101185: Download complete\n",
      "8553e38f9d3b: Verifying Checksum\n",
      "8553e38f9d3b: Download complete\n",
      "b4375d47e797: Download complete\n",
      "906cdf1c6b78: Verifying Checksum\n",
      "906cdf1c6b78: Download complete\n",
      "d70342317ce5: Verifying Checksum\n",
      "d70342317ce5: Download complete\n",
      "b13c015c05f0: Verifying Checksum\n",
      "b13c015c05f0: Download complete\n",
      "0c557f25f33e: Verifying Checksum\n",
      "0c557f25f33e: Download complete\n",
      "acea7e9af8f8: Verifying Checksum\n",
      "acea7e9af8f8: Download complete\n",
      "32eed1f081f7: Verifying Checksum\n",
      "32eed1f081f7: Download complete\n",
      "80c2cbe5e4a8: Verifying Checksum\n",
      "80c2cbe5e4a8: Download complete\n",
      "85c3c971789d: Verifying Checksum\n",
      "85c3c971789d: Download complete\n",
      "fa58d293bde3: Verifying Checksum\n",
      "fa58d293bde3: Download complete\n",
      "a0efb95d3b56: Download complete\n",
      "1bb05b14fb7d: Verifying Checksum\n",
      "1bb05b14fb7d: Download complete\n",
      "4b5c1c89bd3d: Verifying Checksum\n",
      "4b5c1c89bd3d: Download complete\n",
      "3cebcf201134: Verifying Checksum\n",
      "3cebcf201134: Download complete\n",
      "e9ec5ae321aa: Verifying Checksum\n",
      "e9ec5ae321aa: Download complete\n",
      "69a2bfee9a44: Pull complete\n",
      "381964195b8b: Pull complete\n",
      "fe1468e51d2b: Pull complete\n",
      "e807ad87032f: Pull complete\n",
      "0c557f25f33e: Pull complete\n",
      "67cab7d11474: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "999747c8e1ca: Pull complete\n",
      "e92bc58784f1: Pull complete\n",
      "a9d25440a572: Pull complete\n",
      "ee75ae25ade1: Pull complete\n",
      "b13c015c05f0: Pull complete\n",
      "8f0d2639aefc: Pull complete\n",
      "11646adc2850: Pull complete\n",
      "14c7723c1bbe: Pull complete\n",
      "6252b7e4a35a: Pull complete\n",
      "ae96ea101185: Pull complete\n",
      "8553e38f9d3b: Pull complete\n",
      "b4375d47e797: Pull complete\n",
      "906cdf1c6b78: Pull complete\n",
      "d70342317ce5: Pull complete\n",
      "acea7e9af8f8: Pull complete\n",
      "e9ec5ae321aa: Pull complete\n",
      "32eed1f081f7: Pull complete\n",
      "4b5c1c89bd3d: Pull complete\n",
      "80c2cbe5e4a8: Pull complete\n",
      "85c3c971789d: Pull complete\n",
      "fa58d293bde3: Pull complete\n",
      "a0efb95d3b56: Pull complete\n",
      "1bb05b14fb7d: Pull complete\n",
      "3cebcf201134: Pull complete\n",
      "Digest: sha256:5d99c562fcc484d1bd104abab75267f37586d23a97a5a6e354c7828a1d2dfb83\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.8.0\n",
      " ---> 864d5f66048d\n",
      "Step 2/6 : COPY requirements.txt requirements.txt\n",
      " ---> b1841bc20c02\n",
      "Step 3/6 : RUN pip install -r requirements.txt\n",
      " ---> Running in ee545fe99ac3\n",
      "Collecting kfp==1.8.12\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.2/301.2 kB 10.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-cloud-bigquery==2.34.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (2.34.3)\n",
      "Collecting google-cloud-bigquery-storage==2.13.2\n",
      "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180.2/180.2 kB 26.2 MB/s eta 0:00:00\n",
      "Collecting google-cloud-aiplatform==1.14.0\n",
      "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 63.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-pubsub in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.12.1)\n",
      "Collecting cloudml-hypertune==0.1.0.dev6\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pytest==7.1.2\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 36.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow-data-validation==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: tensorflow-transform==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: tfx==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (1.8.0)\n",
      "Collecting tensorflow-io==0.26.0\n",
      "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/25.9 MB 62.0 MB/s eta 0:00:00\n",
      "Collecting apache-beam[gcp]==2.39.0\n",
      "  Downloading apache_beam-2.39.0-cp37-cp37m-manylinux2010_x86_64.whl (10.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/10.3 MB 91.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.31.5)\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.12.11)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.0/58.0 kB 11.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 11.1 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (0.1.15)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 14.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (3.20.1)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.9.0)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (20.9)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.46.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (4.11.3)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 17.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (20.3.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.3.5)\n",
      "Requirement already satisfied: tfx-bsl<1.9,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.1)\n",
      "Requirement already satisfied: numpy<2,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.21.6)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.3.2)\n",
      "Requirement already satisfied: tensorflow-metadata<1.9,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.8.0)\n",
      "Requirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: joblib<0.15,>=0.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.14.1)\n",
      "Requirement already satisfied: pyarrow<6,>=1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (5.0.0)\n",
      "Requirement already satisfied: pydot<2,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.8.0->-r requirements.txt (line 10)) (1.4.2)\n",
      "Requirement already satisfied: ml-pipelines-sdk==1.8.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: google-apitools<1,>=0.5 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.5.31)\n",
      "Requirement already satisfied: portpicker<2,>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: docker<5,>=4.1 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (4.4.4)\n",
      "Requirement already satisfied: keras-tuner<2,>=1.0.4 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.1.2)\n",
      "Requirement already satisfied: ml-metadata<1.9.0,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: jinja2<4,>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (2.10)\n",
      "Requirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-model-analysis<0.40,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.39.0)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.26.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 83.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2022.1)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.6.8)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.7)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.3.1.1)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.12.3)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.19.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2.7.0)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.4.11)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.15.4)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.0.1)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.6.2)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.1.0)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.2.2)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Using cached google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.16.2)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.19.2)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.2.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub->-r requirements.txt (line 5)) (0.12.4)\n",
      "Requirement already satisfied: grpcio-status>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub->-r requirements.txt (line 5)) (1.46.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.12->-r requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<5,>=4.1->tfx==1.8.0->-r requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.12->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12->-r requirements.txt (line 1)) (59.8.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12->-r requirements.txt (line 1)) (1.56.1)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (0.17.3)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (4.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (4.8)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (6.1.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest==7.1.2->-r requirements.txt (line 7)) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2<4,>=2.7.3->tfx==1.8.0->-r requirements.txt (line 11)) (2.0.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12->-r requirements.txt (line 1)) (0.18.1)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (1.0.4)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (7.33.0)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.8.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (1.26.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from portpicker<2,>=1.3.1->tfx==1.8.0->-r requirements.txt (line 11)) (5.9.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.12->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.5.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (14.0.1)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.0rc0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (7.7.0)\n",
      "Requirement already satisfied: scipy<2,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.7.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.5.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (5.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.12.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (3.0.29)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.13.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (5.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (3.6.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (0.4.8)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (3.3.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.6.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (7.3.1)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.5.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.8.3)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (4.10.0)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (2.15.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.4.11)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.4.5)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.13.3)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (21.3.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (4.11.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (5.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.5.13)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.5.1)\n",
      "Building wheels for collected packages: kfp, cloudml-hypertune, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=c71fb66e549af6b1b2c4f09fb1629c798579eb011edcfbc5be0abc5d4547616f\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=b25afd768c1a363c416f42b4629c7397f960a1e1033eb6951771ace7aeeed673\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=a1578731b2117df07628b2394e2c12aab57d15187287a73dc3b8956a6c65d2a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=536a4d4e26a17737a9b47ebd4b82d1121a2a71f139db1c5c3f3994b122b041c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=635bc5f16babf15eab13f21129c6d3482df54cd658e4c209f3459e141a61c5cc\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp cloudml-hypertune fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, iniconfig, cloudml-hypertune, typer, tensorflow-io-gcs-filesystem, tabulate, strip-hints, py, fire, docstring-parser, Deprecated, tensorflow-io, requests-toolbelt, kfp-server-api, jsonschema, pytest, apache-beam, google-cloud-core, google-cloud-storage, google-cloud-bigquery-storage, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.2.0\n",
      "    Uninstalling typing_extensions-4.2.0:\n",
      "      Successfully uninstalled typing_extensions-4.2.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.23.1\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.23.1:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.23.1\n",
      "  Attempting uninstall: tensorflow-io\n",
      "    Found existing installation: tensorflow-io 0.23.1\n",
      "    Uninstalling tensorflow-io-0.23.1:\n",
      "      Successfully uninstalled tensorflow-io-0.23.1\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.5.1\n",
      "    Uninstalling jsonschema-4.5.1:\n",
      "      Successfully uninstalled jsonschema-4.5.1\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.38.0\n",
      "    Uninstalling apache-beam-2.38.0:\n",
      "      Successfully uninstalled apache-beam-2.38.0\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.2.2\n",
      "    Uninstalling google-cloud-core-2.2.2:\n",
      "      Successfully uninstalled google-cloud-core-2.2.2\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.2.1\n",
      "    Uninstalling google-cloud-storage-2.2.1:\n",
      "      Successfully uninstalled google-cloud-storage-2.2.1\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.13.1\n",
      "    Uninstalling google-cloud-bigquery-storage-2.13.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.13.1\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.13.0\n",
      "    Uninstalling google-cloud-aiplatform-1.13.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.13.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 apache-beam-2.39.0 cloudml-hypertune-0.1.0.dev6 docstring-parser-0.14.1 fire-0.4.0 google-cloud-aiplatform-1.14.0 google-cloud-bigquery-storage-2.13.2 google-cloud-core-1.7.2 google-cloud-storage-2.1.0 iniconfig-1.1.1 jsonschema-3.2.0 kfp-1.8.12 kfp-server-api-1.8.2 py-1.11.0 pytest-7.1.2 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 tensorflow-io-0.26.0 tensorflow-io-gcs-filesystem-0.26.0 typer-0.4.1 typing-extensions-3.10.0.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: There was an error checking the latest version of pip.\n",
      "\u001b[0mRemoving intermediate container ee545fe99ac3\n",
      " ---> 89d9932b4d31\n",
      "Step 4/6 : RUN pip install -U numpy --ignore-installed\n",
      " ---> Running in a77de1a1ae97\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 79.8 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "statsmodels 0.13.2 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
      "\u001b[0mSuccessfully installed numpy-1.21.6\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: There was an error checking the latest version of pip.\n",
      "\u001b[0mRemoving intermediate container a77de1a1ae97\n",
      " ---> 068cee2879fc\n",
      "Step 5/6 : COPY src/ src/\n",
      " ---> 4f3ddd21e0b9\n",
      "Step 6/6 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in 783ae0dadcb8\n",
      "Removing intermediate container 783ae0dadcb8\n",
      " ---> c0a2aa4ee795\n",
      "Successfully built c0a2aa4ee795\n",
      "Successfully tagged europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n",
      "PUSH\n",
      "Pushing europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8\n",
      "The push refers to repository [europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex]\n",
      "91885e7876e8: Preparing\n",
      "2af584f9627d: Preparing\n",
      "3f4b01ba4a1a: Preparing\n",
      "683394350b8e: Preparing\n",
      "849f99ab0557: Preparing\n",
      "003ab0deb210: Preparing\n",
      "051b5111dbe3: Preparing\n",
      "105aac973237: Preparing\n",
      "6b279ee1dea4: Preparing\n",
      "0f5815af70ed: Preparing\n",
      "a439fe54d797: Preparing\n",
      "e5bb7384706a: Preparing\n",
      "529b51f6018a: Preparing\n",
      "4d5391a66f17: Preparing\n",
      "8776dda77d84: Preparing\n",
      "f7bf6100a736: Preparing\n",
      "2427ba19d9ab: Preparing\n",
      "c5d8ddc90738: Preparing\n",
      "f9d66d415903: Preparing\n",
      "003ab0deb210: Waiting\n",
      "051b5111dbe3: Waiting\n",
      "105aac973237: Waiting\n",
      "6b279ee1dea4: Waiting\n",
      "0f5815af70ed: Waiting\n",
      "a439fe54d797: Waiting\n",
      "e5bb7384706a: Waiting\n",
      "529b51f6018a: Waiting\n",
      "4d5391a66f17: Waiting\n",
      "8776dda77d84: Waiting\n",
      "f7bf6100a736: Waiting\n",
      "2427ba19d9ab: Waiting\n",
      "0a0b70a03299: Preparing\n",
      "01d285020d37: Preparing\n",
      "ad52cc5ce980: Preparing\n",
      "40d867f1633d: Preparing\n",
      "695cde20e218: Preparing\n",
      "eab9c045ef1b: Preparing\n",
      "ad7c511b31df: Preparing\n",
      "cd9f5c9bd89e: Preparing\n",
      "ca40136e604d: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "08e753b98db4: Preparing\n",
      "8f9243705224: Preparing\n",
      "ba42d5f65b46: Preparing\n",
      "51981f322139: Preparing\n",
      "cbe679dd18e3: Preparing\n",
      "ce07be50029c: Preparing\n",
      "7011392a3aa0: Preparing\n",
      "0cfddc66f231: Preparing\n",
      "a4a375cdde15: Preparing\n",
      "f9d66d415903: Waiting\n",
      "ac5045d5adeb: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "0a0b70a03299: Waiting\n",
      "ca40136e604d: Waiting\n",
      "01d285020d37: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "08e753b98db4: Waiting\n",
      "ad52cc5ce980: Waiting\n",
      "8f9243705224: Waiting\n",
      "ba42d5f65b46: Waiting\n",
      "40d867f1633d: Waiting\n",
      "51981f322139: Waiting\n",
      "695cde20e218: Waiting\n",
      "cbe679dd18e3: Waiting\n",
      "ce07be50029c: Waiting\n",
      "eab9c045ef1b: Waiting\n",
      "7011392a3aa0: Waiting\n",
      "0cfddc66f231: Waiting\n",
      "ad7c511b31df: Waiting\n",
      "a4a375cdde15: Waiting\n",
      "ac5045d5adeb: Waiting\n",
      "cd9f5c9bd89e: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "849f99ab0557: Layer already exists\n",
      "003ab0deb210: Layer already exists\n",
      "051b5111dbe3: Layer already exists\n",
      "105aac973237: Layer already exists\n",
      "6b279ee1dea4: Layer already exists\n",
      "683394350b8e: Pushed\n",
      "91885e7876e8: Pushed\n",
      "0f5815af70ed: Layer already exists\n",
      "529b51f6018a: Layer already exists\n",
      "a439fe54d797: Layer already exists\n",
      "e5bb7384706a: Layer already exists\n",
      "8776dda77d84: Layer already exists\n",
      "4d5391a66f17: Layer already exists\n",
      "f7bf6100a736: Layer already exists\n",
      "2427ba19d9ab: Layer already exists\n",
      "c5d8ddc90738: Layer already exists\n",
      "f9d66d415903: Layer already exists\n",
      "0a0b70a03299: Layer already exists\n",
      "2af584f9627d: Pushed\n",
      "01d285020d37: Layer already exists\n",
      "ad52cc5ce980: Layer already exists\n",
      "40d867f1633d: Layer already exists\n",
      "ad7c511b31df: Layer already exists\n",
      "eab9c045ef1b: Layer already exists\n",
      "695cde20e218: Layer already exists\n",
      "cd9f5c9bd89e: Layer already exists\n",
      "ca40136e604d: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "08e753b98db4: Layer already exists\n",
      "8f9243705224: Layer already exists\n",
      "ba42d5f65b46: Layer already exists\n",
      "51981f322139: Layer already exists\n",
      "cbe679dd18e3: Layer already exists\n",
      "ce07be50029c: Layer already exists\n",
      "7011392a3aa0: Layer already exists\n",
      "0cfddc66f231: Layer already exists\n",
      "a4a375cdde15: Layer already exists\n",
      "ac5045d5adeb: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "3f4b01ba4a1a: Pushed\n",
      "tfx-1.8: digest: sha256:7853e0b60c1320ce13714cc50d3f1d5c29faf27c7d2f9e1cbae7f02f97f77ebe size: 8726\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                                               STATUS\n",
      "1155e839-2aa0-4f21-924d-b6c9e9e0738a  2022-06-29T12:40:23+00:00  3M57S     gs://pbalm-cxb-aa_cloudbuild/source/1656506421.629663-322d1986978746c396d705a6e3809069.tgz  europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:tfx-1.8  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cp build/Dockerfile.vertex Dockerfile\n",
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155568ca",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c1d5ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://pbalm-cxb-aa-eu/creditcards/tfx_artifacts/creditcards-cla\"}\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "installing to /tmp/tmpbwxcsesb\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /tmp/tmpbwxcsesb\n",
      "copying build/lib/etl.py -> /tmp/tmpbwxcsesb\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmpbwxcsesb/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpbwxcsesb/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL\n",
      "creating '/tmp/tmpucxrdsxa/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' and adding '/tmp/tmpbwxcsesb' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/RECORD'\n",
      "removing /tmp/tmpbwxcsesb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying trainer.py -> build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying task.py -> build/lib\n",
      "installing to /tmp/tmp0no4trhs\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/trainer.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/model.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/runner.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/task.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/data.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/defaults.py -> /tmp/tmp0no4trhs\n",
      "copying build/lib/exporter.py -> /tmp/tmp0no4trhs\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmp0no4trhs/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp0no4trhs/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/WHEEL\n",
      "creating '/tmp/tmpvztlac_0/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7-py3-none-any.whl' and adding '/tmp/tmp0no4trhs' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'task.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/RECORD'\n",
      "removing /tmp/tmp0no4trhs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "pipeline_definition = runner.compile_training_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5e506ad-e26d-4fd6-b235-e52302061ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "vertex_ai.init(project=PROJECT, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "855d0d6b-60cf-4bc4-8598-9a0f45acef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://creditcards-classifier-v02-train-pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 31.6 KiB/ 31.6 KiB]                                                \n",
      "Operation completed over 1 objects/31.6 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/compiled_pipelines/\"\n",
    "!gsutil cp {pipeline_definition_file} {PIPELINES_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb943e",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e625665-e0ac-4978-b820-561018e0adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/creditcards-classifier-v02-train-pipeline-20220630091659?project=188940921537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west4/pipelines/runs/creditcards-classifier-v02-train-pipeline-20220630091659?project=188940921537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/188940921537/locations/europe-west4/pipelineJobs/creditcards-classifier-v02-train-pipeline-20220630091659 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "    \n",
    "job = pipeline_jobs.PipelineJob(template_path = pipeline_definition_file,\n",
    "                                display_name=DATASET_DISPLAY_NAME,\n",
    "                                #enable_caching=False,\n",
    "                                parameter_values={\n",
    "                                    'learning_rate': 0.003,\n",
    "                                    'batch_size': 512,\n",
    "                                    'hidden_units': '128,128',\n",
    "                                    'num_epochs': 30,\n",
    "                                })\n",
    "\n",
    "job.run(sync=False, service_account=DATAFLOW_SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be1fd",
   "metadata": {},
   "source": [
    "### Extracting pipeline runs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37ae4aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:hidden_units</th>\n",
       "      <th>param.input:batch_size</th>\n",
       "      <th>param.input:num_epochs</th>\n",
       "      <th>param.input:learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>256,126</td>\n",
       "      <td>512</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>256,126</td>\n",
       "      <td>512</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>256,126</td>\n",
       "      <td>512</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>256,126</td>\n",
       "      <td>512</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>creditcards-classifier-v02-train-pipeline</td>\n",
       "      <td>creditcards-classifier-v02-train-pipeline-2022...</td>\n",
       "      <td>128,128</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                pipeline_name  \\\n",
       "0   creditcards-classifier-v02-train-pipeline   \n",
       "1   creditcards-classifier-v02-train-pipeline   \n",
       "2   creditcards-classifier-v02-train-pipeline   \n",
       "3   creditcards-classifier-v02-train-pipeline   \n",
       "4   creditcards-classifier-v02-train-pipeline   \n",
       "5   creditcards-classifier-v02-train-pipeline   \n",
       "6   creditcards-classifier-v02-train-pipeline   \n",
       "7   creditcards-classifier-v02-train-pipeline   \n",
       "8   creditcards-classifier-v02-train-pipeline   \n",
       "9   creditcards-classifier-v02-train-pipeline   \n",
       "10  creditcards-classifier-v02-train-pipeline   \n",
       "11  creditcards-classifier-v02-train-pipeline   \n",
       "12  creditcards-classifier-v02-train-pipeline   \n",
       "13  creditcards-classifier-v02-train-pipeline   \n",
       "14  creditcards-classifier-v02-train-pipeline   \n",
       "15  creditcards-classifier-v02-train-pipeline   \n",
       "16  creditcards-classifier-v02-train-pipeline   \n",
       "17  creditcards-classifier-v02-train-pipeline   \n",
       "18  creditcards-classifier-v02-train-pipeline   \n",
       "19  creditcards-classifier-v02-train-pipeline   \n",
       "20  creditcards-classifier-v02-train-pipeline   \n",
       "21  creditcards-classifier-v02-train-pipeline   \n",
       "\n",
       "                                             run_name  \\\n",
       "0   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "1   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "2   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "3   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "4   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "5   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "6   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "7   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "8   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "9   creditcards-classifier-v02-train-pipeline-2022...   \n",
       "10  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "11  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "12  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "13  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "14  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "15  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "16  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "17  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "18  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "19  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "20  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "21  creditcards-classifier-v02-train-pipeline-2022...   \n",
       "\n",
       "   param.input:hidden_units param.input:batch_size param.input:num_epochs  \\\n",
       "0                   128,128                    512                     30   \n",
       "1                   128,128                    512                     30   \n",
       "2                   128,128                    512                     30   \n",
       "3                   128,128                    512                     30   \n",
       "4                   256,126                    512                      7   \n",
       "5                   128,128                    512                     30   \n",
       "6                   128,128                    512                     30   \n",
       "7                   128,128                    512                     30   \n",
       "8                   128,128                    512                     30   \n",
       "9                   256,126                    512                      7   \n",
       "10                  256,126                    512                      7   \n",
       "11                  256,126                    512                      7   \n",
       "12                  128,128                    512                     30   \n",
       "13                  128,128                    512                     30   \n",
       "14                      NaN                    NaN                    NaN   \n",
       "15                      NaN                    NaN                    NaN   \n",
       "16                      NaN                    NaN                    NaN   \n",
       "17                  128,128                    512                     30   \n",
       "18                  128,128                    512                     30   \n",
       "19                  128,128                    512                     30   \n",
       "20                  128,128                    512                     30   \n",
       "21                  128,128                    512                     30   \n",
       "\n",
       "    param.input:learning_rate  \n",
       "0                      0.0030  \n",
       "1                      0.0030  \n",
       "2                      0.0030  \n",
       "3                      0.0030  \n",
       "4                      0.0015  \n",
       "5                      0.0030  \n",
       "6                      0.0030  \n",
       "7                      0.0030  \n",
       "8                      0.0030  \n",
       "9                      0.0015  \n",
       "10                     0.0015  \n",
       "11                     0.0015  \n",
       "12                     0.0030  \n",
       "13                     0.0030  \n",
       "14                        NaN  \n",
       "15                        NaN  \n",
       "16                        NaN  \n",
       "17                     0.0030  \n",
       "18                     0.0030  \n",
       "19                     0.0030  \n",
       "20                     0.0030  \n",
       "21                     0.0030  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
    "pipeline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b454fe9",
   "metadata": {},
   "source": [
    "## 3. Execute the pipeline deployment CI/CD steps in Cloud Build\n",
    "\n",
    "The CI/CD routine is defined in the [pipeline-deployment.yaml](pipeline-deployment.yaml) file, and consists of the following steps:\n",
    "1. Clone the repository to the build environment.\n",
    "2. Run unit tests.\n",
    "3. Run a local e2e test of the pipeline.\n",
    "4. Build the ML container image for pipeline steps.\n",
    "5. Compile the pipeline.\n",
    "6. Upload the pipeline to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29688d4d",
   "metadata": {},
   "source": [
    "### Build CI/CD container Image for Cloud Build\n",
    "\n",
    "This is the runtime environment where the steps of testing and deploying the pipeline will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4759b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n"
     ]
    }
   ],
   "source": [
    "!echo $CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc09c3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 16 file(s) totalling 27.6 KiB before compression.\n",
      "Uploading tarball of [build/.] to [gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/pbalm-cxb-aa/locations/global/builds/fa254c17-768b-459e-b679-98c9ae4a9b3e].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/fa254c17-768b-459e-b679-98c9ae4a9b3e?project=188940921537].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"fa254c17-768b-459e-b679-98c9ae4a9b3e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz#1656498940963890\n",
      "Copying gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz#1656498940963890...\n",
      "/ [1 files][  4.2 KiB/  4.2 KiB]                                                \n",
      "Operation completed over 1 objects/4.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  41.98kB\n",
      "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:1.8.0\n",
      "1.8.0: Pulling from tfx-oss-public/tfx\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "086b79b77a03: Pulling fs layer\n",
      "4698168f5888: Pulling fs layer\n",
      "86de3d566666: Pulling fs layer\n",
      "30d00d530989: Pulling fs layer\n",
      "69a2bfee9a44: Pulling fs layer\n",
      "381964195b8b: Pulling fs layer\n",
      "fe1468e51d2b: Pulling fs layer\n",
      "e807ad87032f: Pulling fs layer\n",
      "0c557f25f33e: Pulling fs layer\n",
      "67cab7d11474: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "999747c8e1ca: Pulling fs layer\n",
      "e92bc58784f1: Pulling fs layer\n",
      "a9d25440a572: Pulling fs layer\n",
      "ee75ae25ade1: Pulling fs layer\n",
      "b13c015c05f0: Pulling fs layer\n",
      "69a2bfee9a44: Waiting\n",
      "8f0d2639aefc: Pulling fs layer\n",
      "11646adc2850: Pulling fs layer\n",
      "14c7723c1bbe: Pulling fs layer\n",
      "6252b7e4a35a: Pulling fs layer\n",
      "381964195b8b: Waiting\n",
      "ae96ea101185: Pulling fs layer\n",
      "8553e38f9d3b: Pulling fs layer\n",
      "b4375d47e797: Pulling fs layer\n",
      "fe1468e51d2b: Waiting\n",
      "906cdf1c6b78: Pulling fs layer\n",
      "d70342317ce5: Pulling fs layer\n",
      "acea7e9af8f8: Pulling fs layer\n",
      "e9ec5ae321aa: Pulling fs layer\n",
      "32eed1f081f7: Pulling fs layer\n",
      "4b5c1c89bd3d: Pulling fs layer\n",
      "80c2cbe5e4a8: Pulling fs layer\n",
      "85c3c971789d: Pulling fs layer\n",
      "fa58d293bde3: Pulling fs layer\n",
      "a0efb95d3b56: Pulling fs layer\n",
      "1bb05b14fb7d: Pulling fs layer\n",
      "3cebcf201134: Pulling fs layer\n",
      "e807ad87032f: Waiting\n",
      "0c557f25f33e: Waiting\n",
      "67cab7d11474: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "999747c8e1ca: Waiting\n",
      "e92bc58784f1: Waiting\n",
      "a9d25440a572: Waiting\n",
      "ee75ae25ade1: Waiting\n",
      "b13c015c05f0: Waiting\n",
      "8f0d2639aefc: Waiting\n",
      "11646adc2850: Waiting\n",
      "14c7723c1bbe: Waiting\n",
      "86de3d566666: Waiting\n",
      "30d00d530989: Waiting\n",
      "6252b7e4a35a: Waiting\n",
      "80c2cbe5e4a8: Waiting\n",
      "ae96ea101185: Waiting\n",
      "8553e38f9d3b: Waiting\n",
      "85c3c971789d: Waiting\n",
      "b4375d47e797: Waiting\n",
      "906cdf1c6b78: Waiting\n",
      "d70342317ce5: Waiting\n",
      "fa58d293bde3: Waiting\n",
      "acea7e9af8f8: Waiting\n",
      "e9ec5ae321aa: Waiting\n",
      "a0efb95d3b56: Waiting\n",
      "32eed1f081f7: Waiting\n",
      "4b5c1c89bd3d: Waiting\n",
      "1bb05b14fb7d: Waiting\n",
      "3cebcf201134: Waiting\n",
      "086b79b77a03: Download complete\n",
      "4698168f5888: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "30d00d530989: Verifying Checksum\n",
      "30d00d530989: Download complete\n",
      "86de3d566666: Verifying Checksum\n",
      "86de3d566666: Download complete\n",
      "381964195b8b: Verifying Checksum\n",
      "381964195b8b: Download complete\n",
      "e807ad87032f: Verifying Checksum\n",
      "e807ad87032f: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "086b79b77a03: Pull complete\n",
      "4698168f5888: Pull complete\n",
      "86de3d566666: Pull complete\n",
      "30d00d530989: Pull complete\n",
      "69a2bfee9a44: Verifying Checksum\n",
      "69a2bfee9a44: Download complete\n",
      "67cab7d11474: Verifying Checksum\n",
      "67cab7d11474: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "fe1468e51d2b: Download complete\n",
      "999747c8e1ca: Verifying Checksum\n",
      "999747c8e1ca: Download complete\n",
      "a9d25440a572: Verifying Checksum\n",
      "a9d25440a572: Download complete\n",
      "ee75ae25ade1: Verifying Checksum\n",
      "ee75ae25ade1: Download complete\n",
      "e92bc58784f1: Verifying Checksum\n",
      "e92bc58784f1: Download complete\n",
      "8f0d2639aefc: Verifying Checksum\n",
      "8f0d2639aefc: Download complete\n",
      "11646adc2850: Download complete\n",
      "14c7723c1bbe: Verifying Checksum\n",
      "14c7723c1bbe: Download complete\n",
      "6252b7e4a35a: Verifying Checksum\n",
      "6252b7e4a35a: Download complete\n",
      "ae96ea101185: Verifying Checksum\n",
      "ae96ea101185: Download complete\n",
      "8553e38f9d3b: Verifying Checksum\n",
      "8553e38f9d3b: Download complete\n",
      "b4375d47e797: Verifying Checksum\n",
      "b4375d47e797: Download complete\n",
      "b13c015c05f0: Verifying Checksum\n",
      "b13c015c05f0: Download complete\n",
      "906cdf1c6b78: Verifying Checksum\n",
      "906cdf1c6b78: Download complete\n",
      "d70342317ce5: Verifying Checksum\n",
      "d70342317ce5: Download complete\n",
      "acea7e9af8f8: Verifying Checksum\n",
      "acea7e9af8f8: Download complete\n",
      "0c557f25f33e: Download complete\n",
      "32eed1f081f7: Verifying Checksum\n",
      "32eed1f081f7: Download complete\n",
      "80c2cbe5e4a8: Verifying Checksum\n",
      "80c2cbe5e4a8: Download complete\n",
      "85c3c971789d: Verifying Checksum\n",
      "85c3c971789d: Download complete\n",
      "fa58d293bde3: Verifying Checksum\n",
      "fa58d293bde3: Download complete\n",
      "a0efb95d3b56: Verifying Checksum\n",
      "a0efb95d3b56: Download complete\n",
      "1bb05b14fb7d: Verifying Checksum\n",
      "1bb05b14fb7d: Download complete\n",
      "4b5c1c89bd3d: Verifying Checksum\n",
      "4b5c1c89bd3d: Download complete\n",
      "3cebcf201134: Verifying Checksum\n",
      "3cebcf201134: Download complete\n",
      "e9ec5ae321aa: Verifying Checksum\n",
      "e9ec5ae321aa: Download complete\n",
      "69a2bfee9a44: Pull complete\n",
      "381964195b8b: Pull complete\n",
      "fe1468e51d2b: Pull complete\n",
      "e807ad87032f: Pull complete\n",
      "0c557f25f33e: Pull complete\n",
      "67cab7d11474: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "999747c8e1ca: Pull complete\n",
      "e92bc58784f1: Pull complete\n",
      "a9d25440a572: Pull complete\n",
      "ee75ae25ade1: Pull complete\n",
      "b13c015c05f0: Pull complete\n",
      "8f0d2639aefc: Pull complete\n",
      "11646adc2850: Pull complete\n",
      "14c7723c1bbe: Pull complete\n",
      "6252b7e4a35a: Pull complete\n",
      "ae96ea101185: Pull complete\n",
      "8553e38f9d3b: Pull complete\n",
      "b4375d47e797: Pull complete\n",
      "906cdf1c6b78: Pull complete\n",
      "d70342317ce5: Pull complete\n",
      "acea7e9af8f8: Pull complete\n",
      "e9ec5ae321aa: Pull complete\n",
      "32eed1f081f7: Pull complete\n",
      "4b5c1c89bd3d: Pull complete\n",
      "80c2cbe5e4a8: Pull complete\n",
      "85c3c971789d: Pull complete\n",
      "fa58d293bde3: Pull complete\n",
      "a0efb95d3b56: Pull complete\n",
      "1bb05b14fb7d: Pull complete\n",
      "3cebcf201134: Pull complete\n",
      "Digest: sha256:5d99c562fcc484d1bd104abab75267f37586d23a97a5a6e354c7828a1d2dfb83\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.8.0\n",
      " ---> 864d5f66048d\n",
      "Step 2/4 : RUN pip install -U pip\n",
      " ---> Running in 0dfac0969e9e\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.1.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 39.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.1\n",
      "    Uninstalling pip-22.1.1:\n",
      "      Successfully uninstalled pip-22.1.1\n",
      "Successfully installed pip-22.1.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 0dfac0969e9e\n",
      " ---> 6626c72f06d3\n",
      "Step 3/4 : RUN pip install google-cloud-aiplatform==1.14.0 google-cloud-aiplatform[tensorboard]\n",
      " ---> Running in 57f7a24bd324\n",
      "Collecting google-cloud-aiplatform==1.14.0\n",
      "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 36.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-aiplatform[tensorboard] in /opt/conda/lib/python3.7/site-packages (1.13.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.31.5)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.4.1)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.19.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (3.20.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.20.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (2.2.1)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (20.9)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (2.34.3)\n",
      "Requirement already satisfied: tensorflow<3.0.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (2.8.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (59.8.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.16.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.35.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2022.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.56.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.46.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.2.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.14.0) (0.12.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform==1.14.0) (2.4.7)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.0.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.8.0rc0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.6.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.5.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (4.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.8.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.8.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (14.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.37.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (1.1.2)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.5.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (3.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.8.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (4.11.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3.0.0dev,>=2.3.0->google-cloud-aiplatform==1.14.0) (3.2.0)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.13.0\n",
      "    Uninstalling google-cloud-aiplatform-1.13.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.13.0\n",
      "Successfully installed google-cloud-aiplatform-1.14.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 57f7a24bd324\n",
      " ---> 0c01591c987b\n",
      "Step 4/4 : RUN pip install pytest kfp==1.8.12 google-cloud-bigquery==2.34.3 google-cloud-bigquery-storage==2.13.2 google-cloud-aiplatform==1.14.0\n",
      " ---> Running in 2ad06ed59bd7\n",
      "Collecting pytest\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 11.0 MB/s eta 0:00:00\n",
      "Collecting kfp==1.8.12\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.2/301.2 kB 37.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-cloud-bigquery==2.34.3 in /opt/conda/lib/python3.7/site-packages (2.34.3)\n",
      "Collecting google-cloud-bigquery-storage==2.13.2\n",
      "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180.2/180.2 kB 26.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-aiplatform==1.14.0 in /opt/conda/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.0.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (5.4.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.31.5)\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (12.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.12.11)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 kB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.0/58.0 kB 10.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (0.1.15)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 15.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (3.20.1)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (3.0.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12) (1.9.0)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.27.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (1.46.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.3.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (20.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (2.2.2)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3) (1.20.3)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0) (1.4.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (20.3.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 17.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (4.11.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp==1.8.12) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.12) (1.14.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.12) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (59.8.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (1.56.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12) (2022.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.12) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.8.12) (0.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12) (4.8)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.14.0) (0.12.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3) (1.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.8.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12) (0.18.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (1.26.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery==2.34.3) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3) (2.0.12)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.12) (0.37.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.8.12) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12) (3.2.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3) (2.21)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=4ae57b0502ee076c6806094bd99ab61d90a5a62860814f0e4474df8d63cf36dd\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=0fbcd5ab32bce697650e1fc618a5d5bf2a770e8761649e6f21907ae690078f61\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=cb684bcc68e439caaf28a79d5ef3061624c8509c43fbd4af6cd8f8e1f9517c97\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=673f937a4c2b3775e0ff0de365fdfca886899c2bfd2d7b7ab082c07f2678445b\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, iniconfig, typer, tabulate, strip-hints, py, fire, docstring-parser, Deprecated, requests-toolbelt, kfp-server-api, jsonschema, pytest, google-cloud-storage, google-cloud-bigquery-storage, kfp\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.2.0\n",
      "    Uninstalling typing_extensions-4.2.0:\n",
      "      Successfully uninstalled typing_extensions-4.2.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.5.1\n",
      "    Uninstalling jsonschema-4.5.1:\n",
      "      Successfully uninstalled jsonschema-4.5.1\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.2.1\n",
      "    Uninstalling google-cloud-storage-2.2.1:\n",
      "      Successfully uninstalled google-cloud-storage-2.2.1\n",
      "  Attempting uninstall: google-cloud-bigquery-storage\n",
      "    Found existing installation: google-cloud-bigquery-storage 2.13.1\n",
      "    Uninstalling google-cloud-bigquery-storage-2.13.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-storage-2.13.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 docstring-parser-0.14.1 fire-0.4.0 google-cloud-bigquery-storage-2.13.2 google-cloud-storage-2.1.0 iniconfig-1.1.1 jsonschema-3.2.0 kfp-1.8.12 kfp-server-api-1.8.2 py-1.11.0 pytest-7.1.2 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 typer-0.4.1 typing-extensions-3.10.0.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 2ad06ed59bd7\n",
      " ---> 06cdf6877b53\n",
      "Successfully built 06cdf6877b53\n",
      "Successfully tagged europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n",
      "PUSH\n",
      "Pushing europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n",
      "The push refers to repository [europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd]\n",
      "b0ce04659859: Preparing\n",
      "012d696bfe00: Preparing\n",
      "6d47b426a1e8: Preparing\n",
      "849f99ab0557: Preparing\n",
      "003ab0deb210: Preparing\n",
      "051b5111dbe3: Preparing\n",
      "105aac973237: Preparing\n",
      "6b279ee1dea4: Preparing\n",
      "0f5815af70ed: Preparing\n",
      "a439fe54d797: Preparing\n",
      "e5bb7384706a: Preparing\n",
      "529b51f6018a: Preparing\n",
      "4d5391a66f17: Preparing\n",
      "8776dda77d84: Preparing\n",
      "f7bf6100a736: Preparing\n",
      "2427ba19d9ab: Preparing\n",
      "c5d8ddc90738: Preparing\n",
      "f9d66d415903: Preparing\n",
      "0a0b70a03299: Preparing\n",
      "01d285020d37: Preparing\n",
      "ad52cc5ce980: Preparing\n",
      "40d867f1633d: Preparing\n",
      "695cde20e218: Preparing\n",
      "eab9c045ef1b: Preparing\n",
      "ad7c511b31df: Preparing\n",
      "cd9f5c9bd89e: Preparing\n",
      "ca40136e604d: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "08e753b98db4: Preparing\n",
      "8f9243705224: Preparing\n",
      "ba42d5f65b46: Preparing\n",
      "51981f322139: Preparing\n",
      "cbe679dd18e3: Preparing\n",
      "ce07be50029c: Preparing\n",
      "7011392a3aa0: Preparing\n",
      "0cfddc66f231: Preparing\n",
      "a4a375cdde15: Preparing\n",
      "ac5045d5adeb: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "ad52cc5ce980: Waiting\n",
      "40d867f1633d: Waiting\n",
      "695cde20e218: Waiting\n",
      "eab9c045ef1b: Waiting\n",
      "ad7c511b31df: Waiting\n",
      "051b5111dbe3: Waiting\n",
      "cd9f5c9bd89e: Waiting\n",
      "ca40136e604d: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "105aac973237: Waiting\n",
      "6b279ee1dea4: Waiting\n",
      "08e753b98db4: Waiting\n",
      "8f9243705224: Waiting\n",
      "0f5815af70ed: Waiting\n",
      "ba42d5f65b46: Waiting\n",
      "a439fe54d797: Waiting\n",
      "e5bb7384706a: Waiting\n",
      "51981f322139: Waiting\n",
      "cbe679dd18e3: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "ce07be50029c: Waiting\n",
      "7011392a3aa0: Waiting\n",
      "0cfddc66f231: Waiting\n",
      "a4a375cdde15: Waiting\n",
      "ac5045d5adeb: Waiting\n",
      "529b51f6018a: Waiting\n",
      "4d5391a66f17: Waiting\n",
      "c5d8ddc90738: Waiting\n",
      "8776dda77d84: Waiting\n",
      "f9d66d415903: Waiting\n",
      "f7bf6100a736: Waiting\n",
      "2427ba19d9ab: Waiting\n",
      "0a0b70a03299: Waiting\n",
      "01d285020d37: Waiting\n",
      "003ab0deb210: Layer already exists\n",
      "849f99ab0557: Layer already exists\n",
      "051b5111dbe3: Layer already exists\n",
      "105aac973237: Layer already exists\n",
      "6b279ee1dea4: Layer already exists\n",
      "0f5815af70ed: Layer already exists\n",
      "a439fe54d797: Layer already exists\n",
      "e5bb7384706a: Layer already exists\n",
      "529b51f6018a: Layer already exists\n",
      "4d5391a66f17: Layer already exists\n",
      "8776dda77d84: Layer already exists\n",
      "f7bf6100a736: Layer already exists\n",
      "2427ba19d9ab: Layer already exists\n",
      "c5d8ddc90738: Layer already exists\n",
      "b0ce04659859: Pushed\n",
      "012d696bfe00: Pushed\n",
      "6d47b426a1e8: Pushed\n",
      "f9d66d415903: Layer already exists\n",
      "0a0b70a03299: Layer already exists\n",
      "01d285020d37: Layer already exists\n",
      "ad52cc5ce980: Layer already exists\n",
      "40d867f1633d: Layer already exists\n",
      "ad7c511b31df: Layer already exists\n",
      "695cde20e218: Layer already exists\n",
      "eab9c045ef1b: Layer already exists\n",
      "cd9f5c9bd89e: Layer already exists\n",
      "ca40136e604d: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "cbe679dd18e3: Layer already exists\n",
      "08e753b98db4: Layer already exists\n",
      "8f9243705224: Layer already exists\n",
      "ba42d5f65b46: Layer already exists\n",
      "51981f322139: Layer already exists\n",
      "0cfddc66f231: Layer already exists\n",
      "ce07be50029c: Layer already exists\n",
      "7011392a3aa0: Layer already exists\n",
      "a4a375cdde15: Layer already exists\n",
      "ac5045d5adeb: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "latest: digest: sha256:276eb1acaebb72fc71f76a3a0549b0ceb4c1b911e0ee68a023f9e6d812f42bad size: 8519\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                                               STATUS\n",
      "fa254c17-768b-459e-b679-98c9ae4a9b3e  2022-06-29T10:35:41+00:00  3M39S     gs://pbalm-cxb-aa_cloudbuild/source/1656498940.433089-c6bc070eb85341b9a995fbaa7f18a58b.tgz  europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!cp build/Dockerfile.cicd build/Dockerfile\n",
    "!gcloud builds submit --tag $CICD_IMAGE_URI build/. --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9b2af",
   "metadata": {},
   "source": [
    "### Run CI/CD from pipeline deployment using Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00b55593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_REPO_URL=https://github.com/pbalm/mlops-with-vertex-ai.git,_BRANCH=main,_CICD_IMAGE_URI=europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest,_PROJECT=pbalm-cxb-aa,_REGION=europe-west4,_GCS_LOCATION=gs://pbalm-cxb-aa-eu/creditcards/,_TEST_GCS_LOCATION=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests,_BQ_LOCATION=EU,_BQ_DATASET_NAME=vertex_eu,_BQ_TABLE_NAME=creditcards_ml,_DATASET_DISPLAY_NAME=creditcards,_MODEL_DISPLAY_NAME=creditcards-classifier-v02,_CI_TRAIN_LIMIT=1000,_CI_TEST_LIMIT=100,_CI_UPLOAD_MODEL=0,_CI_ACCURACY_THRESHOLD=-0.1,_BEAM_RUNNER=DataflowRunner,_TRAINING_RUNNER=vertex,_TFX_IMAGE_URI=europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:latest,_PIPELINE_NAME=creditcards-classifier-v02-train-pipeline,_PIPELINES_STORE=gs://pbalm-cxb-aa-eu/creditcards/compiled_pipelines,_SUBNETWORK=https://www.googleapis.com/compute/v1/projects/pbalm-cxb-aa/regions/europe-west4/subnetworks/default,_GCS_BUCKET=pbalm-cxb-aa-eu/cloudbuild,_SERVICE_ACCOUNT=188940921537-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "REPO_URL=\"https://github.com/pbalm/mlops-with-vertex-ai.git\"\n",
    "\n",
    "BRANCH = \"main\"\n",
    "\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 1000\n",
    "CI_TEST_LIMIT = 100\n",
    "CI_UPLOAD_MODEL = 0\n",
    "CI_ACCURACY_THRESHOLD = -0.1 # again setting accuracy threshold to negative\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "VERSION = 'latest'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINES_STORE = os.path.join(GCS_LOCATION, \"compiled_pipelines\")\n",
    "\n",
    "TFX_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT}/{DATASET_DISPLAY_NAME}/vertex:{VERSION}\"\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={DATAFLOW_REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_BQ_TABLE_NAME={BQ_TABLE_NAME},\\\n",
    "_DATASET_DISPLAY_NAME={DATASET_DISPLAY_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_UPLOAD_MODEL={CI_UPLOAD_MODEL},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINES_STORE={PIPELINES_STORE},\\\n",
    "_SUBNETWORK={DATAFLOW_SUBNETWORK},\\\n",
    "_GCS_BUCKET={BUCKET}/cloudbuild,\\\n",
    "_SERVICE_ACCOUNT={DATAFLOW_SERVICE_ACCOUNT}\\\n",
    "\"\"\"\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c79ea6ba-0d59-439b-9117-e0fbffa1ca21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created [https://cloudbuild.googleapis.com/v1/projects/pbalm-cxb-aa/locations/global/builds/22e94189-b4c4-489e-b1c5-d4b7d7f8ff69].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/22e94189-b4c4-489e-b1c5-d4b7d7f8ff69?project=188940921537].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"22e94189-b4c4-489e-b1c5-d4b7d7f8ff69\"\n",
      "\n",
      "FETCHSOURCE\n",
      "BUILD\n",
      "Starting Step #0 - \"Clone Repository\"\n",
      "Step #0 - \"Clone Repository\": Already have image (with digest): gcr.io/cloud-builders/git\n",
      "Step #0 - \"Clone Repository\": Cloning into 'mlops-with-vertex-ai'...\n",
      "Step #0 - \"Clone Repository\": POST git-upload-pack (352 bytes)\n",
      "Step #0 - \"Clone Repository\": POST git-upload-pack (194 bytes)\n",
      "Finished Step #0 - \"Clone Repository\"\n",
      "Starting Step #4 - \"Copy Dockerfile\"\n",
      "Starting Step #1 - \"Unit Test Datasource Utils\"\n",
      "Starting Step #2 - \"Unit Test Model\"\n",
      "Step #4 - \"Copy Dockerfile\": Pulling image: ubuntu\n",
      "Step #2 - \"Unit Test Model\": Pulling image: europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n",
      "Step #1 - \"Unit Test Datasource Utils\": Pulling image: europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n",
      "Step #4 - \"Copy Dockerfile\": Using default tag: latest\n",
      "Step #4 - \"Copy Dockerfile\": latest: Pulling from library/ubuntu\n",
      "Step #4 - \"Copy Dockerfile\": Digest: sha256:b6b83d3c331794420340093eb706a6f152d9c1fa51b262d9bf34594887c2c7ac\n",
      "Step #4 - \"Copy Dockerfile\": Status: Downloaded newer image for ubuntu:latest\n",
      "Step #4 - \"Copy Dockerfile\": docker.io/library/ubuntu:latest\n",
      "Finished Step #4 - \"Copy Dockerfile\"\n",
      "Step #1 - \"Unit Test Datasource Utils\": latest: Pulling from pbalm-cxb-aa/creditcards/cicd\n",
      "Step #1 - \"Unit Test Datasource Utils\": d5fd17ec1767: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 086b79b77a03: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4698168f5888: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 86de3d566666: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 30d00d530989: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 69a2bfee9a44: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 381964195b8b: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": fe1468e51d2b: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": e807ad87032f: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c557f25f33e: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 67cab7d11474: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4f4fb700ef54: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 999747c8e1ca: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": e92bc58784f1: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": a9d25440a572: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": ee75ae25ade1: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": b13c015c05f0: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f0d2639aefc: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 11646adc2850: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14c7723c1bbe: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 6252b7e4a35a: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae96ea101185: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8553e38f9d3b: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": b4375d47e797: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 906cdf1c6b78: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": d70342317ce5: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": acea7e9af8f8: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": e9ec5ae321aa: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 32eed1f081f7: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4b5c1c89bd3d: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80c2cbe5e4a8: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 85c3c971789d: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": fa58d293bde3: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": a0efb95d3b56: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1bb05b14fb7d: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 3cebcf201134: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0f54d0dfb3f: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 467cfababfc1: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": a2c3237bbcba: Pulling fs layer\n",
      "Step #1 - \"Unit Test Datasource Utils\": 86de3d566666: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14c7723c1bbe: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4f4fb700ef54: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 999747c8e1ca: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80c2cbe5e4a8: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 85c3c971789d: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": e92bc58784f1: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": fa58d293bde3: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": a9d25440a572: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 30d00d530989: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f0d2639aefc: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": ee75ae25ade1: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 6252b7e4a35a: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": a0efb95d3b56: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": b13c015c05f0: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 69a2bfee9a44: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 11646adc2850: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae96ea101185: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1bb05b14fb7d: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 381964195b8b: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 67cab7d11474: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8553e38f9d3b: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c557f25f33e: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 3cebcf201134: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": fe1468e51d2b: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": e9ec5ae321aa: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": e807ad87032f: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": b4375d47e797: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 32eed1f081f7: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4b5c1c89bd3d: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0f54d0dfb3f: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": acea7e9af8f8: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": d70342317ce5: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 906cdf1c6b78: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": a2c3237bbcba: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 467cfababfc1: Waiting\n",
      "Step #2 - \"Unit Test Model\": latest: Pulling from pbalm-cxb-aa/creditcards/cicd\n",
      "Step #2 - \"Unit Test Model\": d5fd17ec1767: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 086b79b77a03: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 4698168f5888: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 86de3d566666: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 30d00d530989: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 69a2bfee9a44: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 381964195b8b: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": fe1468e51d2b: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": e807ad87032f: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 0c557f25f33e: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 67cab7d11474: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 4f4fb700ef54: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 999747c8e1ca: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": e92bc58784f1: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": a9d25440a572: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": ee75ae25ade1: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": b13c015c05f0: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 8f0d2639aefc: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 11646adc2850: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 14c7723c1bbe: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 6252b7e4a35a: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": ae96ea101185: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 8553e38f9d3b: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": b4375d47e797: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 906cdf1c6b78: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": d70342317ce5: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": acea7e9af8f8: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": e9ec5ae321aa: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 32eed1f081f7: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 4b5c1c89bd3d: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 80c2cbe5e4a8: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 85c3c971789d: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": fa58d293bde3: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": a0efb95d3b56: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 1bb05b14fb7d: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 3cebcf201134: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": c0f54d0dfb3f: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": 467cfababfc1: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": a2c3237bbcba: Pulling fs layer\n",
      "Step #2 - \"Unit Test Model\": a2c3237bbcba: Waiting\n",
      "Step #2 - \"Unit Test Model\": 11646adc2850: Waiting\n",
      "Step #2 - \"Unit Test Model\": e807ad87032f: Waiting\n",
      "Step #2 - \"Unit Test Model\": 32eed1f081f7: Waiting\n",
      "Step #2 - \"Unit Test Model\": 14c7723c1bbe: Waiting\n",
      "Step #2 - \"Unit Test Model\": 6252b7e4a35a: Waiting\n",
      "Step #2 - \"Unit Test Model\": 86de3d566666: Waiting\n",
      "Step #2 - \"Unit Test Model\": ae96ea101185: Waiting\n",
      "Step #2 - \"Unit Test Model\": 30d00d530989: Waiting\n",
      "Step #2 - \"Unit Test Model\": 8553e38f9d3b: Waiting\n",
      "Step #2 - \"Unit Test Model\": 69a2bfee9a44: Waiting\n",
      "Step #2 - \"Unit Test Model\": b4375d47e797: Waiting\n",
      "Step #2 - \"Unit Test Model\": 381964195b8b: Waiting\n",
      "Step #2 - \"Unit Test Model\": 906cdf1c6b78: Waiting\n",
      "Step #2 - \"Unit Test Model\": fe1468e51d2b: Waiting\n",
      "Step #2 - \"Unit Test Model\": d70342317ce5: Waiting\n",
      "Step #2 - \"Unit Test Model\": acea7e9af8f8: Waiting\n",
      "Step #2 - \"Unit Test Model\": e92bc58784f1: Waiting\n",
      "Step #2 - \"Unit Test Model\": e9ec5ae321aa: Waiting\n",
      "Step #2 - \"Unit Test Model\": 0c557f25f33e: Waiting\n",
      "Step #2 - \"Unit Test Model\": 4f4fb700ef54: Waiting\n",
      "Step #2 - \"Unit Test Model\": 999747c8e1ca: Waiting\n",
      "Step #2 - \"Unit Test Model\": 67cab7d11474: Waiting\n",
      "Step #2 - \"Unit Test Model\": a0efb95d3b56: Waiting\n",
      "Step #2 - \"Unit Test Model\": 4b5c1c89bd3d: Waiting\n",
      "Step #2 - \"Unit Test Model\": 85c3c971789d: Waiting\n",
      "Step #2 - \"Unit Test Model\": fa58d293bde3: Waiting\n",
      "Step #2 - \"Unit Test Model\": 80c2cbe5e4a8: Waiting\n",
      "Step #2 - \"Unit Test Model\": ee75ae25ade1: Waiting\n",
      "Step #2 - \"Unit Test Model\": a9d25440a572: Waiting\n",
      "Step #2 - \"Unit Test Model\": 3cebcf201134: Waiting\n",
      "Step #2 - \"Unit Test Model\": 1bb05b14fb7d: Waiting\n",
      "Step #2 - \"Unit Test Model\": b13c015c05f0: Waiting\n",
      "Step #2 - \"Unit Test Model\": 8f0d2639aefc: Waiting\n",
      "Step #2 - \"Unit Test Model\": c0f54d0dfb3f: Waiting\n",
      "Step #2 - \"Unit Test Model\": 467cfababfc1: Waiting\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4698168f5888: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4698168f5888: Download complete\n",
      "Step #2 - \"Unit Test Model\": 4698168f5888: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 4698168f5888: Download complete\n",
      "Step #2 - \"Unit Test Model\": 086b79b77a03: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 086b79b77a03: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 086b79b77a03: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 086b79b77a03: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 86de3d566666: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 86de3d566666: Download complete\n",
      "Step #2 - \"Unit Test Model\": 86de3d566666: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 86de3d566666: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": d5fd17ec1767: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": d5fd17ec1767: Download complete\n",
      "Step #2 - \"Unit Test Model\": d5fd17ec1767: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": d5fd17ec1767: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": d5fd17ec1767: Pull complete\n",
      "Step #2 - \"Unit Test Model\": d5fd17ec1767: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 086b79b77a03: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 086b79b77a03: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 4698168f5888: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4698168f5888: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 86de3d566666: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 86de3d566666: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 30d00d530989: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 30d00d530989: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 30d00d530989: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 30d00d530989: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 381964195b8b: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 381964195b8b: Download complete\n",
      "Step #2 - \"Unit Test Model\": 381964195b8b: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 381964195b8b: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e807ad87032f: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": e807ad87032f: Download complete\n",
      "Step #2 - \"Unit Test Model\": e807ad87032f: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": e807ad87032f: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 69a2bfee9a44: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 69a2bfee9a44: Download complete\n",
      "Step #2 - \"Unit Test Model\": 69a2bfee9a44: Download complete\n",
      "Step #2 - \"Unit Test Model\": 67cab7d11474: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 67cab7d11474: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 67cab7d11474: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 67cab7d11474: Download complete\n",
      "Step #2 - \"Unit Test Model\": 4f4fb700ef54: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 4f4fb700ef54: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4f4fb700ef54: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4f4fb700ef54: Download complete\n",
      "Step #2 - \"Unit Test Model\": fe1468e51d2b: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": fe1468e51d2b: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": fe1468e51d2b: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": fe1468e51d2b: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 999747c8e1ca: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 999747c8e1ca: Download complete\n",
      "Step #2 - \"Unit Test Model\": 999747c8e1ca: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 999747c8e1ca: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c557f25f33e: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 0c557f25f33e: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 0c557f25f33e: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 0c557f25f33e: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": a9d25440a572: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": a9d25440a572: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": a9d25440a572: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": a9d25440a572: Download complete\n",
      "Step #2 - \"Unit Test Model\": ee75ae25ade1: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": ee75ae25ade1: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e92bc58784f1: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": e92bc58784f1: Download complete\n",
      "Step #2 - \"Unit Test Model\": e92bc58784f1: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": e92bc58784f1: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f0d2639aefc: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8f0d2639aefc: Download complete\n",
      "Step #2 - \"Unit Test Model\": 8f0d2639aefc: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 8f0d2639aefc: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14c7723c1bbe: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 14c7723c1bbe: Download complete\n",
      "Step #2 - \"Unit Test Model\": 14c7723c1bbe: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 14c7723c1bbe: Download complete\n",
      "Step #2 - \"Unit Test Model\": 6252b7e4a35a: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 6252b7e4a35a: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 6252b7e4a35a: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 11646adc2850: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 11646adc2850: Download complete\n",
      "Step #2 - \"Unit Test Model\": 11646adc2850: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 11646adc2850: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": ae96ea101185: Download complete\n",
      "Step #2 - \"Unit Test Model\": ae96ea101185: Download complete\n",
      "Step #2 - \"Unit Test Model\": 8553e38f9d3b: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 8553e38f9d3b: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 8553e38f9d3b: Download complete\n",
      "Step #2 - \"Unit Test Model\": b4375d47e797: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": b4375d47e797: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": b4375d47e797: Download complete\n",
      "Step #2 - \"Unit Test Model\": b4375d47e797: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 906cdf1c6b78: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 906cdf1c6b78: Download complete\n",
      "Step #2 - \"Unit Test Model\": 906cdf1c6b78: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 906cdf1c6b78: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": d70342317ce5: Download complete\n",
      "Step #2 - \"Unit Test Model\": d70342317ce5: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": b13c015c05f0: Download complete\n",
      "Step #2 - \"Unit Test Model\": b13c015c05f0: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": b13c015c05f0: Download complete\n",
      "Step #2 - \"Unit Test Model\": 69a2bfee9a44: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 69a2bfee9a44: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 381964195b8b: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 381964195b8b: Pull complete\n",
      "Step #2 - \"Unit Test Model\": 32eed1f081f7: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 32eed1f081f7: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 32eed1f081f7: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 32eed1f081f7: Download complete\n",
      "Step #2 - \"Unit Test Model\": acea7e9af8f8: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": acea7e9af8f8: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": acea7e9af8f8: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": acea7e9af8f8: Download complete\n",
      "Step #2 - \"Unit Test Model\": 4b5c1c89bd3d: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 4b5c1c89bd3d: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4b5c1c89bd3d: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 4b5c1c89bd3d: Download complete\n",
      "Step #2 - \"Unit Test Model\": 80c2cbe5e4a8: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 80c2cbe5e4a8: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80c2cbe5e4a8: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 80c2cbe5e4a8: Download complete\n",
      "Step #2 - \"Unit Test Model\": 85c3c971789d: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 85c3c971789d: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 85c3c971789d: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 85c3c971789d: Download complete\n",
      "Step #2 - \"Unit Test Model\": fa58d293bde3: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": fa58d293bde3: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": fa58d293bde3: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": fa58d293bde3: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": a0efb95d3b56: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": a0efb95d3b56: Download complete\n",
      "Step #2 - \"Unit Test Model\": a0efb95d3b56: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": a0efb95d3b56: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1bb05b14fb7d: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 1bb05b14fb7d: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 1bb05b14fb7d: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 1bb05b14fb7d: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e9ec5ae321aa: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": e9ec5ae321aa: Download complete\n",
      "Step #2 - \"Unit Test Model\": e9ec5ae321aa: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": e9ec5ae321aa: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 3cebcf201134: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": 3cebcf201134: Download complete\n",
      "Step #2 - \"Unit Test Model\": 3cebcf201134: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 3cebcf201134: Download complete\n",
      "Step #2 - \"Unit Test Model\": c0f54d0dfb3f: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": c0f54d0dfb3f: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0f54d0dfb3f: Verifying Checksum\n",
      "Step #1 - \"Unit Test Datasource Utils\": c0f54d0dfb3f: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 467cfababfc1: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 467cfababfc1: Verifying Checksum\n",
      "Step #2 - \"Unit Test Model\": 467cfababfc1: Download complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": 467cfababfc1: Download complete\n",
      "Step #2 - \"Unit Test Model\": fe1468e51d2b: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": fe1468e51d2b: Pull complete\n",
      "Step #2 - \"Unit Test Model\": e807ad87032f: Pull complete\n",
      "Step #1 - \"Unit Test Datasource Utils\": e807ad87032f: Pull complete\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TestDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TestDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir/2022-06-30T11:07:39.090577', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     base_type: PROCESS\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelEvaluator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-06-30T11:07:39.090577')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmptczcayhz/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmptczcayhz/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmptczcayhz/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:07:57.553038454       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmptczcayhz/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating examples.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.38.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7fe18a084050> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7fe18a084170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7fe18a084680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7fe18a084710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7fe18a0848c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7fe18a084950> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7fe18a084a70> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7fe18a084b00> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7fe18a084b90> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7fe18a084c20> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7fe18a084e60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7fe18a084dd0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7fe18a084ef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fe18679ea50> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Impulse_12)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Map-lambda-at-iobase-py-898-_13))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Setting socket default timeout to 60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": socket default timeout is 60.0 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting refresh to obtain initial access_token\n",
      "Step #3 - \"Local Test E2E Pipeline\": Started BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'EU'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using location 'EU' from table <TableReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  datasetId: 'vertex_eu'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  tableId: 'creditcards_ml'> referenced by query \n",
      "Step #3 - \"Local Test E2E Pipeline\":     SELECT *\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \n",
      "Step #3 - \"Local Test E2E Pipeline\":     EXCEPT (Time, ML_use)\n",
      "Step #3 - \"Local Test E2E Pipeline\":     FROM vertex_eu.creditcards_ml \n",
      "Step #3 - \"Local Test E2E Pipeline\":     WHERE ML_use = 'TEST'\n",
      "Step #3 - \"Local Test E2E Pipeline\":     LIMIT 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dataset pbalm-cxb-aa:beam_temp_dataset_f448ecf5e23744b888d28a6a356c0ca1 does not exist so we will create it as temporary with location=EU\n",
      "Step #3 - \"Local Test E2E Pipeline\": Started BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_d7d585b6-6_1656587282_687'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'EU'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_d7d585b6-6_1656587282_687\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Started BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_d7d585b6-6_1656587288_242'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'EU'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_d7d585b6-6_1656587288_242\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12144899368286133 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((ref_PCollection_PCollection_6_split/Read)+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-PassTh_18))+(ref_AppliedPTransform_InputToRecord-ToTFExample_25))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_SplitData-ParDo-ApplyPartitionFnFn-ParDo-ApplyPartitionFnFn-_28))+(ref_AppliedPTransform_WriteSplit-test-MaybeSerialize_30))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-AddRandomKeys_32))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-ReshufflePerKey-Map-reify_timestamps-_34))+(WriteSplit[test]/Shuffle/ReshufflePerKey/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-Impulse_42)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3229-_43))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-DoOnce-Map-decode-_45))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-InitializeWrite_46))+(ref_PCollection_PCollection_25/Write))+(ref_PCollection_PCollection_26/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:08:17.364219840       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((WriteSplit[test]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-test-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_36))+(ref_AppliedPTransform_WriteSplit-test-Shuffle-RemoveRandomKeys_37))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_47))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-WriteBundles_48))+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-Pair_49))+(WriteSplit[test]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteSplit[test]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-Extract_51))+(ref_PCollection_PCollection_31/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-PreFinalize_52))+(ref_PCollection_PCollection_32/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13073182106018066 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit-test-Write-Write-WriteImpl-FinalizeWrite_53)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12663483619689941 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12522625923156738 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Impulse_6)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-FlatMap-lambda-_7))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Map-decode-_9))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-MapFilesToRemove_10))+(ref_PCollection_PCollection_4/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Impul_20)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-FlatM_21))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Map-d_23))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-Remove_24)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12229633331298828 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples generated.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Step #3 - \"Local Test E2E Pipeline\": Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Step #3 - \"Local Test E2E Pipeline\": Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 4 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TestDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TestDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 4\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component TestDataGen is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component TrainDataGen is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     base_type: PROCESS\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 5\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir/2022-06-30T11:07:39.090577', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     base_type: PROCESS\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"input_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_data_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 6\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"output_file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-06-30T11:07:39.090577')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmp588i9ybc/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmp588i9ybc/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmp588i9ybc/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:08:29.846865931       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmp588i9ybc/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating examples.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.38.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7fe18a084050> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7fe18a084170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7fe18a084680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7fe18a084710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7fe18a0848c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7fe18a084950> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7fe18a084a70> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7fe18a084b00> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7fe18a084b90> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7fe18a084c20> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7fe18a084e60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7fe18a084dd0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7fe18a084ef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fe1866ab2d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-Impulse_67)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3229-_68))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-DoOnce-Map-decode-_70))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-InitializeWrite_71))+(ref_PCollection_PCollection_43/Write))+(ref_PCollection_PCollection_44/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Impulse_12)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-Read-Map-lambda-at-iobase-py-898-_13))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_6_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Started BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'EU'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using location 'EU' from table <TableReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  datasetId: 'vertex_eu'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  tableId: 'creditcards_ml'> referenced by query \n",
      "Step #3 - \"Local Test E2E Pipeline\":     SELECT *\n",
      "Step #3 - \"Local Test E2E Pipeline\":     \n",
      "Step #3 - \"Local Test E2E Pipeline\":     EXCEPT (Time, ML_use)\n",
      "Step #3 - \"Local Test E2E Pipeline\":     FROM vertex_eu.creditcards_ml \n",
      "Step #3 - \"Local Test E2E Pipeline\":     WHERE ML_use = 'UNASSIGNED'\n",
      "Step #3 - \"Local Test E2E Pipeline\":     LIMIT 1000\n",
      "Step #3 - \"Local Test E2E Pipeline\": Dataset pbalm-cxb-aa:beam_temp_dataset_cbcb308abcdf41759aa1936f7dac9b4b does not exist so we will create it as temporary with location=EU\n",
      "Step #3 - \"Local Test E2E Pipeline\": Started BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_630daacd-5_1656587314_715'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'EU'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_630daacd-5_1656587314_715\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Started BigQuery job: <JobReference\n",
      "Step #3 - \"Local Test E2E Pipeline\":  jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_630daacd-5_1656587320_832'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  location: 'EU'\n",
      "Step #3 - \"Local Test E2E Pipeline\":  projectId: 'pbalm-cxb-aa'>\n",
      "Step #3 - \"Local Test E2E Pipeline\":  bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_630daacd-5_1656587320_832\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: RUNNING\n",
      "Step #3 - \"Local Test E2E Pipeline\": Job status: DONE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12699532508850098 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((ref_PCollection_PCollection_6_split/Read)+(InputToRecord/QueryTable/ReadFromBigQuery/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-PassTh_18))+(ref_PCollection_PCollection_9/Write))+(ref_AppliedPTransform_InputToRecord-ToTFExample_25))+(ref_AppliedPTransform_SplitData-ParDo-ApplyPartitionFnFn-ParDo-ApplyPartitionFnFn-_28))+(ref_AppliedPTransform_WriteSplit-eval-MaybeSerialize_55))+(ref_AppliedPTransform_WriteSplit-train-MaybeSerialize_30))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-AddRandomKeys_32))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-ReshufflePerKey-Map-reify_timestamps-_34))+(WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Write))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-AddRandomKeys_57))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-ReshufflePerKey-Map-reify_timestamps-_59))+(WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_61))+(ref_AppliedPTransform_WriteSplit-eval-Shuffle-RemoveRandomKeys_62))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_72))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-WriteBundles_73))+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-Pair_74))+(WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-Extract_76))+(ref_PCollection_PCollection_49/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-PreFinalize_77))+(ref_PCollection_PCollection_50/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1270146369934082 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_43/Read)+(ref_AppliedPTransform_WriteSplit-eval-Write-Write-WriteImpl-FinalizeWrite_78)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12984848022460938 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1270139217376709 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Impulse_6)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-FlatMap-lambda-_7))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-FilesToRemoveImpulse-Map-decode-_9))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-MapFilesToRemove_10))+(ref_PCollection_PCollection_4/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-Impulse_42)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3229-_43))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-DoOnce-Map-decode-_45))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-InitializeWrite_46))+(ref_PCollection_PCollection_26/Write))+(ref_PCollection_PCollection_27/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-train-Shuffle-ReshufflePerKey-FlatMap-restore_timestamps-_36))+(ref_AppliedPTransform_WriteSplit-train-Shuffle-RemoveRandomKeys_37))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_47))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-WriteBundles_48))+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-Pair_49))+(WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-Extract_51))+(ref_PCollection_PCollection_32/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_26/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-PreFinalize_52))+(ref_PCollection_PCollection_33/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12900447845458984 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_26/Read)+(ref_AppliedPTransform_WriteSplit-train-Write-Write-WriteImpl-FinalizeWrite_53)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12764620780944824 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1205906867980957 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Impul_20)+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-FlatM_21))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-Create-Map-d_23))+(ref_AppliedPTransform_InputToRecord-QueryTable-ReadFromBigQuery-_PassThroughThenCleanup-ParDo-Remove_24)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12148714065551758 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples generated.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Step #3 - \"Local Test E2E Pipeline\": Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Step #3 - \"Local Test E2E Pipeline\": Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 5 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 5\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component TrainDataGen is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component WarmstartModelResolver is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.WarmstartModelResolver\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"latest_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             base_type: MODEL\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   resolver_config {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     resolver_steps {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       config_json: \"{}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       input_keys: \"latest_model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Running as an resolver node.\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Artifact type Model is not found in MLMD.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component WarmstartModelResolver is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component StatisticsGen is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     base_type: PROCESS\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 7\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"tfrecords_gzip\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"payload_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1656587335163\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1656587335163\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir/2022-06-30T11:07:39.090577', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     base_type: PROCESS\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-06-30T11:07:39.090577')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpn414cufl/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmpn414cufl/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmpn414cufl/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:09:02.900844348       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmpn414cufl/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating statistics for split train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.15929436683654785 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Statistics for split train written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating statistics for split eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12031936645507812 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Statistics for split eval written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.38.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7fe18a084050> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7fe18a084170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7fe18a084680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7fe18a084710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7fe18a0848c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7fe18a084950> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7fe18a084a70> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7fe18a084b00> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7fe18a084b90> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7fe18a084c20> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7fe18a084e60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7fe18a084dd0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7fe18a084ef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fe185384310> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Impu_146)+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Map-_147))+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_81_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12364721298217773 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12459397315979004 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((((((ref_PCollection_PCollection_81_split/Read)+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-ReadRawRecords-FlattenPCollsFromPatterns_150))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordBeamSource-CollectRawRecordTelemetry-ProfileRawRecords_152))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-RawRecordToRecordBatch-Batch-ParDo-_Glob_156))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-RawRecordToRecordBatch-Decode_157))+(ref_AppliedPTransform_TFXIORead-eval-RawRecordToRecordBatch-CollectRecordBatchTelemetry-ProfileRecor_159))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-GetRecordBatchSize_163))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-KeyWithVoid_178))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-KeyWi_165))+(GenerateStatistics[eval]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-ExtractSlice_181))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_207))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_234))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-RemoveDuplic_183))+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_209))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_235))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/0))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_240))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCom_241))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/1))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_214))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_223))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_225))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_220))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_221))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKUn_230))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-AddSli_248))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_251))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_256))+(ref_PCollection_PCollection_143/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_258)+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_259))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_261))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToList_262))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-GenerateSlicedStatisticsImpl-MergeD_263))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-Map-lambda-at-iobase-py-1130-_274))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-WindowInto-WindowIntoFn-_275))+(WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-Impulse_269)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core_270))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-DoOnce-Map-decode-_272))+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-InitializeWrite_273))+(ref_PCollection_PCollection_151/Write))+(ref_PCollection_PCollection_152/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-WriteBundles_277))+(ref_PCollection_PCollection_156/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_151/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-PreFinalize_278))+(ref_PCollection_PCollection_157/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Imp_7)+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-ReadFromTFRecord-0-Read-Map_8))+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12294459342956543 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1265561580657959 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((((((ref_PCollection_PCollection_2_split/Read)+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-ReadRawRecords-FlattenPCollsFromPatterns_11))+(ref_AppliedPTransform_TFXIORead-train-RawRecordBeamSource-CollectRawRecordTelemetry-ProfileRawRecord_13))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-RawRecordToRecordBatch-Batch-ParDo-_Glo_17))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-RawRecordToRecordBatch-Decode_18))+(ref_AppliedPTransform_TFXIORead-train-RawRecordToRecordBatch-CollectRecordBatchTelemetry-ProfileReco_20))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-GetRecordBatchSize_24))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-KeyWithVoid_39))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-KeyW_26))+(GenerateStatistics[train]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-ExtractSlic_42))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_68))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_95))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-RemoveDupli_44))+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_70))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_96))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/0))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/CombinePerKey(sum)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_75))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_84))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_86))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_81))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_82))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-TopKU_91))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_101))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-RunCo_102))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/1))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Merge))+(GenerateStatistics[train]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-UnKe_31))+(ref_PCollection_PCollection_13/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-UnKey_170))+(ref_PCollection_PCollection_92/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-DoOn_33)+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-DoOn_34))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-DoOn_36))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-Inje_37))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackTotalBytes-IncrementCounter_38)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-DoOnc_172)+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-DoOnc_173))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-DoOnc_175))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-SumTotalBytes-Injec_176))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackTotalBytes-IncrementCounter_177)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-AddSl_109))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_112))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_117))+(ref_PCollection_PCollection_64/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Merge))+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-RemoveDupli_50))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combin_53))+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-RemoveDuplic_189))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combine_192))+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(GenerateStatistics[train]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combin_58))+(ref_PCollection_PCollection_28/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combine_197))+(ref_PCollection_PCollection_107/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combin_60)+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combin_61))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combin_63))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combin_64))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-TrackDistinctSliceKeys-IncrementCo_65)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combine_199)+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combine_200))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combine_202))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-Size-Combine_203))+(ref_AppliedPTransform_GenerateStatistics-eval-RunStatsGenerators-TrackDistinctSliceKeys-IncrementCou_204)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-Impulse_130)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_131))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-DoOnce-Map-decode-_133))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-InitializeWrite_134))+(ref_PCollection_PCollection_72/Write))+(ref_PCollection_PCollection_73/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_119)+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_120))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_122))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-ToLis_123))+(ref_AppliedPTransform_GenerateStatistics-train-RunStatsGenerators-GenerateSlicedStatisticsImpl-Merge_124))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-Map-lambda-at-iobase-py-1130_135))+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-WindowInto-WindowIntoFn-_136))+(WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-WriteBundles_138))+(ref_PCollection_PCollection_77/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_72/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-PreFinalize_139))+(ref_PCollection_PCollection_78/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_72/Read)+(ref_AppliedPTransform_WriteStatsOutput-train-WriteStats-Write-WriteImpl-FinalizeWrite_140)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12453269958496094 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_151/Read)+(ref_AppliedPTransform_WriteStatsOutput-eval-WriteStats-Write-WriteImpl-FinalizeWrite_279)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1266629695892334 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 7 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 7\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component StatisticsGen is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component ExampleValidator is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 8\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'statistics': [Artifact(artifact: id: 5\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 22\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:StatisticsGen:statistics:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1656587354005\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1656587354005\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 22\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'schema': [Artifact(artifact: id: 2\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"src/raw_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1656587268290\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1656587268290\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:ExampleValidator:anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:ExampleValidator:anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir/2022-06-30T11:07:39.090577', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"exclude_splits\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"[]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"StatisticsGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-06-30T11:07:39.090577')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validating schema against the computed statistics for split train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validation complete for split train. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-train.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validating schema against the computed statistics for split eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Validation complete for split eval. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-eval.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateless execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Execution 8 succeeded.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Cleaning up stateful execution info.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:ExampleValidator:anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:ExampleValidator:anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}) for execution 8\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component ExampleValidator is finished.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Component DataTransformer is running.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running launcher for node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.transform.component.Transform\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     base_type: TRANSFORM\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transform_graph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transformed_examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"updated_analyzer_cache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformCache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"custom_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"disable_statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"force_tf_compat_v1\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"module_path\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"splits_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": MetadataStore with DB connection initialized\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution 9\n",
      "Step #3 - \"Local Test E2E Pipeline\": Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"src/raw_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1656587268290\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1656587268290\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 18\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'examples': [Artifact(artifact: id: 4\n",
      "Step #3 - \"Local Test E2E Pipeline\": type_id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"file_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"tfrecords_gzip\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"payload_format\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"tfx_version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"1.8.0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": state: LIVE\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:TrainDataGen:examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": create_time_since_epoch: 1656587335163\n",
      "Step #3 - \"Local Test E2E Pipeline\": last_update_time_since_epoch: 1656587335163\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: id: 20\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}, output_dict=defaultdict(<class 'list'>, {'transform_graph': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:transform_graph:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:transform_graph:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'transformed_examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:transformed_examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:transformed_examples:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'post_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_stats/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:post_transform_stats:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:post_transform_stats:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'pre_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_schema/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:pre_transform_schema:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:pre_transform_schema:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'pre_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_stats/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:pre_transform_stats:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:pre_transform_stats:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'post_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_schema/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:post_transform_schema:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:post_transform_schema:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/updated_analyzer_cache/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:updated_analyzer_cache:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:updated_analyzer_cache:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"TransformCache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": )], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_anomalies/9\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": custom_properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"name\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:post_transform_anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": name: \"creditcards-classifier-v02-train-pipeline:2022-06-30T11:07:39.090577:DataTransformer:post_transform_anomalies:0\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , artifact_type: name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":   value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": )]}), exec_properties={'disable_statistics': 0, 'force_tf_compat_v1': 0, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}', 'custom_config': 'null'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir/2022-06-30T11:07:39.090577', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name: \"tfx.components.transform.component.Transform\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     base_type: TRANSFORM\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   id: \"DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   contexts {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   inputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       channels {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         producer_node_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           id: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"pipeline_run\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"2022-06-30T11:07:39.090577\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         context_queries {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"node\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":               string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         artifact_query {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         output_key: \"result\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       min_count: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_anomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleAnomalies\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"post_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Schema\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"pre_transform_stats\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"ExampleStatistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: STATISTICS\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transform_graph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformGraph\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"transformed_examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"Examples\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"span\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"split_names\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: STRING\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           properties {\n",
      "Step #3 - \"Local Test E2E Pipeline\":             key: \"version\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":             value: INT\n",
      "Step #3 - \"Local Test E2E Pipeline\":           }\n",
      "Step #3 - \"Local Test E2E Pipeline\":           base_type: DATASET\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   outputs {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"updated_analyzer_cache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       artifact_spec {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         type {\n",
      "Step #3 - \"Local Test E2E Pipeline\":           name: \"TransformCache\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":         }\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"custom_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"null\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"disable_statistics\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"force_tf_compat_v1\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         int_value: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"module_path\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   parameters {\n",
      "Step #3 - \"Local Test E2E Pipeline\":     key: \"splits_config\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":       field_value {\n",
      "Step #3 - \"Local Test E2E Pipeline\":         string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":       }\n",
      "Step #3 - \"Local Test E2E Pipeline\":     }\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"ExampleValidator\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"SchemaImporter\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": upstream_nodes: \"TrainDataGen\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": downstream_nodes: \"ModelTrainer\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": execution_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   caching_options {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   }\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": , pipeline_run_id='2022-06-30T11:07:39.090577')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Attempting to infer TFX Python dependency for beam\n",
      "Step #3 - \"Local Test E2E Pipeline\": Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmppx1fwyq3/build/tfx\n",
      "Step #3 - \"Local Test E2E Pipeline\": Generating a temp setup file at /tmp/tmppx1fwyq3/build/tfx/setup.py\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating temporary sdist package, logs available at /tmp/tmppx1fwyq3/build/tfx/setup.log\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:09:44.287657094       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Added --extra_package=/tmp/tmppx1fwyq3/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Step #3 - \"Local Test E2E Pipeline\": udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing '/tmp/tmp6ooagych/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:09:47.936075497       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmpin8hjp_e', '/tmp/tmp6ooagych/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmp6ooagych/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-DataTransformer\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed '/tmp/tmp6ooagych/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:09:50.782669370       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing '/tmp/tmpo91cu4_3/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmptvs330un', '/tmp/tmpo91cu4_3/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmpo91cu4_3/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-DataTransformer\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed '/tmp/tmpo91cu4_3/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing '/tmp/tmptcxeu6ed/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Executing: ['/opt/conda/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmprmykuim4', '/tmp/tmptcxeu6ed/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:09:53.318475949       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmptcxeu6ed/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-DataTransformer\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed '/tmp/tmptcxeu6ed/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-06-30 11:09:56.372338: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-06-30 11:09:56.372401: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-06-30 11:09:56.372422: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a2e5a69dcea4): /proc/driver/nvidia/version does not exist\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-06-30 11:09:56.372659: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "Step #3 - \"Local Test E2E Pipeline\": To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Step #3 - \"Local Test E2E Pipeline\": From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:326: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Instructions for updating:\n",
      "Step #3 - \"Local Test E2E Pipeline\": Use ref() instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.21573162078857422 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12506508827209473 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12186193466186523 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V1 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V2 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V3 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V4 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V5 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V6 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V7 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V8 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V9 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V10 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V11 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V12 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V13 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V14 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V15 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V16 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V17 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V18 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V19 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V20 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V21 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V22 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V23 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V24 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V25 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V26 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V27 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature V28 has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Amount has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Feature Class has a shape dim {\n",
      "Step #3 - \"Local Test E2E Pipeline\":   size: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": }\n",
      "Step #3 - \"Local Test E2E Pipeline\": . Setting to DenseTensor.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.38.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7fe18a084050> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7fe18a084170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7fe18a084680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7fe18a084710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7fe18a0848c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7fe18a084950> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7fe18a084a70> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7fe18a084b00> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7fe18a084b90> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7fe18a084c20> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7fe18a084e60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7fe18a084dd0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7fe18a084ef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fe164698f50> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-PackedCombineMerge-29-Count-CreateSole-Impulse_308)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-Count-CreateSole-FlatMap-lambda-at-core-py-3229-_309))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-Count-CreateSole-Map-decode-_311))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-Count-Count_312)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-Impulse_976)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-FlatMap-_977))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-DoOnce-Map-deco_979))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-InitializeWrite_980))+(ref_PCollection_PCollection_577/Write))+(ref_PCollection_PCollection_578/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-DoOnce-Impulse_1056)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-DoOnce-FlatMap_1057))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-DoOnce-Map-dec_1059))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-InitializeWrit_1060))+(ref_PCollection_PCollection_632/Write))+(ref_PCollection_PCollection_633/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-DoOnce-Impulse_1136)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-DoOnce-FlatMap_1137))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-DoOnce-Map-dec_1139))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-InitializeWrit_1140))+(ref_PCollection_PCollection_687/Write))+(ref_PCollection_PCollection_688/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-DoOnce-Impulse_1216)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-DoOnce-FlatMap_1217))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-DoOnce-Map-dec_1219))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-InitializeWrit_1220))+(ref_PCollection_PCollection_742/Write))+(ref_PCollection_PCollection_743/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-DoOnce-Impulse_1296)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-DoOnce-FlatMap_1297))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-DoOnce-Map-dec_1299))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-InitializeWrit_1300))+(ref_PCollection_PCollection_797/Write))+(ref_PCollection_PCollection_798/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-Impulse_880)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-FlatMap-_881))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-DoOnce-Map-deco_883))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-InitializeWrite_884))+(ref_PCollection_PCollection_511/Write))+(ref_PCollection_PCollection_512/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-Impulse_928)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-FlatMap-_929))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-DoOnce-Map-deco_931))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-InitializeWrite_932))+(ref_PCollection_PCollection_544/Write))+(ref_PCollection_PCollection_545/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-Impulse_1008)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-FlatMap-_1009))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-DoOnce-Map-deco_1011))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-InitializeWrite_1012))+(ref_PCollection_PCollection_599/Write))+(ref_PCollection_PCollection_600/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-DoOnce-Impulse_1088)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-DoOnce-FlatMap_1089))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-DoOnce-Map-dec_1091))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-InitializeWrit_1092))+(ref_PCollection_PCollection_654/Write))+(ref_PCollection_PCollection_655/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-DoOnce-Impulse_1168)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-DoOnce-FlatMap_1169))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-DoOnce-Map-dec_1171))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-InitializeWrit_1172))+(ref_PCollection_PCollection_709/Write))+(ref_PCollection_PCollection_710/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-DoOnce-Impulse_1248)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-DoOnce-FlatMap_1249))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-DoOnce-Map-dec_1251))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-InitializeWrit_1252))+(ref_PCollection_PCollection_764/Write))+(ref_PCollection_PCollection_765/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-DoOnce-Impulse_1328)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-DoOnce-FlatMap_1329))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-DoOnce-Map-dec_1331))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-InitializeWrit_1332))+(ref_PCollection_PCollection_819/Write))+(ref_PCollection_PCollection_820/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-Impulse_896)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-FlatMap-_897))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-DoOnce-Map-deco_899))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-InitializeWrite_900))+(ref_PCollection_PCollection_522/Write))+(ref_PCollection_PCollection_523/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-Impulse_960)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-FlatMap-_961))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-DoOnce-Map-deco_963))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-InitializeWrite_964))+(ref_PCollection_PCollection_566/Write))+(ref_PCollection_PCollection_567/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-Impulse_1040)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-FlatMap_1041))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-DoOnce-Map-dec_1043))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-InitializeWrit_1044))+(ref_PCollection_PCollection_621/Write))+(ref_PCollection_PCollection_622/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-DoOnce-Impulse_1120)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-DoOnce-FlatMap_1121))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-DoOnce-Map-dec_1123))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-InitializeWrit_1124))+(ref_PCollection_PCollection_676/Write))+(ref_PCollection_PCollection_677/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-DoOnce-Impulse_1200)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-DoOnce-FlatMap_1201))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-DoOnce-Map-dec_1203))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-InitializeWrit_1204))+(ref_PCollection_PCollection_731/Write))+(ref_PCollection_PCollection_732/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-DoOnce-Impulse_1280)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-DoOnce-FlatMap_1281))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-DoOnce-Map-dec_1283))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-InitializeWrit_1284))+(ref_PCollection_PCollection_786/Write))+(ref_PCollection_PCollection_787/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-Impulse_912)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-FlatMap-_913))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-DoOnce-Map-deco_915))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-InitializeWrite_916))+(ref_PCollection_PCollection_533/Write))+(ref_PCollection_PCollection_534/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-Impulse_992)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-FlatMap-_993))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-DoOnce-Map-deco_995))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-InitializeWrite_996))+(ref_PCollection_PCollection_588/Write))+(ref_PCollection_PCollection_589/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-DoOnce-Impulse_1072)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-DoOnce-FlatMap_1073))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-DoOnce-Map-dec_1075))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-InitializeWrit_1076))+(ref_PCollection_PCollection_643/Write))+(ref_PCollection_PCollection_644/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-DoOnce-Impulse_1152)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-DoOnce-FlatMap_1153))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-DoOnce-Map-dec_1155))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-InitializeWrit_1156))+(ref_PCollection_PCollection_698/Write))+(ref_PCollection_PCollection_699/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-DoOnce-Impulse_1312)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-DoOnce-FlatMap_1313))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-DoOnce-Map-dec_1315))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-InitializeWrit_1316))+(ref_PCollection_PCollection_808/Write))+(ref_PCollection_PCollection_809/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-DoOnce-Impulse_1232)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-DoOnce-FlatMap_1233))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-DoOnce-Map-dec_1235))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-InitializeWrit_1236))+(ref_PCollection_PCollection_753/Write))+(ref_PCollection_PCollection_754/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-ReadFromT_20)+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-ReadFromT_21))+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_9_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12594366073608398 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.13218092918395996 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-Impulse_67)+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-FlatMap_68))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSole-Map-dec_70))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-CreateSavedModel_71))+(ref_PCollection_PCollection_37/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": 2022-06-30 11:10:41.493477: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/906d97c02ee44258ba339624f124c263/assets\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((((((((((((((((((ref_PCollection_PCollection_9_split/Read)+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-ReadRawRecords-FlattenPC_24))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordBeamSource-CollectRawRecordTelemetr_26))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-RawRecordToRecordBatc_30))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-RawRecordToRecordBatc_31))+(ref_AppliedPTransform_TFXIOReadAndDecode-AnalysisIndex0-RawRecordToRecordBatch-CollectRecordBatchTel_33))+(ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-GetRecordBatchSi_43))+(ref_AppliedPTransform_Analyze-ExtractInputForSavedModel-AnalysisIndex0-Identity_80))+(ref_AppliedPTransform_FlattenAnalysisDatasets_1340))+(ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-SumTotalBytes-Ke_45))+(Analyze/InstrumentInputBytes[AnalysisPCollDict][AnalysisIndex0]/SumTotalBytes/CombinePerKey/Precombine))+(Analyze/InstrumentInputBytes[AnalysisPCollDict][AnalysisIndex0]/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze-ApplySavedModel-Phase0-AnalysisIndex0-ApplySavedModel_82))+(ref_AppliedPTransform_Analyze-ApplySavedModel-Phase0-AnalysisIndex0-ConvertToNumpy_83))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_86))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_89))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Transcode/0))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_90))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Precombine))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Write))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Write/0))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-FilterInternalColumn_1342))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1346))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-K_1361))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1348))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1364))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1391))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1366))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Write))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/0))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1392))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": struct2tensor is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_decision_forests is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_text is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Merge))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_95))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_96))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Transcode/1))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/Flatten/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Precombine))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Read)+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Merge))+(Analyze/PackedCombineAccumulate[ApplySavedModel[Phase0][AnalysisIndex0]]/InitialPackedCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_102))+(ref_PCollection_PCollection_56/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-Impulse_944)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-FlatMap-_945))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-DoOnce-Map-deco_947))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-InitializeWrite_948))+(ref_PCollection_PCollection_555/Write))+(ref_PCollection_PCollection_556/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-Impulse_1024)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-FlatMap-_1025))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-DoOnce-Map-deco_1027))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-InitializeWrite_1028))+(ref_PCollection_PCollection_610/Write))+(ref_PCollection_PCollection_611/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-DoOnce-Impulse_1104)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-DoOnce-FlatMap_1105))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-DoOnce-Map-dec_1107))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-InitializeWrit_1108))+(ref_PCollection_PCollection_665/Write))+(ref_PCollection_PCollection_666/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-DoOnce-Impulse_1184)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-DoOnce-FlatMap_1185))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-DoOnce-Map-dec_1187))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-InitializeWrit_1188))+(ref_PCollection_PCollection_720/Write))+(ref_PCollection_PCollection_721/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-DoOnce-Impulse_1264)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-DoOnce-FlatMap_1265))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-DoOnce-Map-dec_1267))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-InitializeWrit_1268))+(ref_PCollection_PCollection_775/Write))+(ref_PCollection_PCollection_776/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_104)+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_105))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_107))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-InitialP_108))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-AnalysisIndex_117))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1-min_and_max-AnalysisIndex0-Ext_123))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_1-min_and_max-AnalysisIndex0-E_129))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_2-min_and_max-AnalysisIndex0-E_135))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_3-min_and_max-AnalysisIndex0-E_141))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_4-min_and_max-AnalysisIndex0-E_147))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_5-min_and_max-AnalysisIndex0-E_153))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_6-min_and_max-AnalysisIndex0-E_159))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_7-min_and_max-AnalysisIndex0-E_165))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_8-min_and_max-AnalysisIndex0-E_171))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_9-min_and_max-AnalysisIndex0-E_177))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_10-min_and_max-AnalysisIndex0-_183))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_11-min_and_max-AnalysisIndex0-_189))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_12-min_and_max-AnalysisIndex0-_195))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_13-min_and_max-AnalysisIndex0-_201))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_14-min_and_max-AnalysisIndex0-_207))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_15-min_and_max-AnalysisIndex0-_213))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_16-min_and_max-AnalysisIndex0-_219))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_17-min_and_max-AnalysisIndex0-_225))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_18-min_and_max-AnalysisIndex0-_231))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_19-min_and_max-AnalysisIndex0-_237))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_20-min_and_max-AnalysisIndex0-_243))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_21-min_and_max-AnalysisIndex0-_249))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_22-min_and_max-AnalysisIndex0-_255))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_23-min_and_max-AnalysisIndex0-_261))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_24-min_and_max-AnalysisIndex0-_267))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_25-min_and_max-AnalysisIndex0-_273))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_26-min_and_max-AnalysisIndex0-_279))+(ref_AppliedPTransform_Analyze-CacheableCombineAccumulate-scale_to_0_1_27-min_and_max-AnalysisIndex0-_285))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_z_score-mean_and_var-Flatt_119))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_590))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_z_score-mean_and_var-AddKey_121))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/0))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1-min_and_max-Flatten_125))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1-min_and_max-Analys_599))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1-min_and_max-AddKey_127))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/1))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_1-min_and_max-Flatten_131))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_1-min_and_max-Anal_608))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_1-min_and_max-AddKey_133))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/2))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_2-min_and_max-Flatten_137))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_2-min_and_max-Anal_617))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_2-min_and_max-AddKey_139))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/3))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_3-min_and_max-Flatten_143))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_3-min_and_max-Anal_626))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_3-min_and_max-AddKey_145))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/4))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_4-min_and_max-Flatten_149))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_4-min_and_max-Anal_635))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_4-min_and_max-AddKey_151))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/5))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_5-min_and_max-Flatten_155))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_5-min_and_max-Anal_644))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_5-min_and_max-AddKey_157))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/6))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_6-min_and_max-Flatten_161))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_6-min_and_max-Anal_653))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_6-min_and_max-AddKey_163))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/7))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_7-min_and_max-Flatten_167))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_7-min_and_max-Anal_662))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_7-min_and_max-AddKey_169))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/8))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_8-min_and_max-Flatten_173))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_8-min_and_max-Anal_671))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_8-min_and_max-AddKey_175))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/9))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_9-min_and_max-Flatten_179))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_9-min_and_max-Anal_680))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_9-min_and_max-AddKey_181))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/10))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_10-min_and_max-Flatten_185))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_10-min_and_max-Ana_689))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_10-min_and_max-AddKey_187))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/11))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_11-min_and_max-Flatten_191))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_11-min_and_max-Ana_698))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_11-min_and_max-AddKey_193))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/12))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_12-min_and_max-Flatten_197))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_12-min_and_max-Ana_707))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_12-min_and_max-AddKey_199))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/13))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_13-min_and_max-Flatten_203))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_13-min_and_max-Ana_716))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_13-min_and_max-AddKey_205))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/14))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_14-min_and_max-Flatten_209))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_14-min_and_max-Ana_725))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_14-min_and_max-AddKey_211))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/15))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_15-min_and_max-Flatten_215))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_15-min_and_max-Ana_734))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_15-min_and_max-AddKey_217))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/16))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_16-min_and_max-Flatten_221))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_16-min_and_max-Ana_743))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_16-min_and_max-AddKey_223))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/17))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_17-min_and_max-Flatten_227))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_17-min_and_max-Ana_752))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_17-min_and_max-AddKey_229))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/18))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_18-min_and_max-Flatten_233))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_18-min_and_max-Ana_761))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_18-min_and_max-AddKey_235))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/19))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_19-min_and_max-Flatten_239))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_19-min_and_max-Ana_770))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_19-min_and_max-AddKey_241))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/20))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_20-min_and_max-Flatten_245))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_20-min_and_max-Ana_779))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_20-min_and_max-AddKey_247))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/21))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_21-min_and_max-Flatten_251))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_21-min_and_max-Ana_788))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_21-min_and_max-AddKey_253))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/22))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_22-min_and_max-Flatten_257))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_22-min_and_max-Ana_797))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_22-min_and_max-AddKey_259))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/23))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_23-min_and_max-Flatten_263))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_23-min_and_max-Ana_806))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_23-min_and_max-AddKey_265))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/24))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_24-min_and_max-Flatten_269))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_24-min_and_max-Ana_815))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_24-min_and_max-AddKey_271))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/25))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_25-min_and_max-Flatten_275))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_25-min_and_max-Ana_824))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_25-min_and_max-AddKey_277))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/26))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_26-min_and_max-Flatten_281))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_26-min_and_max-Ana_833))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_26-min_and_max-AddKey_283))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/27))+(ref_AppliedPTransform_Analyze-FlattenCache-CacheableCombineMerge-scale_to_0_1_27-min_and_max-Flatten_287))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_27-min_and_max-Ana_842))+(ref_AppliedPTransform_Analyze-AddKey-CacheableCombineMerge-scale_to_0_1_27-min_and_max-AddKey_289))+(Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Write/28))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-WindowInto-Wind_885))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-WindowInto-Wind_901))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-WindowInto-Wind_917))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-WindowInto-Wind_933))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-WindowInto-Wind_949))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-WindowInto-Wind_965))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-WindowInto-Wind_981))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-WindowInto-Wind_997))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-WindowInto-Wind_1013))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-WindowInto-Wind_1029))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-WindowInto-Win_1045))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-WindowInto-Win_1061))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-WindowInto-Win_1077))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-WindowInto-Win_1093))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-WindowInto-Win_1109))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-WindowInto-Win_1125))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-WindowInto-Win_1141))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-WindowInto-Win_1157))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-WindowInto-Win_1173))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-WindowInto-Win_1189))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-WindowInto-Win_1205))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-WindowInto-Win_1221))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-WindowInto-Win_1237))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-WindowInto-Win_1253))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-WindowInto-Win_1269))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-WindowInto-Win_1285))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-WindowInto-Win_1301))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-WindowInto-Win_1317))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-WindowInto-Win_1333))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-WriteBundles_886))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-Pair_887))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-WriteBundles_902))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-Pair_903))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-WriteBundles_918))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-Pair_919))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-WriteBundles_934))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-Pair_935))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-WriteBundles_950))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-Pair_951))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-WriteBundles_966))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-Pair_967))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-WriteBundles_982))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-Pair_983))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex6]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-WriteBundles_998))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-Pair_999))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex7]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-WriteBundles_1014))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-Pair_1015))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex8]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-WriteBundles_1030))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-Pair_1031))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex9]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-WriteBundles_1046))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-Pair_1047))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex10]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-WriteBundles_1062))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-Pair_1063))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex11]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-WriteBundles_1078))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-Pair_1079))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex12]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-WriteBundles_1094))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-Pair_1095))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex13]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-WriteBundles_1110))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-Pair_1111))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex14]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-WriteBundles_1126))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-Pair_1127))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex15]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-WriteBundles_1142))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-Pair_1143))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex16]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-WriteBundles_1158))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-Pair_1159))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex17]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-WriteBundles_1174))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-Pair_1175))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex18]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-WriteBundles_1190))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-Pair_1191))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex19]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-WriteBundles_1206))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-Pair_1207))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex20]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-WriteBundles_1222))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-Pair_1223))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex21]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-WriteBundles_1238))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-Pair_1239))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex22]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-WriteBundles_1254))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-Pair_1255))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex23]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-WriteBundles_1270))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-Pair_1271))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex24]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-WriteBundles_1286))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-Pair_1287))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex25]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-WriteBundles_1302))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-Pair_1303))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex26]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-WriteBundles_1318))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-Pair_1319))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex27]/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-WriteBundles_1334))+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-Pair_1335))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex28]/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((Analyze/FlattenInputForPackedCombineMerge[29]/Flatten/Read)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-MergePackedCombinesGlobally-KeyWithVoid_294))+(Analyze/PackedCombineMerge[29]/MergePackedCombinesGlobally/CombinePerKey/Precombine))+(Analyze/PackedCombineMerge[29]/MergePackedCombinesGlobally/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/PackedCombineMerge[29]/MergePackedCombinesGlobally/CombinePerKey/Group/Read)+(Analyze/PackedCombineMerge[29]/MergePackedCombinesGlobally/CombinePerKey/Merge))+(Analyze/PackedCombineMerge[29]/MergePackedCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-MergePackedCombinesGlobally-UnKey_299))+(ref_PCollection_PCollection_156/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((ref_AppliedPTransform_Analyze-PackedCombineMerge-29-MergePackedCombinesGlobally-DoOnce-Impulse_301)+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-MergePackedCombinesGlobally-DoOnce-FlatMap-lambd_302))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-MergePackedCombinesGlobally-DoOnce-Map-decode-_304))+(ref_AppliedPTransform_Analyze-PackedCombineMerge-29-MergePackedCombinesGlobally-InjectDefault_305))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_27-min_and_max-Extr_314))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_z_score-mean_and_var-Ex_323))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1-min_and_max-Extract_332))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_1-min_and_max-Extra_341))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_2-min_and_max-Extra_350))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_3-min_and_max-Extra_359))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_4-min_and_max-Extra_368))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_5-min_and_max-Extra_377))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_6-min_and_max-Extra_386))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_7-min_and_max-Extra_395))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_8-min_and_max-Extra_404))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_9-min_and_max-Extra_413))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_10-min_and_max-Extr_422))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_11-min_and_max-Extr_431))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_12-min_and_max-Extr_440))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_13-min_and_max-Extr_449))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_14-min_and_max-Extr_458))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_15-min_and_max-Extr_467))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_16-min_and_max-Extr_476))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_17-min_and_max-Extr_485))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_18-min_and_max-Extr_494))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_19-min_and_max-Extr_503))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_20-min_and_max-Extr_512))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_21-min_and_max-Extr_521))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_22-min_and_max-Extr_530))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_23-min_and_max-Extr_539))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_24-min_and_max-Extr_548))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_25-min_and_max-Extr_557))+(ref_AppliedPTransform_Analyze-ExtractFromDict-CacheableCombineMerge-scale_to_0_1_26-min_and_max-Extr_566))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_27_317))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_27-min_and_max-temporary_analyzer_out_319))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_27-min_and_max-temporary_analyzer_out_321))+(ref_PCollection_PCollection_169/Write))+(ref_PCollection_PCollection_170/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_z_scor_326))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score-mean_and_var-temporary_analyzer_o_330))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_z_score-mean_and_var-temporary_analyzer_o_328))+(ref_PCollection_PCollection_175/Write))+(ref_PCollection_PCollection_176/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1-mi_335))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1-min_and_max-temporary_analyzer_output_339))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1-min_and_max-temporary_analyzer_output_337))+(ref_PCollection_PCollection_181/Write))+(ref_PCollection_PCollection_182/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_1-_344))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_1-min_and_max-temporary_analyzer_outp_348))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_1-min_and_max-temporary_analyzer_outp_346))+(ref_PCollection_PCollection_187/Write))+(ref_PCollection_PCollection_188/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_2-_353))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_2-min_and_max-temporary_analyzer_outp_355))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_2-min_and_max-temporary_analyzer_outp_357))+(ref_PCollection_PCollection_193/Write))+(ref_PCollection_PCollection_194/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_3-_362))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_3-min_and_max-temporary_analyzer_outp_366))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_3-min_and_max-temporary_analyzer_outp_364))+(ref_PCollection_PCollection_199/Write))+(ref_PCollection_PCollection_200/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_4-_371))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_4-min_and_max-temporary_analyzer_outp_375))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_4-min_and_max-temporary_analyzer_outp_373))+(ref_PCollection_PCollection_205/Write))+(ref_PCollection_PCollection_206/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_5-_380))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_5-min_and_max-temporary_analyzer_outp_382))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_5-min_and_max-temporary_analyzer_outp_384))+(ref_PCollection_PCollection_211/Write))+(ref_PCollection_PCollection_212/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_6-_389))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_6-min_and_max-temporary_analyzer_outp_391))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_6-min_and_max-temporary_analyzer_outp_393))+(ref_PCollection_PCollection_217/Write))+(ref_PCollection_PCollection_218/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_7-_398))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_7-min_and_max-temporary_analyzer_outp_402))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_7-min_and_max-temporary_analyzer_outp_400))+(ref_PCollection_PCollection_223/Write))+(ref_PCollection_PCollection_224/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_8-_407))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_8-min_and_max-temporary_analyzer_outp_411))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_8-min_and_max-temporary_analyzer_outp_409))+(ref_PCollection_PCollection_229/Write))+(ref_PCollection_PCollection_230/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_9-_416))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_9-min_and_max-temporary_analyzer_outp_420))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_9-min_and_max-temporary_analyzer_outp_418))+(ref_PCollection_PCollection_235/Write))+(ref_PCollection_PCollection_236/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_10_425))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_10-min_and_max-temporary_analyzer_out_429))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_10-min_and_max-temporary_analyzer_out_427))+(ref_PCollection_PCollection_241/Write))+(ref_PCollection_PCollection_242/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_11_434))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_11-min_and_max-temporary_analyzer_out_438))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_11-min_and_max-temporary_analyzer_out_436))+(ref_PCollection_PCollection_247/Write))+(ref_PCollection_PCollection_248/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_12_443))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_12-min_and_max-temporary_analyzer_out_447))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_12-min_and_max-temporary_analyzer_out_445))+(ref_PCollection_PCollection_253/Write))+(ref_PCollection_PCollection_254/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_13_452))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_13-min_and_max-temporary_analyzer_out_456))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_13-min_and_max-temporary_analyzer_out_454))+(ref_PCollection_PCollection_259/Write))+(ref_PCollection_PCollection_260/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_14_461))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_14-min_and_max-temporary_analyzer_out_463))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_14-min_and_max-temporary_analyzer_out_465))+(ref_PCollection_PCollection_265/Write))+(ref_PCollection_PCollection_266/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_15_470))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_15-min_and_max-temporary_analyzer_out_474))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_15-min_and_max-temporary_analyzer_out_472))+(ref_PCollection_PCollection_271/Write))+(ref_PCollection_PCollection_272/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_16_479))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_16-min_and_max-temporary_analyzer_out_483))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_16-min_and_max-temporary_analyzer_out_481))+(ref_PCollection_PCollection_277/Write))+(ref_PCollection_PCollection_278/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_17_488))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_17-min_and_max-temporary_analyzer_out_492))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_17-min_and_max-temporary_analyzer_out_490))+(ref_PCollection_PCollection_283/Write))+(ref_PCollection_PCollection_284/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_18_497))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_18-min_and_max-temporary_analyzer_out_499))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_18-min_and_max-temporary_analyzer_out_501))+(ref_PCollection_PCollection_289/Write))+(ref_PCollection_PCollection_290/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_19_506))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_19-min_and_max-temporary_analyzer_out_508))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_19-min_and_max-temporary_analyzer_out_510))+(ref_PCollection_PCollection_295/Write))+(ref_PCollection_PCollection_296/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_20_515))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_20-min_and_max-temporary_analyzer_out_517))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_20-min_and_max-temporary_analyzer_out_519))+(ref_PCollection_PCollection_301/Write))+(ref_PCollection_PCollection_302/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_21_524))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_21-min_and_max-temporary_analyzer_out_526))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_21-min_and_max-temporary_analyzer_out_528))+(ref_PCollection_PCollection_307/Write))+(ref_PCollection_PCollection_308/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_22_533))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_22-min_and_max-temporary_analyzer_out_537))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_22-min_and_max-temporary_analyzer_out_535))+(ref_PCollection_PCollection_313/Write))+(ref_PCollection_PCollection_314/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_23_542))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_23-min_and_max-temporary_analyzer_out_544))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_23-min_and_max-temporary_analyzer_out_546))+(ref_PCollection_PCollection_319/Write))+(ref_PCollection_PCollection_320/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_24_551))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_24-min_and_max-temporary_analyzer_out_553))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_24-min_and_max-temporary_analyzer_out_555))+(ref_PCollection_PCollection_325/Write))+(ref_PCollection_PCollection_326/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_25_560))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_25-min_and_max-temporary_analyzer_out_562))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_25-min_and_max-temporary_analyzer_out_564))+(ref_PCollection_PCollection_331/Write))+(ref_PCollection_PCollection_332/Write))+(ref_AppliedPTransform_Analyze-ExtractPackedCombineMergeOutputs-CacheableCombineMerge-scale_to_0_1_26_569))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_26-min_and_max-temporary_analyzer_out_571))+(ref_AppliedPTransform_Analyze-CreateTensorBinding-scale_to_0_1_26-min_and_max-temporary_analyzer_out_573))+(ref_PCollection_PCollection_337/Write))+(ref_PCollection_PCollection_338/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((((((((ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-Impulse_576)+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-FlatMap-lambda-at-core-py-3229-_577))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSole-Map-decode-_579))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-ReplaceWithConstants_580))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-CreateSavedModel_581))+(ref_AppliedPTransform_Analyze-ComputeDeferredMetadata-compat_v1-False-_850))+(ref_AppliedPTransform_Analyze-MakeCheapBarrier_851))+(ref_AppliedPTransform_WriteTransformFn-WriteTransformFnToTemp_868))+(ref_PCollection_PCollection_343/Write))+(ref_AppliedPTransform_WriteTransformFn-WriteMetadataToTemp-WriteMetadata_867))+(ref_AppliedPTransform_Transform-TransformIndex0-GetDeferredSchema_1477))+(ref_AppliedPTransform_Transform-TransformIndex1-GetDeferredSchema_1534))+(ref_PCollection_PCollection_494/Write))+(ref_PCollection_PCollection_503/Write))+(ref_PCollection_PCollection_504/Write))+(ref_AppliedPTransform_Transform-TransformIndex0-MakeTensorToArrowConverter_1502))+(ref_PCollection_PCollection_907/Write))+(ref_AppliedPTransform_Transform-TransformIndex0-MapToTensorRepresentations_1503))+(ref_PCollection_PCollection_923/Write))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentTransformOutputTensors-IncrementCounters_1506))+(ref_AppliedPTransform_Transform-TransformIndex1-MakeTensorToArrowConverter_1559))+(ref_PCollection_PCollection_940/Write))+(ref_AppliedPTransform_Transform-TransformIndex1-MapToTensorRepresentations_1560))+(ref_PCollection_PCollection_956/Write))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentTransformOutputTensors-IncrementCounters_1563)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/388d1ce10f684118992094e928bf6d92/assets\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_WriteTransformFn-CreateSole-Impulse_870)+(ref_AppliedPTransform_WriteTransformFn-CreateSole-FlatMap-lambda-at-core-py-3229-_871))+(ref_AppliedPTransform_WriteTransformFn-CreateSole-Map-decode-_873))+(ref_AppliedPTransform_WriteTransformFn-PublishMetadataAndTransformFn_874))+(ref_PCollection_PCollection_508/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-Impulse_10)+(ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-FlatMap-lambda-at-core-py-3229-_11))+(ref_AppliedPTransform_IncrementPipelineMetrics-CreateSole-Map-decode-_13))+(ref_AppliedPTransform_IncrementPipelineMetrics-Count_14)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_23-min_and_max-Ana_809)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_23-min_and_max-Ana_810))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_23-min_and_max-Ana_812))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_23-min_and_max-Ana_813)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_24-min_and_max-Ana_818)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_24-min_and_max-Ana_819))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_24-min_and_max-Ana_821))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_24-min_and_max-Ana_822)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_25-min_and_max-Ana_827)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_25-min_and_max-Ana_828))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_25-min_and_max-Ana_830))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_25-min_and_max-Ana_831)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_26-min_and_max-Ana_836)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_26-min_and_max-Ana_837))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_26-min_and_max-Ana_839))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_26-min_and_max-Ana_840)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_27-min_and_max-Ana_845)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_27-min_and_max-Ana_846))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_27-min_and_max-Ana_848))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_27-min_and_max-Ana_849)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-Impulse_853)+(ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-core-py-3229-_854))+(ref_AppliedPTransform_Analyze-PrepareToClearSharedKeepAlives-Map-decode-_856))+(ref_AppliedPTransform_Analyze-WaitAndClearSharedKeepAlives_857)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-Extract_921))+(ref_PCollection_PCollection_539/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_533/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-PreFinalize_922))+(ref_PCollection_PCollection_540/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1291799545288086 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-Extract_937))+(ref_PCollection_PCollection_550/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_544/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-PreFinalize_938))+(ref_PCollection_PCollection_551/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12453842163085938 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_544/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex3-Write-WriteImpl-FinalizeWrite_939)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1249992847442627 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.15848088264465332 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-Extract_953))+(ref_PCollection_PCollection_561/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_555/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-PreFinalize_954))+(ref_PCollection_PCollection_562/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12549066543579102 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_555/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex4-Write-WriteImpl-FinalizeWrite_955)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1244211196899414 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13191485404968262 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-Extract_889))+(ref_PCollection_PCollection_517/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_511/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-PreFinalize_890))+(ref_PCollection_PCollection_518/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12269258499145508 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-Impulse_1737)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_1738))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-DoOnce-Map-decode-_1740))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-InitializeWrite_1741))+(ref_PCollection_PCollection_1067/Write))+(ref_PCollection_PCollection_1068/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-ReadFrom_1519)+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-ReadFrom_1520))+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_933_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12691140174865723 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1538560390472412 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((((((ref_PCollection_PCollection_933_split/Read)+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-ReadRawRecords-FlattenP_1523))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordBeamSource-CollectRawRecordTelemet_1525))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-RawRecordToRecordBat_1529))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-RawRecordToRecordBat_1530))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex1-RawRecordToRecordBatch-CollectRecordBatchTe_1532))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-GetRecordBatchSize_1536))+(ref_AppliedPTransform_Transform-TransformIndex1-Transform_1558))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-SumTotalBytes-KeyWith_1538))+(Transform[TransformIndex1]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Precombine))+(Transform[TransformIndex1]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Transform-TransformIndex1-ConvertToRecordBatch_1561))+(ref_AppliedPTransform_Transform-TransformIndex1-MakeCheapBarrier_1564))+(ref_AppliedPTransform_ExtractRecordBatches-TransformIndex1-Keys_1574))+(ref_AppliedPTransform_EncodeAndSerialize-TransformIndex1-_1729))+(ref_PCollection_PCollection_960/Write))+(FlattenTransformedDatasets/Write/1))+(ref_AppliedPTransform_Materialize-TransformIndex1-Values-Values_1732))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_1742))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-WriteBundles_1743))+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-Pair_1744))+(Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": struct2tensor is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_decision_forests is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_text is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Transform[TransformIndex1]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Group/Read)+(Transform[TransformIndex1]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Merge))+(Transform[TransformIndex1]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-SumTotalBytes-UnKey_1543))+(ref_PCollection_PCollection_945/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-SumTotalBytes-DoOnce-_1545)+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-SumTotalBytes-DoOnce-_1546))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-SumTotalBytes-DoOnce-_1548))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-SumTotalBytes-InjectD_1549))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentInputBytes-Transform-IncrementCounter_1550)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Transform-TransformIndex1-CreateTransformInputTensorRepresentations-Impulse_1552)+(ref_AppliedPTransform_Transform-TransformIndex1-CreateTransformInputTensorRepresentations-FlatMap-la_1553))+(ref_AppliedPTransform_Transform-TransformIndex1-CreateTransformInputTensorRepresentations-Map-decode_1555))+(ref_AppliedPTransform_Transform-TransformIndex1-InstrumentTransformInputTensors-IncrementCounters_1557)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-Impulse_1566)+(ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-cor_1567))+(ref_AppliedPTransform_Transform-TransformIndex1-PrepareToClearSharedKeepAlives-Map-decode-_1569))+(ref_AppliedPTransform_Transform-TransformIndex1-WaitAndClearSharedKeepAlives_1570)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-Impulse_1717)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-FlatMap-lambda-at-cor_1718))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-DoOnce-Map-decode-_1720))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-InitializeWrite_1721))+(ref_PCollection_PCollection_1054/Write))+(ref_PCollection_PCollection_1055/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-ReadFrom_1462)+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-ReadFrom_1463))+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_900_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12400293350219727 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1287827491760254 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((((((ref_PCollection_PCollection_900_split/Read)+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-ReadRawRecords-FlattenP_1466))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordBeamSource-CollectRawRecordTelemet_1468))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-RawRecordToRecordBat_1472))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-RawRecordToRecordBat_1473))+(ref_AppliedPTransform_TFXIOReadAndDecode-TransformIndex0-RawRecordToRecordBatch-CollectRecordBatchTe_1475))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-GetRecordBatchSize_1479))+(ref_AppliedPTransform_Transform-TransformIndex0-Transform_1501))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-SumTotalBytes-KeyWith_1481))+(Transform[TransformIndex0]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Precombine))+(Transform[TransformIndex0]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Transform-TransformIndex0-ConvertToRecordBatch_1504))+(ref_AppliedPTransform_Transform-TransformIndex0-MakeCheapBarrier_1507))+(ref_AppliedPTransform_ExtractRecordBatches-TransformIndex0-Keys_1572))+(ref_AppliedPTransform_EncodeAndSerialize-TransformIndex0-_1709))+(ref_PCollection_PCollection_927/Write))+(FlattenTransformedDatasets/Write/0))+(ref_AppliedPTransform_Materialize-TransformIndex0-Values-Values_1712))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-WindowInto-WindowIntoFn-_1722))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-WriteBundles_1723))+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-Pair_1724))+(Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": struct2tensor is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_decision_forests is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_text is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((FlattenTransformedDatasets/Read)+(ref_AppliedPTransform_WaitForTransformWrite_1576))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-FilterInternalColumn_1578))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1582))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1597))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1584))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1600))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1627))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1602))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Write))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/0))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1628))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1589))+(ref_PCollection_PCollection_974/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1591)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1592))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1594))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1595))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1596)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-Impulse_584)+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-FlatMap-lambda-at-core-py_585))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-CreateSole-Map-decode-_587))+(ref_AppliedPTransform_Analyze-CreateSavedModel-tf_v2_only-Count-Count_588)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_593)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_594))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_596))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_z_score-mean_and_var-A_597)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1-min_and_max-Analys_602)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1-min_and_max-Analys_603))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1-min_and_max-Analys_605))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1-min_and_max-Analys_606)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_1-min_and_max-Anal_611)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_1-min_and_max-Anal_612))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_1-min_and_max-Anal_614))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_1-min_and_max-Anal_615)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_2-min_and_max-Anal_620)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_2-min_and_max-Anal_621))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_2-min_and_max-Anal_623))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_2-min_and_max-Anal_624)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_3-min_and_max-Anal_629)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_3-min_and_max-Anal_630))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_3-min_and_max-Anal_632))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_3-min_and_max-Anal_633)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_4-min_and_max-Anal_638)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_4-min_and_max-Anal_639))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_4-min_and_max-Anal_641))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_4-min_and_max-Anal_642)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_5-min_and_max-Anal_647)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_5-min_and_max-Anal_648))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_5-min_and_max-Anal_650))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_5-min_and_max-Anal_651)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_6-min_and_max-Anal_656)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_6-min_and_max-Anal_657))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_6-min_and_max-Anal_659))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_6-min_and_max-Anal_660)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-Extract_1726))+(ref_PCollection_PCollection_1060/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_1054/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-PreFinalize_1727))+(ref_PCollection_PCollection_1061/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.14745545387268066 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-Extract_1746))+(ref_PCollection_PCollection_1073/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_1067/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-PreFinalize_1747))+(ref_PCollection_PCollection_1074/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1265263557434082 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_1067/Read)+(ref_AppliedPTransform_Materialize-TransformIndex1-Write-Write-WriteImpl-FinalizeWrite_1748)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1230776309967041 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12033224105834961 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex11]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-Extract_1065))+(ref_PCollection_PCollection_638/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_632/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-PreFinalize_1066))+(ref_PCollection_PCollection_639/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13275432586669922 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_632/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex11-Write-WriteImpl-FinalizeWrite_1067)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12726235389709473 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12437081336975098 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex21]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-Extract_1225))+(ref_PCollection_PCollection_748/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_742/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-PreFinalize_1226))+(ref_PCollection_PCollection_749/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12241649627685547 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_742/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex21-Write-WriteImpl-FinalizeWrite_1227)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12245512008666992 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1237783432006836 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex12]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-Extract_1081))+(ref_PCollection_PCollection_649/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_643/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-PreFinalize_1082))+(ref_PCollection_PCollection_650/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12232446670532227 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_643/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex12-Write-WriteImpl-FinalizeWrite_1083)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1267378330230713 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1902008056640625 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex22]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-Extract_1241))+(ref_PCollection_PCollection_759/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_753/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-PreFinalize_1242))+(ref_PCollection_PCollection_760/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1289687156677246 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_753/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex22-Write-WriteImpl-FinalizeWrite_1243)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1273491382598877 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12119603157043457 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex13]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-Extract_1097))+(ref_PCollection_PCollection_660/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_654/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-PreFinalize_1098))+(ref_PCollection_PCollection_661/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12408971786499023 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_654/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex13-Write-WriteImpl-FinalizeWrite_1099)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12131571769714355 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12511634826660156 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex23]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-Extract_1257))+(ref_PCollection_PCollection_770/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_764/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-PreFinalize_1258))+(ref_PCollection_PCollection_771/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12011837959289551 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_764/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex23-Write-WriteImpl-FinalizeWrite_1259)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12071776390075684 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13551878929138184 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-Extract_905))+(ref_PCollection_PCollection_528/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_522/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-PreFinalize_906))+(ref_PCollection_PCollection_529/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1208653450012207 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_522/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex1-Write-WriteImpl-FinalizeWrite_907)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1475985050201416 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12272906303405762 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex14]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-Extract_1113))+(ref_PCollection_PCollection_671/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_665/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-PreFinalize_1114))+(ref_PCollection_PCollection_672/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12237882614135742 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_665/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex14-Write-WriteImpl-FinalizeWrite_1115)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12764906883239746 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1315319538116455 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex24]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-Extract_1273))+(ref_PCollection_PCollection_781/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_775/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-PreFinalize_1274))+(ref_PCollection_PCollection_782/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1264355182647705 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_775/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex24-Write-WriteImpl-FinalizeWrite_1275)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12431693077087402 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.11777687072753906 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_WriteMetadata-Create-Impulse_860)+(ref_AppliedPTransform_WriteMetadata-Create-FlatMap-lambda-at-core-py-3229-_861))+(ref_AppliedPTransform_WriteMetadata-Create-Map-decode-_863))+(ref_AppliedPTransform_WriteMetadata-WriteMetadata_864)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1397))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1398))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/1))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1404))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1405))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1408))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1413))+(ref_PCollection_PCollection_870/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1415)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1416))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1418))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1419))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-G_1420))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-M_1431))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-W_1432))+(GenerateStats[FlattenedAnalysisDataset]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1426)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1427))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-D_1429))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-I_1430))+(ref_PCollection_PCollection_878/Write))+(ref_PCollection_PCollection_879/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-W_1434))+(ref_PCollection_PCollection_883/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_878/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-P_1435))+(ref_PCollection_PCollection_884/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_878/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteStats-WriteStats-Write-WriteImpl-F_1436)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12158799171447754 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Impu_1446)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Flat_1447))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-DoOnce-Map-_1449))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-InitializeW_1450))+(ref_PCollection_PCollection_891/Write))+(ref_PCollection_PCollection_892/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-Impulse_1438)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-FlatMap-lambda-at-core-py-_1439))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-CreateSchema-Map-decode-_1441))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-Map-lambda-_1451))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-WindowInto-_1452))+(GenerateStats[FlattenedAnalysisDataset]/WriteSchema/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateStats[FlattenedAnalysisDataset]/WriteSchema/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-WriteBundle_1454))+(ref_PCollection_PCollection_896/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_891/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-PreFinalize_1455))+(ref_PCollection_PCollection_897/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_891/Read)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-WriteSchema-Write-WriteImpl-FinalizeWri_1456)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.13338637351989746 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex17]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-Extract_1161))+(ref_PCollection_PCollection_704/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_698/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-PreFinalize_1162))+(ref_PCollection_PCollection_705/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12270569801330566 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-Impulse_37)+(ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-FlatMap-lambda-at-core-py-3229-_38))+(ref_AppliedPTransform_Analyze-InstrumentAPI-CreateSoleAPIUse-Map-decode-_40))+(ref_AppliedPTransform_Analyze-InstrumentAPI-CountAPIUse_41)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Analyze/InstrumentInputBytes[AnalysisPCollDict][AnalysisIndex0]/SumTotalBytes/CombinePerKey/Group/Read)+(Analyze/InstrumentInputBytes[AnalysisPCollDict][AnalysisIndex0]/SumTotalBytes/CombinePerKey/Merge))+(Analyze/InstrumentInputBytes[AnalysisPCollDict][AnalysisIndex0]/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-SumTotalBytes-Un_50))+(ref_PCollection_PCollection_24/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-SumTotalBytes-Do_52)+(ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-SumTotalBytes-Do_53))+(ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-SumTotalBytes-Do_55))+(ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-SumTotalBytes-In_56))+(ref_AppliedPTransform_Analyze-InstrumentInputBytes-AnalysisPCollDict-AnalysisIndex0-IncrementCounter_57)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-CreateAnalyzeInputTensorRepresentations-Impulse_59)+(ref_AppliedPTransform_Analyze-CreateAnalyzeInputTensorRepresentations-FlatMap-lambda-at-core-py-3229_60))+(ref_AppliedPTransform_Analyze-CreateAnalyzeInputTensorRepresentations-Map-decode-_62))+(ref_AppliedPTransform_Analyze-InstrumentAnalyzeInputTensors-IncrementCounters_64)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-I_74)+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-F_75))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-CreateSole-M_77))+(ref_AppliedPTransform_Analyze-CreateSavedModelForAnalyzerInputs-Phase0-tf_v2_only-Count-Count_78)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1608))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1611))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1616))+(ref_PCollection_PCollection_989/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex25]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-Extract_1289))+(ref_PCollection_PCollection_792/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_786/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-PreFinalize_1290))+(ref_PCollection_PCollection_793/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12766122817993164 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_786/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex25-Write-WriteImpl-FinalizeWrite_1291)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12728214263916016 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12408828735351562 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1618)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1619))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1621))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1622))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1623)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1633))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1634))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Transcode/1))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/Flatten/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex26]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-Extract_1305))+(ref_PCollection_PCollection_803/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_797/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-PreFinalize_1306))+(ref_PCollection_PCollection_804/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12781882286071777 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_797/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex26-Write-WriteImpl-FinalizeWrite_1307)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.13651680946350098 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12746381759643555 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators[0]/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/0))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1640))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1641))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1644))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1649))+(ref_PCollection_PCollection_1010/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex27]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-Extract_1321))+(ref_PCollection_PCollection_814/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_808/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-PreFinalize_1322))+(ref_PCollection_PCollection_815/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12466073036193848 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_808/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex27-Write-WriteImpl-FinalizeWrite_1323)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12826895713806152 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12845206260681152 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1662)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1663))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1665))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1666))+(ref_PCollection_PCollection_1018/Write))+(ref_PCollection_PCollection_1019/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1651)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1652))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1654))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1655))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-GenerateStatistics-RunSt_1656))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1667))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-ValidateStatistics_1693))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1668))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1703))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1704))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteValidation/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteStats/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1670))+(ref_PCollection_PCollection_1023/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_1018/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1671))+(ref_PCollection_PCollection_1024/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_1018/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteStats-WriteStats-Wr_1672)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12076973915100098 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex8]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-Extract_1017))+(ref_PCollection_PCollection_605/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_599/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-PreFinalize_1018))+(ref_PCollection_PCollection_606/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13297629356384277 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_599/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex8-Write-WriteImpl-FinalizeWrite_1019)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12489628791809082 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12029290199279785 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex9]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-Extract_1033))+(ref_PCollection_PCollection_616/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_610/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-PreFinalize_1034))+(ref_PCollection_PCollection_617/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12770795822143555 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_610/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex9-Write-WriteImpl-FinalizeWrite_1035)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1290755271911621 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13603425025939941 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex10]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-Extract_1049))+(ref_PCollection_PCollection_627/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_621/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-PreFinalize_1050))+(ref_PCollection_PCollection_628/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1289060115814209 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_621/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex10-Write-WriteImpl-FinalizeWrite_1051)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.13190293312072754 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12463140487670898 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-Extract_969))+(ref_PCollection_PCollection_572/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_566/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-PreFinalize_970))+(ref_PCollection_PCollection_573/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12323546409606934 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_566/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex5-Write-WriteImpl-FinalizeWrite_971)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12484407424926758 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1281139850616455 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex6]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-Extract_985))+(ref_PCollection_PCollection_583/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_577/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-PreFinalize_986))+(ref_PCollection_PCollection_584/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12299585342407227 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_577/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex6-Write-WriteImpl-FinalizeWrite_987)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1257038116455078 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12803077697753906 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex7]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-Extract_1001))+(ref_PCollection_PCollection_594/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_588/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-PreFinalize_1002))+(ref_PCollection_PCollection_595/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1228940486907959 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_588/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex7-Write-WriteImpl-FinalizeWrite_1003)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12159395217895508 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1348123550415039 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_7-min_and_max-Anal_665)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_7-min_and_max-Anal_666))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_7-min_and_max-Anal_668))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_7-min_and_max-Anal_669)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_8-min_and_max-Anal_674)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_8-min_and_max-Anal_675))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_8-min_and_max-Anal_677))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_8-min_and_max-Anal_678)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_9-min_and_max-Anal_683)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_9-min_and_max-Anal_684))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_9-min_and_max-Anal_686))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_9-min_and_max-Anal_687)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_10-min_and_max-Ana_692)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_10-min_and_max-Ana_693))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_10-min_and_max-Ana_695))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_10-min_and_max-Ana_696)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_11-min_and_max-Ana_701)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_11-min_and_max-Ana_702))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_11-min_and_max-Ana_704))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_11-min_and_max-Ana_705)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_12-min_and_max-Ana_710)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_12-min_and_max-Ana_711))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_12-min_and_max-Ana_713))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_12-min_and_max-Ana_714)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_13-min_and_max-Ana_719)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_13-min_and_max-Ana_720))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_13-min_and_max-Ana_722))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_13-min_and_max-Ana_723)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_14-min_and_max-Ana_728)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_14-min_and_max-Ana_729))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_14-min_and_max-Ana_731))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_14-min_and_max-Ana_732)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_15-min_and_max-Ana_737)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_15-min_and_max-Ana_738))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_15-min_and_max-Ana_740))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_15-min_and_max-Ana_741)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_16-min_and_max-Ana_746)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_16-min_and_max-Ana_747))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_16-min_and_max-Ana_749))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_16-min_and_max-Ana_750)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_533/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex2-Write-WriteImpl-FinalizeWrite_923)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12107372283935547 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1273043155670166 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_17-min_and_max-Ana_755)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_17-min_and_max-Ana_756))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_17-min_and_max-Ana_758))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_17-min_and_max-Ana_759)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_18-min_and_max-Ana_764)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_18-min_and_max-Ana_765))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_18-min_and_max-Ana_767))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_18-min_and_max-Ana_768)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_19-min_and_max-Ana_773)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_19-min_and_max-Ana_774))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_19-min_and_max-Ana_776))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_19-min_and_max-Ana_777)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_20-min_and_max-Ana_782)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_20-min_and_max-Ana_783))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_20-min_and_max-Ana_785))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_20-min_and_max-Ana_786)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_21-min_and_max-Ana_791)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_21-min_and_max-Ana_792))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_21-min_and_max-Ana_794))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_21-min_and_max-Ana_795)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_22-min_and_max-Ana_800)+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_22-min_and_max-Ana_801))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_22-min_and_max-Ana_803))+(ref_AppliedPTransform_Analyze-EncodeCache-CacheableCombineAccumulate-scale_to_0_1_22-min_and_max-Ana_804)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_511/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex0-Write-WriteImpl-FinalizeWrite_891)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12639665603637695 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12904763221740723 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((Transform[TransformIndex0]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Group/Read)+(Transform[TransformIndex0]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/Merge))+(Transform[TransformIndex0]/InstrumentInputBytes[Transform]/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-SumTotalBytes-UnKey_1486))+(ref_PCollection_PCollection_912/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-SumTotalBytes-DoOnce-_1488)+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-SumTotalBytes-DoOnce-_1489))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-SumTotalBytes-DoOnce-_1491))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-SumTotalBytes-InjectD_1492))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentInputBytes-Transform-IncrementCounter_1493)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Transform-TransformIndex0-CreateTransformInputTensorRepresentations-Impulse_1495)+(ref_AppliedPTransform_Transform-TransformIndex0-CreateTransformInputTensorRepresentations-FlatMap-la_1496))+(ref_AppliedPTransform_Transform-TransformIndex0-CreateTransformInputTensorRepresentations-Map-decode_1498))+(ref_AppliedPTransform_Transform-TransformIndex0-InstrumentTransformInputTensors-IncrementCounters_1500)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-Impulse_1509)+(ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-FlatMap-lambda-at-cor_1510))+(ref_AppliedPTransform_Transform-TransformIndex0-PrepareToClearSharedKeepAlives-Map-decode-_1512))+(ref_AppliedPTransform_Transform-TransformIndex0-WaitAndClearSharedKeepAlives_1513)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-Impulse_1674)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-FlatMap-lam_1675))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-CreateSchema-Map-decode-_1677))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1687))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1688))+(GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteSchema/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1682)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1683))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1685))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1686))+(ref_PCollection_PCollection_1031/Write))+(ref_PCollection_PCollection_1032/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteSchema/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1690))+(ref_PCollection_PCollection_1036/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_1031/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1691))+(ref_PCollection_PCollection_1037/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_1031/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteSchema-Write-WriteI_1692)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12179160118103027 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1698)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1699))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1701))+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1702))+(ref_PCollection_PCollection_1042/Write))+(ref_PCollection_PCollection_1043/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((GenerateAndValidateStats[FlattenedTransformedDatasets]/WriteValidation/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1706))+(ref_PCollection_PCollection_1047/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_1042/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1707))+(ref_PCollection_PCollection_1048/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_1042/Read)+(ref_AppliedPTransform_GenerateAndValidateStats-FlattenedTransformedDatasets-WriteValidation-Write-Wr_1708)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.15615200996398926 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_1054/Read)+(ref_AppliedPTransform_Materialize-TransformIndex0-Write-Write-WriteImpl-FinalizeWrite_1728)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12858891487121582 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1308887004852295 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-Impulse_4)+(ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-FlatMap-lambda-at-core-py-3229-_5))+(ref_AppliedPTransform_OptimizeRun-WorkaroundForBug170304777-Map-decode-_7)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_111)+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_112))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Cr_114))+(ref_AppliedPTransform_Analyze-PackedCombineAccumulate-ApplySavedModel-Phase0-AnalysisIndex0-Count-Co_115)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex18]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-Extract_1177))+(ref_PCollection_PCollection_715/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_709/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-PreFinalize_1178))+(ref_PCollection_PCollection_716/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1345980167388916 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_709/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex18-Write-WriteImpl-FinalizeWrite_1179)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1237180233001709 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12749814987182617 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex28]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-Extract_1337))+(ref_PCollection_PCollection_825/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_819/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-PreFinalize_1338))+(ref_PCollection_PCollection_826/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12500524520874023 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_819/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex28-Write-WriteImpl-FinalizeWrite_1339)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12839841842651367 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13361048698425293 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex15]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-Extract_1129))+(ref_PCollection_PCollection_682/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_676/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-PreFinalize_1130))+(ref_PCollection_PCollection_683/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12464189529418945 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_676/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex15-Write-WriteImpl-FinalizeWrite_1131)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1354355812072754 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13031411170959473 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex19]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-Extract_1193))+(ref_PCollection_PCollection_726/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_720/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-PreFinalize_1194))+(ref_PCollection_PCollection_727/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12359833717346191 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_720/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex19-Write-WriteImpl-FinalizeWrite_1195)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12241363525390625 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.11960816383361816 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackTotalBytes/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1353))+(ref_PCollection_PCollection_834/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex16]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-Extract_1145))+(ref_PCollection_PCollection_693/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_687/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-PreFinalize_1146))+(ref_PCollection_PCollection_694/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1362919807434082 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_687/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex16-Write-WriteImpl-FinalizeWrite_1147)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.16588282585144043 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1307373046875 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1355)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1356))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1358))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1359))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1360)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex20]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-Extract_1209))+(ref_PCollection_PCollection_737/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_731/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-PreFinalize_1210))+(ref_PCollection_PCollection_738/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12046575546264648 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_731/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex20-Write-WriteImpl-FinalizeWrite_1211)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.11996221542358398 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1243429183959961 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/RemoveDuplicates/Group/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1372))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1375))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_698/Read)+(ref_AppliedPTransform_WriteCache-Write-AnalysisIndex0-CacheKeyIndex17-Write-WriteImpl-FinalizeWrite_1163)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.13154864311218262 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12334942817687988 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(GenerateStats[FlattenedAnalysisDataset]/GenerateStatistics/RunStatsGenerators/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1380))+(ref_PCollection_PCollection_849/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1382)+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1383))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1385))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1386))+(ref_AppliedPTransform_GenerateStats-FlattenedAnalysisDataset-GenerateStatistics-RunStatsGenerators-T_1387)\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Artifact type Model is not found in MLMD.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Step #3 - \"Local Test E2E Pipeline\": Processing /tmp/tmpqfvk187k/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7-py3-none-any.whl\n",
      "Step #3 - \"Local Test E2E Pipeline\": Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Step #3 - \"Local Test E2E Pipeline\": Successfully installed tfx-user-code-ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7\n",
      "Step #3 - \"Local Test E2E Pipeline\": WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner started...\n",
      "Step #3 - \"Local Test E2E Pipeline\": fn_args: FnArgs(working_dir=None, train_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-train/*'], eval_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-eval/*'], train_steps=None, eval_steps=None, schema_path='src/raw_schema/schema.pbtxt', schema_file='src/raw_schema/schema.pbtxt', transform_graph_path='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', transform_output='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7fe164616710>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7fe164563a70>, data_view_decode_fn=None), serving_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving', eval_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-TFMA', model_run_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model_run/10', base_model=None, hyperparameters={'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}, custom_config=None)\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Hyperparameter:\n",
      "Step #3 - \"Local Test E2E Pipeline\": {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner executing trainer...\n",
      "Step #3 - \"Local Test E2E Pipeline\": Loading tft output from gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model: \"model\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": __________________________________________________________________________________________________\n",
      "Step #3 - \"Local Test E2E Pipeline\":  Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================================================================================================\n",
      "Step #3 - \"Local Test E2E Pipeline\":  Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  concatenate (Concatenate)      (None, 29)           0           ['Amount[0][0]',                 \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V1[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V10[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V11[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V12[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V13[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V14[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V15[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V16[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V17[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V18[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V19[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V2[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V20[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V21[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V22[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V23[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V24[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V25[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V26[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V27[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V28[0][0]',                    \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V3[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V4[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V5[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V6[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V7[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V8[0][0]',                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                   'V9[0][0]']                     \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  dense (Dense)                  (None, 128)          3840        ['concatenate[0][0]']            \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\":  dense_2 (Dense)                (None, 1)            129         ['dense_1[0][0]']                \n",
      "Step #3 - \"Local Test E2E Pipeline\":                                                                                                   \n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================================================================================================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Total params: 20,481\n",
      "Step #3 - \"Local Test E2E Pipeline\": Trainable params: 20,481\n",
      "Step #3 - \"Local Test E2E Pipeline\": Non-trainable params: 0\n",
      "Step #3 - \"Local Test E2E Pipeline\": __________________________________________________________________________________________________\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model training started...\n",
      "  1/451 [..............................] - ETA: 16:56 - loss: 0.9360 - accuracy: 0.0078Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 451 batches). You may need to use the repeat() function when building your dataset.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "451/451 [==============================] - 5s 7ms/step - loss: 0.9360 - accuracy: 0.0078\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model training completed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner executing exporter...\n",
      "Step #3 - \"Local Test E2E Pipeline\": struct2tensor is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_decision_forests is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": tensorflow_text is not available.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model export started...\n",
      "Step #3 - \"Local Test E2E Pipeline\": Function `serve_features_fn` contains input name(s) Amount, V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26, V27, V28, V3, V4, V5, V6, V7, V8, V9 with unsupported characters which will be renamed to amount, v1, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v2, v20, v21, v22, v23, v24, v25, v26, v27, v28, v3, v4, v5, v6, v7, v8, v9 in the SavedModel.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving/assets\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model export completed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Runner completed.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": E0630 11:14:06.793100514       1 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe1486d6690> and <keras.engine.input_layer.InputLayer object at 0x7fe1432a8690>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.16177892684936523 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe140304f50> and <keras.engine.input_layer.InputLayer object at 0x7fe184fefbd0>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Step #3 - \"Local Test E2E Pipeline\": Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Default Python SDK image for environment is apache/beam_python3.7_sdk:2.38.0\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function annotate_downstream_side_inputs at 0x7fe18a084050> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function fix_side_input_pcoll_coders at 0x7fe18a084170> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function pack_combiners at 0x7fe18a084680> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function lift_combiners at 0x7fe18a084710> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_sdf at 0x7fe18a0848c0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function expand_gbk at 0x7fe18a084950> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sink_flattens at 0x7fe18a084a70> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function greedily_fuse at 0x7fe18a084b00> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function read_to_impulse at 0x7fe18a084b90> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function impulse_to_input at 0x7fe18a084c20> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function sort_stages at 0x7fe18a084e60> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function setup_timer_mapping at 0x7fe18a084dd0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ==================== <function populate_data_channel_coders at 0x7fe18a084ef0> ====================\n",
      "Step #3 - \"Local Test E2E Pipeline\": Creating state cache with size 100\n",
      "Step #3 - \"Local Test E2E Pipeline\": Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7fe184b2bfd0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_193)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_194))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_196))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_197))+(ref_PCollection_PCollection_112/Write))+(ref_PCollection_PCollection_113/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_176)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_177))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_179))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_180))+(ref_PCollection_PCollection_100/Write))+(ref_PCollection_PCollection_101/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_159)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_160))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_162))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_163))+(ref_PCollection_PCollection_88/Write))+(ref_PCollection_PCollection_89/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-ReadFromTFRe_7)+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-ReadFromTFRe_8))+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12340950965881348 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12167620658874512 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((((((((((((((((((((((((((((((((ref_PCollection_PCollection_2_split/Read)+(ReadFromTFRecordToArrow[test][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-ReadRawRecords-FlattenPColl_11))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordBeamSource-CollectRawRecordTelemetry-P_13))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-RawRecordToRecordBatch-B_17))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-RawRecordToRecordBatch-D_18))+(ref_AppliedPTransform_ReadFromTFRecordToArrow-test-0-RawRecordToRecordBatch-CollectRecordBatchTeleme_20))+(ref_AppliedPTransform_FlattenExamples_21))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-BatchedInputsToExtracts-AddArrowRecordBatchKey_24))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-GetExtractSi_27))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractFeatures-ExtractFeatu_43))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-SumTotalByte_29))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/TrackInputBytes/SumTotalBytes/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/TrackInputBytes/SumTotalBytes/CombinePerKey/Group/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractTransformedFeatures-P_45))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractLabels-ExtractLabels_47))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractExampleWeights-Extrac_49))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractPredictions-Predict_51))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractUnbatchedInputs-Unbat_53))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-ExtractSliceKeys-ParDo-Extra_55))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_58))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_60))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_63))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_88))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_108))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_65))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_90))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_109))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Transcode/0))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Group/Write))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Write/0)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe164747690> and <keras.engine.input_layer.InputLayer object at 0x7fe141330b90>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_114))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_115))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Transcode/1))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Write/1)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe16477e890> and <keras.engine.input_layer.InputLayer object at 0x7fe184846b90>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe1434cc390> and <keras.engine.input_layer.InputLayer object at 0x7fe1486dfe50>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/Flatten/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe1435a5ed0> and <keras.engine.input_layer.InputLayer object at 0x7fe1434a2bd0>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((((((((((((((((((((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CombineMetricsPerSlice/CombinePerKey(PostCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_122))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_124))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_125))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_126))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_127))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_128))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_129))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Vali_130))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceM_154))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceP_171))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-ConvertSliceA_188))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_206))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_164))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_165))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_166))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteMetrics/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_181))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_182))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_183))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WritePlotsToTFRecord/Write/WriteImpl/GroupByKey/Write))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_198))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_199))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_200))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteAttributionsToTFRecord/Write/WriteImpl/GroupByKey/Write))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe142a02ed0> and <keras.engine.input_layer.InputLayer object at 0x7fe142ab6090>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7fe14233cdd0> and <keras.engine.input_layer.InputLayer object at 0x7fe142351810>).\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteAttributionsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_202))+(ref_PCollection_PCollection_118/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_112/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_203))+(ref_PCollection_PCollection_119/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12586641311645508 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_112/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteAttribut_204)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12899518013000488 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.13859868049621582 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_142)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_143))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_145))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_146))+(ref_PCollection_PCollection_77/Write))+(ref_PCollection_PCollection_78/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-I_134)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-F_135))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-CreateEvalConfig-M_137))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_147))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_148))+(ExtractEvaluateAndWriteResults/WriteResults/WriteEvalConfig/WriteEvalConfig/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteEvalConfig/WriteEvalConfig/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_150))+(ref_PCollection_PCollection_82/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_77/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_151))+(ref_PCollection_PCollection_83/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_77/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteEvalConfig-WriteEvalConfig-Wr_152)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.12627553939819336 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/RemoveDuplicates/Group/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_71))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_74))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/FanoutSlices/TrackDistinctSliceKeys/Size/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_79))+(ref_PCollection_PCollection_38/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_81)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_82))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_84))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_85))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_86)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/MergeValidationResults/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_211))+(ref_PCollection_PCollection_124/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteMetrics/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_168))+(ref_PCollection_PCollection_94/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_88/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_169))+(ref_PCollection_PCollection_95/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.14258193969726562 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_88/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteMetrics-_170)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.171966552734375 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12034344673156738 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.60 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_97)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_98))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_100))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_101)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/CountPerSliceKey/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_104))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-EvaluateMetricsAndPlots-Comp_105)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_222)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_223))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_225))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_226))+(ref_PCollection_PCollection_131/Write))+(ref_PCollection_PCollection_132/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_213)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_214))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_216))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-MergeValidati_217))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_227))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_228))+(ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteValidationsToTFRecord/Write/WriteImpl/GroupByKey/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WriteValidationsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_230))+(ref_PCollection_PCollection_136/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_131/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_231))+(ref_PCollection_PCollection_137/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_131/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WriteValidati_232)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.1285266876220703 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ExtractEvaluateAndWriteResults/WriteResults/WriteMetricsAndPlots/WritePlotsToTFRecord/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_185))+(ref_PCollection_PCollection_106/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((ref_PCollection_PCollection_100/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_186))+(ref_PCollection_PCollection_107/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.1188349723815918 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running (ref_PCollection_PCollection_100/Read)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-WriteResults-WriteMetricsAndPlots-WritePlotsToT_187)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 1 files in 0.16463828086853027 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting the size estimation of the input\n",
      "Step #3 - \"Local Test E2E Pipeline\": Finished listing 0 files in 0.12803196907043457 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Step #3 - \"Local Test E2E Pipeline\": Renamed 1 shards in 0.90 seconds.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ExtractEvaluateAndWriteResults/ExtractAndEvaluate/TrackInputBytes/SumTotalBytes/CombinePerKey/Group/Read)+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/TrackInputBytes/SumTotalBytes/CombinePerKey/Merge))+(ExtractEvaluateAndWriteResults/ExtractAndEvaluate/TrackInputBytes/SumTotalBytes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-SumTotalByte_34))+(ref_PCollection_PCollection_15/Write)\n",
      "Step #3 - \"Local Test E2E Pipeline\": Running ((((ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-SumTotalByte_36)+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-SumTotalByte_37))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-SumTotalByte_39))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-SumTotalByte_40))+(ref_AppliedPTransform_ExtractEvaluateAndWriteResults-ExtractAndEvaluate-TrackInputBytes-IncrementCou_41)\n",
      "Step #3 - \"Local Test E2E Pipeline\": From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Instructions for updating:\n",
      "Step #3 - \"Local Test E2E Pipeline\": Use eager execution and: \n",
      "Step #3 - \"Local Test E2E Pipeline\": `tf.data.TFRecordDataset(path)`\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": stateful_working_dir /workspace/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/GcsModelPusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Step #3 - \"Local Test E2E Pipeline\": Model output: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry/creditcards-classifier-v02\n",
      "Step #3 - \"Local Test E2E Pipeline\": .\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": =============================== warnings summary ===============================\n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "Step #3 - \"Local Test E2E Pipeline\":     import imp\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     'nearest': pil_image.NEAREST,\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     'bilinear': pil_image.BILINEAR,\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     'bicubic': pil_image.BICUBIC,\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     if hasattr(pil_image, 'HAMMING'):\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     if hasattr(pil_image, 'BOX'):\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     if hasattr(pil_image, 'LANCZOS'):\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "Step #3 - \"Local Test E2E Pipeline\":     _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:3680\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:3680: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\",\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4083\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4083: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\",\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4216\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/proto/pubsub_pb2.py:4216: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\",\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/gapic/subscriber_client.py:398\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/google/cloud/pubsub_v1/gapic/subscriber_client.py:398: DeprecationWarning: invalid escape sequence \\ \n",
      "Step #3 - \"Local Test E2E Pipeline\":     \"\"\"\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "Step #3 - \"Local Test E2E Pipeline\":     from collections import MutableMapping\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": ../../opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "Step #3 - \"Local Test E2E Pipeline\":     from collections import Mapping\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2441: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "Step #3 - \"Local Test E2E Pipeline\":     temp_location = pcoll.pipeline.options.view_as(\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2443: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "Step #3 - \"Local Test E2E Pipeline\":     job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2474: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "Step #3 - \"Local Test E2E Pipeline\":     | _PassThroughThenCleanup(files_to_remove_pcoll))\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "Step #3 - \"Local Test E2E Pipeline\":   /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "Step #3 - \"Local Test E2E Pipeline\":     return dispatch_target(*args, **kwargs)\n",
      "Step #3 - \"Local Test E2E Pipeline\": \n",
      "Step #3 - \"Local Test E2E Pipeline\": -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "Step #3 - \"Local Test E2E Pipeline\": ================== 1 passed, 23 warnings in 530.07s (0:08:50) ==================\n",
      "Finished Step #3 - \"Local Test E2E Pipeline\"\n",
      "Starting Step #5 - \"Build TFX Image\"\n",
      "Starting Step #6 - \"Compile Pipeline\"\n",
      "Step #6 - \"Compile Pipeline\": Already have image (with digest): europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/cicd:latest\n",
      "Step #5 - \"Build TFX Image\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #5 - \"Build TFX Image\": Sending build context to Docker daemon  2.945MB\n",
      "Step #5 - \"Build TFX Image\": Step 1/6 : FROM gcr.io/tfx-oss-public/tfx:1.8.0\n",
      "Step #5 - \"Build TFX Image\": 1.8.0: Pulling from tfx-oss-public/tfx\n",
      "Step #5 - \"Build TFX Image\": d5fd17ec1767: Already exists\n",
      "Step #5 - \"Build TFX Image\": 086b79b77a03: Already exists\n",
      "Step #5 - \"Build TFX Image\": 4698168f5888: Already exists\n",
      "Step #5 - \"Build TFX Image\": 86de3d566666: Already exists\n",
      "Step #5 - \"Build TFX Image\": 30d00d530989: Already exists\n",
      "Step #5 - \"Build TFX Image\": 69a2bfee9a44: Already exists\n",
      "Step #5 - \"Build TFX Image\": 381964195b8b: Already exists\n",
      "Step #5 - \"Build TFX Image\": fe1468e51d2b: Already exists\n",
      "Step #5 - \"Build TFX Image\": e807ad87032f: Already exists\n",
      "Step #5 - \"Build TFX Image\": 0c557f25f33e: Already exists\n",
      "Step #5 - \"Build TFX Image\": 67cab7d11474: Already exists\n",
      "Step #5 - \"Build TFX Image\": 4f4fb700ef54: Already exists\n",
      "Step #5 - \"Build TFX Image\": 999747c8e1ca: Already exists\n",
      "Step #5 - \"Build TFX Image\": e92bc58784f1: Already exists\n",
      "Step #5 - \"Build TFX Image\": a9d25440a572: Already exists\n",
      "Step #5 - \"Build TFX Image\": ee75ae25ade1: Already exists\n",
      "Step #5 - \"Build TFX Image\": b13c015c05f0: Already exists\n",
      "Step #5 - \"Build TFX Image\": 8f0d2639aefc: Already exists\n",
      "Step #5 - \"Build TFX Image\": 11646adc2850: Already exists\n",
      "Step #5 - \"Build TFX Image\": 14c7723c1bbe: Already exists\n",
      "Step #5 - \"Build TFX Image\": 6252b7e4a35a: Already exists\n",
      "Step #5 - \"Build TFX Image\": ae96ea101185: Already exists\n",
      "Step #5 - \"Build TFX Image\": 8553e38f9d3b: Already exists\n",
      "Step #5 - \"Build TFX Image\": b4375d47e797: Already exists\n",
      "Step #5 - \"Build TFX Image\": 906cdf1c6b78: Already exists\n",
      "Step #5 - \"Build TFX Image\": d70342317ce5: Already exists\n",
      "Step #5 - \"Build TFX Image\": acea7e9af8f8: Already exists\n",
      "Step #5 - \"Build TFX Image\": e9ec5ae321aa: Already exists\n",
      "Step #5 - \"Build TFX Image\": 32eed1f081f7: Already exists\n",
      "Step #5 - \"Build TFX Image\": 4b5c1c89bd3d: Already exists\n",
      "Step #5 - \"Build TFX Image\": 80c2cbe5e4a8: Already exists\n",
      "Step #5 - \"Build TFX Image\": 85c3c971789d: Already exists\n",
      "Step #5 - \"Build TFX Image\": fa58d293bde3: Already exists\n",
      "Step #5 - \"Build TFX Image\": a0efb95d3b56: Already exists\n",
      "Step #5 - \"Build TFX Image\": 1bb05b14fb7d: Already exists\n",
      "Step #5 - \"Build TFX Image\": 3cebcf201134: Already exists\n",
      "Step #5 - \"Build TFX Image\": Digest: sha256:5d99c562fcc484d1bd104abab75267f37586d23a97a5a6e354c7828a1d2dfb83\n",
      "Step #5 - \"Build TFX Image\": Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.8.0\n",
      "Step #5 - \"Build TFX Image\":  ---> 864d5f66048d\n",
      "Step #5 - \"Build TFX Image\": Step 2/6 : COPY requirements.txt requirements.txt\n",
      "Step #5 - \"Build TFX Image\":  ---> b46463eec5a9\n",
      "Step #5 - \"Build TFX Image\": Step 3/6 : RUN pip install -r requirements.txt\n",
      "Step #5 - \"Build TFX Image\":  ---> Running in ba379373f567\n",
      "Step #5 - \"Build TFX Image\": Collecting kfp==1.8.12\n",
      "Step #5 - \"Build TFX Image\":   Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.2/301.2 kB 7.8 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-bigquery==2.34.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (2.34.3)\n",
      "Step #5 - \"Build TFX Image\": Collecting google-cloud-bigquery-storage==2.13.2\n",
      "Step #5 - \"Build TFX Image\":   Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180.2/180.2 kB 21.3 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Collecting google-cloud-aiplatform==1.14.0\n",
      "Step #5 - \"Build TFX Image\":   Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 50.8 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-pubsub in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (2.12.1)\n",
      "Step #5 - \"Build TFX Image\": Collecting cloudml-hypertune==0.1.0.dev6\n",
      "Step #5 - \"Build TFX Image\":   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\": Collecting pytest==7.1.2\n",
      "Step #5 - \"Build TFX Image\":   Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 34.6 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow-data-validation==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow-transform==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tfx==1.8.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Collecting tensorflow-io==0.26.0\n",
      "Step #5 - \"Build TFX Image\":   Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.9/25.9 MB 73.6 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Collecting apache-beam[gcp]==2.39.0\n",
      "Step #5 - \"Build TFX Image\":   Downloading apache_beam-2.39.0-cp37-cp37m-manylinux2010_x86_64.whl (10.3 MB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/10.3 MB 113.0 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.0.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (5.4.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.31.5)\n",
      "Step #5 - \"Build TFX Image\": Collecting google-cloud-storage<2,>=1.20.0\n",
      "Step #5 - \"Build TFX Image\":   Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 16.6 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (12.0.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.12.11)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.35.0)\n",
      "Step #5 - \"Build TFX Image\": Collecting requests-toolbelt<1,>=0.8.0\n",
      "Step #5 - \"Build TFX Image\":   Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 kB 8.0 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (2.0.0)\n",
      "Step #5 - \"Build TFX Image\": Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "Step #5 - \"Build TFX Image\":   Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.0/58.0 kB 6.5 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\": Collecting jsonschema<4,>=3.0.1\n",
      "Step #5 - \"Build TFX Image\":   Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 7.5 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Collecting tabulate<1,>=0.8.6\n",
      "Step #5 - \"Build TFX Image\":   Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (7.1.2)\n",
      "Step #5 - \"Build TFX Image\": Collecting Deprecated<2,>=1.2.7\n",
      "Step #5 - \"Build TFX Image\":   Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Step #5 - \"Build TFX Image\": Collecting strip-hints<1,>=0.1.8\n",
      "Step #5 - \"Build TFX Image\":   Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\": Collecting docstring-parser<1,>=0.7.3\n",
      "Step #5 - \"Build TFX Image\":   Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (0.1.15)\n",
      "Step #5 - \"Build TFX Image\": Collecting fire<1,>=0.3.1\n",
      "Step #5 - \"Build TFX Image\":   Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 11.4 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (3.20.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (3.0.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp==1.8.12->-r requirements.txt (line 1)) (1.9.0)\n",
      "Step #5 - \"Build TFX Image\": Collecting typer<1.0,>=0.3.2\n",
      "Step #5 - \"Build TFX Image\":   Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Step #5 - \"Build TFX Image\": Collecting typing-extensions<4,>=3.7.4\n",
      "Step #5 - \"Build TFX Image\":   Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (20.9)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.3.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.20.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.8.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.27.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.46.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.2.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.14.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (1.0.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (20.3.0)\n",
      "Step #5 - \"Build TFX Image\": Collecting iniconfig\n",
      "Step #5 - \"Build TFX Image\":   Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (4.11.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest==7.1.2->-r requirements.txt (line 7)) (2.0.1)\n",
      "Step #5 - \"Build TFX Image\": Collecting py>=1.8.2\n",
      "Step #5 - \"Build TFX Image\":   Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 15.4 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: joblib<0.15,>=0.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.14.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pyarrow<6,>=1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (5.0.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow-metadata<1.9,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: numpy<2,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.21.6)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.3.5)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pyfarmhash<0.4,>=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.3.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tfx-bsl<1.9,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.16.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pydot<2,>=1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-transform==1.8.0->-r requirements.txt (line 10)) (1.4.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow-model-analysis<0.40,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.39.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: ml-pipelines-sdk==1.8.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: docker<5,>=4.1 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (4.4.4)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow-hub<0.13,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.12.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: portpicker<2,>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.5.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: keras-tuner<2,>=1.0.4 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.1.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (2.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: jinja2<4,>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (2.10)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: ml-metadata<1.9.0,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-apitools<1,>=0.5 in /opt/conda/lib/python3.7/site-packages (from tfx==1.8.0->-r requirements.txt (line 11)) (0.5.31)\n",
      "Step #5 - \"Build TFX Image\": Collecting tensorflow-io-gcs-filesystem==0.26.0\n",
      "Step #5 - \"Build TFX Image\":   Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 97.8 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.12.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2022.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.4.11)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.6.8)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.7)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.3.1.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.19.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2.7.0)\n",
      "Step #5 - \"Build TFX Image\": Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "Step #5 - \"Build TFX Image\":   Using cached google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.1.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.7.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.0.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.19.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.2.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.4.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (3.6.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.16.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.15.4)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (4.2.4)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (1.3.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: grpcio-status>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub->-r requirements.txt (line 5)) (1.46.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub->-r requirements.txt (line 5)) (0.12.4)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.8.12->-r requirements.txt (line 1)) (1.14.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<5,>=4.1->tfx==1.8.0->-r requirements.txt (line 11)) (1.3.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.8.12->-r requirements.txt (line 1)) (1.1.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12->-r requirements.txt (line 1)) (59.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==1.8.12->-r requirements.txt (line 1)) (1.56.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (4.1.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (0.17.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (4.8)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.8.12->-r requirements.txt (line 1)) (0.2.7)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (6.1.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.1.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.6.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (2.4.7)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest==7.1.2->-r requirements.txt (line 7)) (3.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2<4,>=2.7.3->tfx==1.8.0->-r requirements.txt (line 11)) (2.0.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.8.12->-r requirements.txt (line 1)) (0.18.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (1.0.4)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (7.33.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (1.26.9)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.8.12->-r requirements.txt (line 1)) (2021.10.8)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (1.3.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from portpicker<2,>=1.3.1->tfx==1.8.0->-r requirements.txt (line 11)) (5.9.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.0.12)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (3.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.8.12->-r requirements.txt (line 1)) (0.37.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.0rc0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.1.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.6.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (2.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (0.5.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (14.0.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (3.3.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (3.6.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (7.7.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: scipy<2,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.7.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (1.15.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5->tensorflow-data-validation==1.8.0->-r requirements.txt (line 9)) (1.5.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (5.2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.1.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.12.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (3.0.29)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (5.1.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.5)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (4.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.18.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.13.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (3.6.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (5.4.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.1.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx==1.8.0->-r requirements.txt (line 11)) (0.4.8)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]==2.39.0->-r requirements.txt (line 13)) (0.1.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.6.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (2.1.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (3.3.7)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.4.6)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (1.8.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp==1.8.12->-r requirements.txt (line 1)) (3.2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==2.34.3->-r requirements.txt (line 2)) (2.21)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.5.5)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (7.3.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.6.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.8.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (2.15.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (4.10.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner<2,>=1.0.4->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.5)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.4.11)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.4)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pyzmq>=22.3 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (22.3.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (6.4.5)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.14.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.8.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.13.3)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (21.3.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (1.5.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.7.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.8.4)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.2.2)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (4.11.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (5.0.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.5.13)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.6.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (21.2.0)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (2.3.1)\n",
      "Step #5 - \"Build TFX Image\": Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.40,>=0.39.0->tfx==1.8.0->-r requirements.txt (line 11)) (0.5.1)\n",
      "Step #5 - \"Build TFX Image\": Building wheels for collected packages: kfp, cloudml-hypertune, fire, kfp-server-api, strip-hints\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for kfp (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\":   Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=1374dd783511061809bb890437952033013d7024836accd22d665888f41f103b\n",
      "Step #5 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\":   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=856496eaae4324951a5e11fa04e738745903c4739ecd9ccaffa20a4446b427da\n",
      "Step #5 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for fire (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\":   Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=3dc4da7ffd4676b131468f3c0e2b03803c6f099c869549084bfa66aa6cfb0e84\n",
      "Step #5 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #6 - \"Compile Pipeline\": running bdist_wheel\n",
      "Step #6 - \"Compile Pipeline\": running build\n",
      "Step #6 - \"Compile Pipeline\": running build_py\n",
      "Step #6 - \"Compile Pipeline\": creating build\n",
      "Step #6 - \"Compile Pipeline\": creating build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying etl.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying transformations.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": installing to /tmp/tmpz7w_zqly\n",
      "Step #6 - \"Compile Pipeline\": running install\n",
      "Step #6 - \"Compile Pipeline\": running install_lib\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/etl.py -> /tmp/tmpz7w_zqly\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/transformations.py -> /tmp/tmpz7w_zqly\n",
      "Step #6 - \"Compile Pipeline\": running install_egg_info\n",
      "Step #6 - \"Compile Pipeline\": running egg_info\n",
      "Step #6 - \"Compile Pipeline\": creating tfx_user_code_DataTransformer.egg-info\n",
      "Step #6 - \"Compile Pipeline\": writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "Step #6 - \"Compile Pipeline\": writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "Step #6 - \"Compile Pipeline\": writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "Step #6 - \"Compile Pipeline\": writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Step #6 - \"Compile Pipeline\": reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Step #6 - \"Compile Pipeline\": writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Step #6 - \"Compile Pipeline\": Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmpz7w_zqly/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3.7.egg-info\n",
      "Step #6 - \"Compile Pipeline\": running install_scripts\n",
      "Step #6 - \"Compile Pipeline\": creating /tmp/tmpz7w_zqly/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL\n",
      "Step #6 - \"Compile Pipeline\": creating '/tmp/tmp4_7n2eif/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' and adding '/tmp/tmpz7w_zqly' to it\n",
      "Step #6 - \"Compile Pipeline\": adding 'etl.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'transformations.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/METADATA'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/top_level.txt'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/RECORD'\n",
      "Step #6 - \"Compile Pipeline\": removing /tmp/tmpz7w_zqly\n",
      "Step #6 - \"Compile Pipeline\": /opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "Step #6 - \"Compile Pipeline\":   setuptools.SetuptoolsDeprecationWarning,\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\":   Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99716 sha256=c6858a7b12e02abab3a6150edd45d2bdaffc2e0375dd80fce9b0c0fa559d84f8\n",
      "Step #5 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for strip-hints (setup.py): started\n",
      "Step #5 - \"Build TFX Image\":   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #5 - \"Build TFX Image\":   Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=7f593b6abd0c9deaf88595c0e50565c9df9142eeca811ffe1c2f9226a02eb6c0\n",
      "Step #5 - \"Build TFX Image\":   Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Step #5 - \"Build TFX Image\": Successfully built kfp cloudml-hypertune fire kfp-server-api strip-hints\n",
      "Step #5 - \"Build TFX Image\": Installing collected packages: typing-extensions, iniconfig, cloudml-hypertune, typer, tensorflow-io-gcs-filesystem, tabulate, strip-hints, py, fire, docstring-parser, Deprecated, tensorflow-io, requests-toolbelt, kfp-server-api, jsonschema, pytest, apache-beam, google-cloud-core, google-cloud-storage, google-cloud-bigquery-storage, kfp, google-cloud-aiplatform\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: typing-extensions\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: typing_extensions 4.2.0\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling typing_extensions-4.2.0:\n",
      "Step #6 - \"Compile Pipeline\": running bdist_wheel\n",
      "Step #6 - \"Compile Pipeline\": running build\n",
      "Step #6 - \"Compile Pipeline\": running build_py\n",
      "Step #6 - \"Compile Pipeline\": creating build\n",
      "Step #6 - \"Compile Pipeline\": creating build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying exporter.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying runner.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying data.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying model.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying defaults.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying trainer.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": copying task.py -> build/lib\n",
      "Step #6 - \"Compile Pipeline\": installing to /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": running install\n",
      "Step #6 - \"Compile Pipeline\": running install_lib\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/exporter.py -> /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/runner.py -> /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/data.py -> /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/model.py -> /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/defaults.py -> /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/trainer.py -> /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": copying build/lib/task.py -> /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": running install_egg_info\n",
      "Step #6 - \"Compile Pipeline\": running egg_info\n",
      "Step #6 - \"Compile Pipeline\": creating tfx_user_code_ModelTrainer.egg-info\n",
      "Step #6 - \"Compile Pipeline\": writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "Step #6 - \"Compile Pipeline\": writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "Step #6 - \"Compile Pipeline\": writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "Step #6 - \"Compile Pipeline\": writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Step #6 - \"Compile Pipeline\": reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Step #6 - \"Compile Pipeline\": writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Step #6 - \"Compile Pipeline\": Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmprc83p_uj/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7-py3.7.egg-info\n",
      "Step #6 - \"Compile Pipeline\": running install_scripts\n",
      "Step #6 - \"Compile Pipeline\": creating /tmp/tmprc83p_uj/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/WHEEL\n",
      "Step #6 - \"Compile Pipeline\": creating '/tmp/tmpz57isj2o/tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7-py3-none-any.whl' and adding '/tmp/tmprc83p_uj' to it\n",
      "Step #6 - \"Compile Pipeline\": adding 'data.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'defaults.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'exporter.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'model.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'runner.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'task.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'trainer.py'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/METADATA'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/WHEEL'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/top_level.txt'\n",
      "Step #6 - \"Compile Pipeline\": adding 'tfx_user_code_ModelTrainer-0.0+c551aa385dc755c911bb45ac0790eb4f2170e5e0bb4344e15548545091539cd7.dist-info/RECORD'\n",
      "Step #6 - \"Compile Pipeline\": removing /tmp/tmprc83p_uj\n",
      "Step #6 - \"Compile Pipeline\": /opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "Step #6 - \"Compile Pipeline\":   setuptools.SetuptoolsDeprecationWarning,\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled typing_extensions-4.2.0\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: tensorflow-io-gcs-filesystem 0.23.1\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling tensorflow-io-gcs-filesystem-0.23.1:\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled tensorflow-io-gcs-filesystem-0.23.1\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: tensorflow-io\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: tensorflow-io 0.23.1\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling tensorflow-io-0.23.1:\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled tensorflow-io-0.23.1\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: jsonschema\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: jsonschema 4.5.1\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling jsonschema-4.5.1:\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled jsonschema-4.5.1\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: apache-beam\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: apache-beam 2.38.0\n",
      "Step #6 - \"Compile Pipeline\": Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://pbalm-cxb-aa-eu/creditcards/tfx_artifacts/creditcards-cla\"}\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling apache-beam-2.38.0:\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled apache-beam-2.38.0\n",
      "Finished Step #6 - \"Compile Pipeline\"\n",
      "Starting Step #7 - \"Upload Pipeline to GCS\"\n",
      "Step #7 - \"Upload Pipeline to GCS\": Already have image (with digest): gcr.io/cloud-builders/gsutil\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: google-cloud-core\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: google-cloud-core 2.2.2\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling google-cloud-core-2.2.2:\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled google-cloud-core-2.2.2\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: google-cloud-storage\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: google-cloud-storage 2.2.1\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling google-cloud-storage-2.2.1:\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled google-cloud-storage-2.2.1\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: google-cloud-bigquery-storage\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: google-cloud-bigquery-storage 2.13.1\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling google-cloud-bigquery-storage-2.13.1:\n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled google-cloud-bigquery-storage-2.13.1\n",
      "Step #5 - \"Build TFX Image\":   Attempting uninstall: google-cloud-aiplatform\n",
      "Step #5 - \"Build TFX Image\":     Found existing installation: google-cloud-aiplatform 1.13.0\n",
      "Step #5 - \"Build TFX Image\":     Uninstalling google-cloud-aiplatform-1.13.0:\n",
      "Step #7 - \"Upload Pipeline to GCS\": Copying file://creditcards-classifier-v02-train-pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 31.1 KiB/ 31.1 KiB]                                                                                    \n",
      "Step #7 - \"Upload Pipeline to GCS\": Operation completed over 1 objects/31.1 KiB.                                     \n",
      "Step #5 - \"Build TFX Image\":       Successfully uninstalled google-cloud-aiplatform-1.13.0\n",
      "Finished Step #7 - \"Upload Pipeline to GCS\"\n",
      "Step #5 - \"Build TFX Image\": \u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #5 - \"Build TFX Image\": black 22.3.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "Step #5 - \"Build TFX Image\": \u001b[0mSuccessfully installed Deprecated-1.2.13 apache-beam-2.39.0 cloudml-hypertune-0.1.0.dev6 docstring-parser-0.14.1 fire-0.4.0 google-cloud-aiplatform-1.14.0 google-cloud-bigquery-storage-2.13.2 google-cloud-core-1.7.2 google-cloud-storage-2.1.0 iniconfig-1.1.1 jsonschema-3.2.0 kfp-1.8.12 kfp-server-api-1.8.2 py-1.11.0 pytest-7.1.2 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 tensorflow-io-0.26.0 tensorflow-io-gcs-filesystem-0.26.0 typer-0.4.1 typing-extensions-3.10.0.2\n",
      "Step #5 - \"Build TFX Image\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #5 - \"Build TFX Image\": \u001b[0m\u001b[91mWARNING: There was an error checking the latest version of pip.\n",
      "Step #5 - \"Build TFX Image\": \u001b[0mRemoving intermediate container ba379373f567\n",
      "Step #5 - \"Build TFX Image\":  ---> 1daa44d695e0\n",
      "Step #5 - \"Build TFX Image\": Step 4/6 : RUN pip install -U numpy --ignore-installed\n",
      "Step #5 - \"Build TFX Image\":  ---> Running in 8581ea02ce26\n",
      "Step #5 - \"Build TFX Image\": Collecting numpy\n",
      "Step #5 - \"Build TFX Image\":   Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Step #5 - \"Build TFX Image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 101.1 MB/s eta 0:00:00\n",
      "Step #5 - \"Build TFX Image\": Installing collected packages: numpy\n",
      "Step #5 - \"Build TFX Image\": \u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #5 - \"Build TFX Image\": statsmodels 0.13.2 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
      "Step #5 - \"Build TFX Image\": \u001b[0mSuccessfully installed numpy-1.21.6\n",
      "Step #5 - \"Build TFX Image\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #5 - \"Build TFX Image\": \u001b[0m\u001b[91mWARNING: There was an error checking the latest version of pip.\n",
      "Step #5 - \"Build TFX Image\": \u001b[0mRemoving intermediate container 8581ea02ce26\n",
      "Step #5 - \"Build TFX Image\":  ---> e2ae9fd826c3\n",
      "Step #5 - \"Build TFX Image\": Step 5/6 : COPY src/ src/\n",
      "Step #5 - \"Build TFX Image\":  ---> 665e673690c5\n",
      "Step #5 - \"Build TFX Image\": Step 6/6 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "Step #5 - \"Build TFX Image\":  ---> Running in 796e43e2ef59\n",
      "Step #5 - \"Build TFX Image\": Removing intermediate container 796e43e2ef59\n",
      "Step #5 - \"Build TFX Image\":  ---> 7b6501cd3c3a\n",
      "Step #5 - \"Build TFX Image\": Successfully built 7b6501cd3c3a\n",
      "Step #5 - \"Build TFX Image\": Successfully tagged europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:latest\n",
      "Finished Step #5 - \"Build TFX Image\"\n",
      "PUSH\n",
      "Pushing europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex:latest\n",
      "The push refers to repository [europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex]\n",
      "becd57fe3998: Preparing\n",
      "21416c260387: Preparing\n",
      "658bac6dd7a4: Preparing\n",
      "4b181380b67b: Preparing\n",
      "849f99ab0557: Preparing\n",
      "003ab0deb210: Preparing\n",
      "051b5111dbe3: Preparing\n",
      "105aac973237: Preparing\n",
      "6b279ee1dea4: Preparing\n",
      "051b5111dbe3: Waiting\n",
      "105aac973237: Waiting\n",
      "0f5815af70ed: Preparing\n",
      "a439fe54d797: Preparing\n",
      "003ab0deb210: Waiting\n",
      "e5bb7384706a: Preparing\n",
      "529b51f6018a: Preparing\n",
      "4d5391a66f17: Preparing\n",
      "6b279ee1dea4: Waiting\n",
      "0f5815af70ed: Waiting\n",
      "8776dda77d84: Preparing\n",
      "a439fe54d797: Waiting\n",
      "f7bf6100a736: Preparing\n",
      "529b51f6018a: Waiting\n",
      "4d5391a66f17: Waiting\n",
      "2427ba19d9ab: Preparing\n",
      "c5d8ddc90738: Preparing\n",
      "f9d66d415903: Preparing\n",
      "0a0b70a03299: Preparing\n",
      "01d285020d37: Preparing\n",
      "ad52cc5ce980: Preparing\n",
      "40d867f1633d: Preparing\n",
      "f7bf6100a736: Waiting\n",
      "2427ba19d9ab: Waiting\n",
      "c5d8ddc90738: Waiting\n",
      "f9d66d415903: Waiting\n",
      "0a0b70a03299: Waiting\n",
      "01d285020d37: Waiting\n",
      "695cde20e218: Preparing\n",
      "eab9c045ef1b: Preparing\n",
      "ad7c511b31df: Preparing\n",
      "cd9f5c9bd89e: Preparing\n",
      "ca40136e604d: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "08e753b98db4: Preparing\n",
      "8f9243705224: Preparing\n",
      "ba42d5f65b46: Preparing\n",
      "51981f322139: Preparing\n",
      "cbe679dd18e3: Preparing\n",
      "40d867f1633d: Waiting\n",
      "ce07be50029c: Preparing\n",
      "695cde20e218: Waiting\n",
      "7011392a3aa0: Preparing\n",
      "0cfddc66f231: Preparing\n",
      "eab9c045ef1b: Waiting\n",
      "ca40136e604d: Waiting\n",
      "a4a375cdde15: Preparing\n",
      "ad7c511b31df: Waiting\n",
      "ac5045d5adeb: Preparing\n",
      "5f70bf18a086: Waiting\n",
      "bf8cedc62fb3: Preparing\n",
      "cd9f5c9bd89e: Waiting\n",
      "08e753b98db4: Waiting\n",
      "8f9243705224: Waiting\n",
      "0cfddc66f231: Waiting\n",
      "ba42d5f65b46: Waiting\n",
      "cbe679dd18e3: Waiting\n",
      "a4a375cdde15: Waiting\n",
      "51981f322139: Waiting\n",
      "ac5045d5adeb: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "ce07be50029c: Waiting\n",
      "849f99ab0557: Layer already exists\n",
      "003ab0deb210: Layer already exists\n",
      "051b5111dbe3: Layer already exists\n",
      "105aac973237: Layer already exists\n",
      "4b181380b67b: Pushed\n",
      "6b279ee1dea4: Layer already exists\n",
      "becd57fe3998: Pushed\n",
      "0f5815af70ed: Layer already exists\n",
      "a439fe54d797: Layer already exists\n",
      "e5bb7384706a: Layer already exists\n",
      "529b51f6018a: Layer already exists\n",
      "4d5391a66f17: Layer already exists\n",
      "8776dda77d84: Layer already exists\n",
      "f7bf6100a736: Layer already exists\n",
      "2427ba19d9ab: Layer already exists\n",
      "c5d8ddc90738: Layer already exists\n",
      "f9d66d415903: Layer already exists\n",
      "0a0b70a03299: Layer already exists\n",
      "01d285020d37: Layer already exists\n",
      "21416c260387: Pushed\n",
      "ad52cc5ce980: Layer already exists\n",
      "40d867f1633d: Layer already exists\n",
      "695cde20e218: Layer already exists\n",
      "eab9c045ef1b: Layer already exists\n",
      "ad7c511b31df: Layer already exists\n",
      "ca40136e604d: Layer already exists\n",
      "cd9f5c9bd89e: Layer already exists\n",
      "8f9243705224: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "08e753b98db4: Layer already exists\n",
      "ba42d5f65b46: Layer already exists\n",
      "51981f322139: Layer already exists\n",
      "ce07be50029c: Layer already exists\n",
      "cbe679dd18e3: Layer already exists\n",
      "7011392a3aa0: Layer already exists\n",
      "0cfddc66f231: Layer already exists\n",
      "ac5045d5adeb: Layer already exists\n",
      "a4a375cdde15: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "658bac6dd7a4: Pushed\n",
      "latest: digest: sha256:1af834e2d42f449000a2c16947755330369f0f072347f0981528b2c55dd6a7e2 size: 8726\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE  IMAGES                                                                 STATUS\n",
      "22e94189-b4c4-489e-b1c5-d4b7d7f8ff69  2022-06-30T11:03:21+00:00  12M24S    -       europe-west4-docker.pkg.dev/pbalm-cxb-aa/creditcards/vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --no-source --timeout=60m --config build/pipeline-deployment.yaml --substitutions {SUBSTITUTIONS} --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd317880-9274-4cd2-8a56-4d743a9b396c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
