{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109707f2",
   "metadata": {},
   "source": [
    "# 04 - Test and Deploy Training Pipeline to Vertex Pipelines\n",
    "\n",
    "The purpose of this notebook is to test, deploy, and run the `TFX` pipeline on `Vertex Pipelines`. The notebook covers the following tasks:\n",
    "1. Run the tests locally.\n",
    "2. Run the pipeline using `Vertex Pipelines`\n",
    "3. Execute the pipeline deployment `CI/CD` steps using `Cloud Build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a51af1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e7d80",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3bb184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Tensorflow Version: 1.8.0\n",
      "KFP Version: 1.8.12\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "import tfx.v1 as tfx\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)\n",
    "print(\"KFP Version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bdded",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b4b22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: pbalm-cxb-aa\n",
      "Region: europe-west4\n",
      "Bucket name: pbalm-cxb-aa-eu\n",
      "Service Account: 188940921537-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'pbalm-cxb-aa'\n",
    "REGION = 'europe-west4'\n",
    "CF_REGION = 'europe-west1' # No Cloud Functions in europe-west4\n",
    "BUCKET =  PROJECT + '-eu'\n",
    "SERVICE_ACCOUNT = \"188940921537-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP project id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn't exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a9dc9",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a75e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_LOCATION = 'EU'\n",
    "BQ_DATASET_NAME = 'vertex_eu' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'creditcards_ml'\n",
    "\n",
    "VERSION = 'v02'\n",
    "DATASET_DISPLAY_NAME = 'creditcards'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT}/creditcards/{CICD_IMAGE_NAME}\"\n",
    "\n",
    "DATAFLOW_REGION = 'europe-west4'\n",
    "DATAFLOW_SERVICE_ACCOUNT = SERVICE_ACCOUNT\n",
    "DATAFLOW_SUBNETWORK = f'https://www.googleapis.com/compute/v1/projects/{PROJECT}/regions/{REGION}/subnetworks/default'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06be3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'src/raw_schema/.ipynb_checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r src/raw_schema/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44678373",
   "metadata": {},
   "source": [
    "## 1. Run the CICD steps locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68204ed",
   "metadata": {},
   "source": [
    "### Set pipeline configurations for the local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05a1d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] =  MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"BQ_TABLE_NAME\"] = BQ_TABLE_NAME\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"1000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"100\"\n",
    "os.environ[\"UPLOAD_MODEL\"] = \"1\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"-0.1\"    # NB Negative accuracy threshold makes no sense - allows everything\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d353e3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: pbalm-cxb-aa\n",
      "REGION: europe-west4\n",
      "GCS_LOCATION: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests\n",
      "ARTIFACT_STORE_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry\n",
      "DATASET_DISPLAY_NAME: creditcards\n",
      "MODEL_DISPLAY_NAME: creditcards-classifier-v02\n",
      "PIPELINE_NAME: creditcards-classifier-v02-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 1000\n",
      "TEST_LIMIT: 100\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: -0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/pbalm-cxb-aa/tfx-creditcards:latest\n",
      "BEAM_RUNNER: DirectRunner\n",
      "SERVICE_ACCOUNT: \n",
      "SUBNETWORK: \n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', '--region=europe-west4', '--runner=DirectRunner', '--service_account_email=', '--no_use_public_ips', '--subnetwork=']\n",
      "TRAINING_RUNNER: local\n",
      "VERTEX_TRAINING_ARGS: {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/pbalm-cxb-aa/tfx-creditcards:latest'}}]}\n",
      "VERTEX_TRAINING_CONFIG: {'ai_platform_training_enable_ucaip': True, 'ai_platform_training_ucaip_region': 'europe-west4', 'ai_platform_training_args': {'project': 'pbalm-cxb-aa', 'worker_pool_specs': [{'machine_spec': {'machine_type': 'n1-standard-4'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/pbalm-cxb-aa/tfx-creditcards:latest'}}]}, 'use_gpu': False}\n",
      "SERVING_RUNTIME: tf2-cpu.2-5\n",
      "SERVING_IMAGE_URI: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', 'gcs_location': 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp', 'project': 'pbalm-cxb-aa', 'region': 'europe-west4', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: creditcards-classifier-v02-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 1\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d51c12",
   "metadata": {},
   "source": [
    "### Run unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be84a8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 2 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/datasource_utils_tests.py BigQuery Source: pbalm-cxb-aa.vertex_eu.creditcards_ml\n",
      "\u001b[32m.\u001b[0mBigQuery Source: pbalm-cxb-aa.vertex_eu.creditcards_ml\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 5.85s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4358f955",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 3 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/model_tests.py \u001b[32m.\u001b[0m\u001b[33ms\u001b[0m2022-06-24 13:33:34.438742: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-24 13:33:34.440267: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['V1[0][0]',                     \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'Amount[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           1920        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           2080        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,033\n",
      "Trainable params: 4,033\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19\n",
      "  /opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "    'nearest': pil_image.NEAREST,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "    'bilinear': pil_image.BILINEAR,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "    'bicubic': pil_image.BICUBIC,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    if hasattr(pil_image, 'HAMMING'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    if hasattr(pil_image, 'BOX'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    if hasattr(pil_image, 'LANCZOS'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================== \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m1 skipped\u001b[0m, \u001b[33m\u001b[1m10 warnings\u001b[0m\u001b[33m in 2.68s\u001b[0m\u001b[33m ===================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa00fd5",
   "metadata": {},
   "source": [
    "### Run e2e pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb9aad70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.12, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.6.1\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py upload_model: 1\n",
      "Pipeline e2e test artifacts stored in: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests\n",
      "ML metadata store is ready.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Excluding no splits because exclude_splits is not set.\n",
      "Labels for model: {\"dataset_name\": \"creditcards\", \"pipeline_name\": \"creditcards-classifier-v02-train-pipeline\", \"pipeline_root\": \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/credi\"}\n",
      "Pipeline components: ['HyperparamsGen', 'TrainDataGen', 'TestDataGen', 'StatisticsGen', 'SchemaImporter', 'ExampleValidator', 'DataTransformer', 'WarmstartModelResolver', 'ModelTrainer', 'BaselineModelResolver', 'ModelEvaluator', 'GcsModelPusher', 'VertexUploader']\n",
      "Beam pipeline args: ['--project=pbalm-cxb-aa', '--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp']\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai/src/preprocessing/transformations.py' (including modules: ['etl', 'transformations']).\n",
      "User module package has hash fingerprint version 29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.\n",
      "Executing: ['/opt/conda/bin/python3.7', '/tmp/tmp28opvkxw/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpwojj2emi', '--dist-dir', '/tmp/tmp46xbs44c']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying etl.py -> build/lib\n",
      "copying transformations.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmpwojj2emi\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformations.py -> /tmp/tmpwojj2emi\n",
      "copying build/lib/etl.py -> /tmp/tmpwojj2emi\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_DataTransformer.egg-info\n",
      "writing tfx_user_code_DataTransformer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_DataTransformer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_DataTransformer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_DataTransformer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_DataTransformer.egg-info to /tmp/tmpwojj2emi/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpwojj2emi/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL\n",
      "creating '/tmp/tmp46xbs44c/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' and adding '/tmp/tmpwojj2emi' to it\n",
      "adding 'etl.py'\n",
      "adding 'transformations.py'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/METADATA'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b.dist-info/RECORD'\n",
      "removing /tmp/tmpwojj2emi\n",
      "Successfully built user code wheel distribution at 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'; target user module is 'transformations'.\n",
      "Full user module path is 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'\n",
      "Generating ephemeral wheel package for '/home/jupyter/mlops-with-vertex-ai/src/model_training/runner.py' (including modules: ['trainer', 'runner', 'model', 'defaults', 'exporter', 'data', 'task']).\n",
      "User module package has hash fingerprint version 0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0.\n",
      "Executing: ['/opt/conda/bin/python3.7', '/tmp/tmpj0ewjjc6/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpbv8rle8s', '--dist-dir', '/tmp/tmp31v2rftg']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying trainer.py -> build/lib\n",
      "copying runner.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying defaults.py -> build/lib\n",
      "copying exporter.py -> build/lib\n",
      "copying data.py -> build/lib\n",
      "copying task.py -> build/lib\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "installing to /tmp/tmpbv8rle8s\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/trainer.py -> /tmp/tmpbv8rle8s\n",
      "copying build/lib/model.py -> /tmp/tmpbv8rle8s\n",
      "copying build/lib/runner.py -> /tmp/tmpbv8rle8s\n",
      "copying build/lib/task.py -> /tmp/tmpbv8rle8s\n",
      "copying build/lib/data.py -> /tmp/tmpbv8rle8s\n",
      "copying build/lib/defaults.py -> /tmp/tmpbv8rle8s\n",
      "copying build/lib/exporter.py -> /tmp/tmpbv8rle8s\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_ModelTrainer.egg-info\n",
      "writing tfx_user_code_ModelTrainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_ModelTrainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_ModelTrainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_ModelTrainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_ModelTrainer.egg-info to /tmp/tmpbv8rle8s/tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpbv8rle8s/tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0.dist-info/WHEEL\n",
      "creating '/tmp/tmp31v2rftg/tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0-py3-none-any.whl' and adding '/tmp/tmpbv8rle8s' to it\n",
      "adding 'data.py'\n",
      "adding 'defaults.py'\n",
      "adding 'exporter.py'\n",
      "adding 'model.py'\n",
      "adding 'runner.py'\n",
      "adding 'task.py'\n",
      "adding 'trainer.py'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0.dist-info/METADATA'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0.dist-info/RECORD'\n",
      "removing /tmp/tmpbv8rle8s\n",
      "Successfully built user code wheel distribution at 'gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0-py3-none-any.whl'; target user module is 'runner'.\n",
      "Full user module path is 'runner@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0-py3-none-any.whl'\n",
      "Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"DataTransformer\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"GcsModelPusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"HyperparamsGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.hyperparameters_gen_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelEvaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ModelTrainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--project=pbalm-cxb-aa\"\n",
      "      beam_pipeline_args: \"--temp_location=gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/temp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"VertexUploader\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"src.tfx_pipelines.components.vertex_model_uploader_Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TestDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"TrainDataGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.QueryBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"mlmd.sqllite\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"mlmd.sqllite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "Component BaselineModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"BaselineModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.BaselineModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"model\"\n",
      "      input_keys: \"model_blessing\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type ModelBlessing is not found in MLMD.\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component BaselineModelResolver is finished.\n",
      "Component HyperparamsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 2\n",
      "Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={}, output_dict=defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}), exec_properties={'learning_rate': 0.001, 'hidden_units': '128,128', 'batch_size': 512, 'num_epochs': 1}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir/2022-06-27T10:00:52.552595', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"src.tfx_pipelines.components.hyperparameters_gen\"\n",
      "  }\n",
      "  id: \"HyperparamsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.HyperparamsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"hyperparameters\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"HyperParameters\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"batch_size\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 512\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"hidden_units\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"128,128\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"learning_rate\"\n",
      "    value {\n",
      "      field_value {\n",
      "        double_value: 0.001\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"num_epochs\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-27T10:00:52.552595')\n",
      "Hyperparameters: {'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "Hyperparameters are written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2/hyperparameters.json\n",
      "Cleaning up stateless execution info.\n",
      "Execution 2 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'hyperparameters': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/HyperparamsGen/hyperparameters/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:HyperparamsGen:hyperparameters:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:HyperparamsGen:hyperparameters:0\"\n",
      ", artifact_type: name: \"HyperParameters\"\n",
      ")]}) for execution 2\n",
      "MetadataStore with DB connection initialized\n",
      "Component HyperparamsGen is finished.\n",
      "Component SchemaImporter is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "  }\n",
      "  id: \"SchemaImporter\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"result\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"artifact_uri\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"src/raw_schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"reimport\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an importer node.\n",
      "MetadataStore with DB connection initialized\n",
      "Processing source uri: src/raw_schema, properties: {}, custom_properties: {}\n",
      "Component SchemaImporter is finished.\n",
      "Component TestDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 4\n",
      "Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\"\\n    }\\n  ]\\n}', 'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"test\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir/2022-06-27T10:00:52.552595', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TestDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TestDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'TEST\\'\\\\n    LIMIT 100\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"test\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelEvaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-27T10:00:52.552595')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpqnn3omwb/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpqnn3omwb/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpqnn3omwb/build/tfx/setup.log\n",
      "E0627 10:00:58.609237514   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmpqnn3omwb/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f5073c2ccb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f5073c2cdd0> ====================\n",
      "==================== <function pack_combiners at 0x7f5073c32320> ====================\n",
      "==================== <function lift_combiners at 0x7f5073c323b0> ====================\n",
      "==================== <function expand_sdf at 0x7f5073c32560> ====================\n",
      "==================== <function expand_gbk at 0x7f5073c325f0> ====================\n",
      "==================== <function sink_flattens at 0x7f5073c32710> ====================\n",
      "==================== <function greedily_fuse at 0x7f5073c327a0> ====================\n",
      "==================== <function read_to_impulse at 0x7f5073c32830> ====================\n",
      "==================== <function impulse_to_input at 0x7f5073c328c0> ====================\n",
      "==================== <function sort_stages at 0x7f5073c32b00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f5073c32c20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f5073c32a70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f5073c32b90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f50703187d0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "E0627 10:01:00.903017867   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Setting socket default timeout to 60 seconds.\n",
      "socket default timeout is 60.0 seconds.\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Using location 'EU' from table <TableReference\n",
      " datasetId: 'vertex_eu'\n",
      " projectId: 'pbalm-cxb-aa'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM vertex_eu.creditcards_ml \n",
      "    WHERE ML_use = 'TEST'\n",
      "    LIMIT 100\n",
      "Dataset pbalm-cxb-aa:beam_temp_dataset_a3ab08c6ec0c4d36bab07980a6c91c68 does not exist so we will create it as temporary with location=EU\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_245399f5-0_1656324063_72'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_245399f5-0_1656324063_72\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_245399f5-0_1656324069_39'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_245399f5-0_1656324069_39\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033262014389038086 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.032447099685668945 seconds.\n",
      "Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03488588333129883 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02939295768737793 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030890941619873047 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 4 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TestDataGen/examples/4\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TestDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TestDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 4\n",
      "MetadataStore with DB connection initialized\n",
      "Component TestDataGen is finished.\n",
      "Component TrainDataGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 5\n",
      "Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 4,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': None}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir/2022-06-27T10:00:52.552595', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"TrainDataGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n    SELECT *\\\\n    \\\\n    EXCEPT (Time, ML_use)\\\\n    FROM vertex_eu.creditcards_ml \\\\n    WHERE ML_use = \\'UNASSIGNED\\'\\\\n    LIMIT 1000\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-27T10:00:52.552595')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpr_vit4mx/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpr_vit4mx/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpr_vit4mx/build/tfx/setup.log\n",
      "E0627 10:01:21.810430986   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmpr_vit4mx/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Length of label `tfx-extensions-google_cloud_big_query-example_gen-executor-executor` exceeds maximum length(63), trimmed.\n",
      "Generating examples.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f5073c2ccb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f5073c2cdd0> ====================\n",
      "==================== <function pack_combiners at 0x7f5073c32320> ====================\n",
      "==================== <function lift_combiners at 0x7f5073c323b0> ====================\n",
      "==================== <function expand_sdf at 0x7f5073c32560> ====================\n",
      "==================== <function expand_gbk at 0x7f5073c325f0> ====================\n",
      "==================== <function sink_flattens at 0x7f5073c32710> ====================\n",
      "==================== <function greedily_fuse at 0x7f5073c327a0> ====================\n",
      "==================== <function read_to_impulse at 0x7f5073c32830> ====================\n",
      "==================== <function impulse_to_input at 0x7f5073c328c0> ====================\n",
      "==================== <function sort_stages at 0x7f5073c32b00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f5073c32c20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f5073c32a70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f5073c32b90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f5070195110> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Started BigQuery job: <JobReference\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa None\n",
      "Using location 'EU' from table <TableReference\n",
      " datasetId: 'vertex_eu'\n",
      " projectId: 'pbalm-cxb-aa'\n",
      " tableId: 'creditcards_ml'> referenced by query \n",
      "    SELECT *\n",
      "    \n",
      "    EXCEPT (Time, ML_use)\n",
      "    FROM vertex_eu.creditcards_ml \n",
      "    WHERE ML_use = 'UNASSIGNED'\n",
      "    LIMIT 1000\n",
      "Dataset pbalm-cxb-aa:beam_temp_dataset_2fd94cb56656495bad9aaeb4e6422049 does not exist so we will create it as temporary with location=EU\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_QUERY_BQ_EXPORT_JOB_96334ae2-2_1656324086_188'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_QUERY_BQ_EXPORT_JOB_96334ae2-2_1656324086_188\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Started BigQuery job: <JobReference\n",
      " jobId: 'beam_bq_job_EXPORT_BQ_EXPORT_JOB_96334ae2-2_1656324092_651'\n",
      " location: 'EU'\n",
      " projectId: 'pbalm-cxb-aa'>\n",
      " bq show -j --format=prettyjson --project_id=pbalm-cxb-aa beam_bq_job_EXPORT_BQ_EXPORT_JOB_96334ae2-2_1656324092_651\n",
      "Job status: RUNNING\n",
      "Job status: DONE\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03392529487609863 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03915739059448242 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.026682138442993164 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04382801055908203 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03471231460571289 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.042545318603515625 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.027267932891845703 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04058265686035156 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Examples generated.\n",
      "Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "Value type <class 'NoneType'> of key input_fingerprint in exec_properties is not supported, going to drop it\n",
      "Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "Cleaning up stateless execution info.\n",
      "Execution 5 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 5\n",
      "MetadataStore with DB connection initialized\n",
      "Component TrainDataGen is finished.\n",
      "Component WarmstartModelResolver is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"WarmstartModelResolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.WarmstartModelResolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"latest_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  resolver_config {\n",
      "    resolver_steps {\n",
      "      class_path: \"tfx.dsl.input_resolution.strategies.latest_artifact_strategy.LatestArtifactStrategy\"\n",
      "      config_json: \"{}\"\n",
      "      input_keys: \"latest_model\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "Running as an resolver node.\n",
      "MetadataStore with DB connection initialized\n",
      "Artifact type Model is not found in MLMD.\n",
      "Component WarmstartModelResolver is finished.\n",
      "Component StatisticsGen is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 7\n",
      "Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1656324102104\n",
      "last_update_time_since_epoch: 1656324102104\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir/2022-06-27T10:00:52.552595', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-27T10:00:52.552595')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpwrk7ikmu/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpwrk7ikmu/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpwrk7ikmu/build/tfx/setup.log\n",
      "E0627 10:01:44.729719853   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Added --extra_package=/tmp/tmpwrk7ikmu/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Generating statistics for split train.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.043297529220581055 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split train written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-train.\n",
      "Generating statistics for split eval.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03474020957946777 seconds.\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Using Any for unsupported type: typing.Mapping[tensorflow_data_validation.types.FeaturePath, ForwardRef('schema_pb2.FeatureType')]\n",
      "Statistics for split eval written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7/Split-eval.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f5073c2ccb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f5073c2cdd0> ====================\n",
      "==================== <function pack_combiners at 0x7f5073c32320> ====================\n",
      "==================== <function lift_combiners at 0x7f5073c323b0> ====================\n",
      "==================== <function expand_sdf at 0x7f5073c32560> ====================\n",
      "==================== <function expand_gbk at 0x7f5073c325f0> ====================\n",
      "==================== <function sink_flattens at 0x7f5073c32710> ====================\n",
      "==================== <function greedily_fuse at 0x7f5073c327a0> ====================\n",
      "==================== <function read_to_impulse at 0x7f5073c32830> ====================\n",
      "==================== <function impulse_to_input at 0x7f5073c328c0> ====================\n",
      "==================== <function sort_stages at 0x7f5073c32b00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f5073c32c20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f5073c32a70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f5073c32b90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f506adb7590> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033257246017456055 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.026165485382080078 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.027663230895996094 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033039093017578125 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03299522399902344 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03174734115600586 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 7 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:StatisticsGen:statistics:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 7\n",
      "MetadataStore with DB connection initialized\n",
      "Component StatisticsGen is finished.\n",
      "Component ExampleValidator is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 8\n",
      "Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'statistics': [Artifact(artifact: id: 5\n",
      "type_id: 22\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/StatisticsGen/statistics/7\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:StatisticsGen:statistics:0\"\n",
      "create_time_since_epoch: 1656324111500\n",
      "last_update_time_since_epoch: 1656324111500\n",
      ", artifact_type: id: 22\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1656324055594\n",
      "last_update_time_since_epoch: 1656324055594\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir/2022-06-27T10:00:52.552595', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"DataTransformer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-27T10:00:52.552595')\n",
      "Validating schema against the computed statistics for split train.\n",
      "Validation complete for split train. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-train.\n",
      "Validating schema against the computed statistics for split eval.\n",
      "Validation complete for split eval. Anomalies written to gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8/Split-eval.\n",
      "Cleaning up stateless execution info.\n",
      "Execution 8 succeeded.\n",
      "Cleaning up stateful execution info.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ExampleValidator/anomalies/8\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:ExampleValidator:anomalies:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:ExampleValidator:anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 8\n",
      "MetadataStore with DB connection initialized\n",
      "Component ExampleValidator is finished.\n",
      "Component DataTransformer is running.\n",
      "Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "MetadataStore with DB connection initialized\n",
      "MetadataStore with DB connection initialized\n",
      "Going to run a new execution 9\n",
      "Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'schema': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"src/raw_schema\"\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1656324055594\n",
      "last_update_time_since_epoch: 1656324055594\n",
      ", artifact_type: id: 18\n",
      "name: \"Schema\"\n",
      ")], 'examples': [Artifact(artifact: id: 4\n",
      "type_id: 20\n",
      "uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/TrainDataGen/examples/5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:TrainDataGen:examples:0\"\n",
      "create_time_since_epoch: 1656324102104\n",
      "last_update_time_since_epoch: 1656324102104\n",
      ", artifact_type: id: 20\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'updated_analyzer_cache': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/updated_analyzer_cache/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:updated_analyzer_cache:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:updated_analyzer_cache:0\"\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:pre_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:pre_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transform_graph': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:transform_graph:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:transform_graph:0\"\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_anomalies/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:post_transform_anomalies:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:post_transform_anomalies:0\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:transformed_examples:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:transformed_examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:post_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:post_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'pre_transform_stats': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/pre_transform_stats/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:pre_transform_stats:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:pre_transform_stats:0\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/post_transform_schema/9\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:post_transform_schema:0\"\n",
      "  }\n",
      "}\n",
      "name: \"creditcards-classifier-v02-train-pipeline:2022-06-27T10:00:52.552595:DataTransformer:post_transform_schema:0\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}), exec_properties={'custom_config': 'null', 'splits_config': '{\\n  \"analyze\": [\\n    \"train\"\\n  ],\\n  \"transform\": [\\n    \"train\",\\n    \"eval\"\\n  ]\\n}', 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'disable_statistics': 0, 'force_tf_compat_v1': 0}, execution_output_uri='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/executor_output.pb', stateful_working_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir/2022-06-27T10:00:52.552595', tmp_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"DataTransformer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-06-27T10:00:52.552595\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"creditcards-classifier-v02-train-pipeline.DataTransformer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"TrainDataGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.TrainDataGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaImporter\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-06-27T10:00:52.552595\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"creditcards-classifier-v02-train-pipeline.SchemaImporter\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"result\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"splits_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"analyze\\\": [\\n    \\\"train\\\"\\n  ],\\n  \\\"transform\\\": [\\n    \\\"train\\\",\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"ExampleValidator\"\n",
      "upstream_nodes: \"SchemaImporter\"\n",
      "upstream_nodes: \"TrainDataGen\"\n",
      "downstream_nodes: \"ModelTrainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"creditcards-classifier-v02-train-pipeline\"\n",
      ", pipeline_run_id='2022-06-27T10:00:52.552595')\n",
      "Attempting to infer TFX Python dependency for beam\n",
      "Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpl56valf7/build/tfx\n",
      "Generating a temp setup file at /tmp/tmpl56valf7/build/tfx/setup.py\n",
      "Creating temporary sdist package, logs available at /tmp/tmpl56valf7/build/tfx/setup.log\n",
      "Added --extra_package=/tmp/tmpl56valf7/build/tfx/dist/tfx_ephemeral-1.8.0.tar.gz to beam args\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "Installing '/tmp/tmpwua9829c/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmpv7z5lcfv', '/tmp/tmpwua9829c/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "E0627 10:02:02.696602568   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmpwua9829c/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Successfully installed '/tmp/tmpwua9829c/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "udf_utils.get_fn {'module_file': None, 'module_path': 'transformations@gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/_wheels/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "Installing '/tmp/tmpskfmrf5m/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmphf_m2bp9', '/tmp/tmpskfmrf5m/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "E0627 10:02:06.082027335   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmpskfmrf5m/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Successfully installed '/tmp/tmpskfmrf5m/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "Installing '/tmp/tmpiqqpykdu/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl' to a temporary directory.\n",
      "Executing: ['/opt/conda/bin/python3.7', '-m', 'pip', 'install', '--target', '/tmp/tmpd56intf_', '/tmp/tmpiqqpykdu/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl']\n",
      "Processing /tmp/tmpiqqpykdu/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-DataTransformer\n",
      "Successfully installed tfx-user-code-DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b\n",
      "Successfully installed '/tmp/tmpiqqpykdu/tfx_user_code_DataTransformer-0.0+29bc5439691a704c7e799c0c5bab8e8ad062a220045b7f41c288244ffe47a05b-py3-none-any.whl'.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "2022-06-27 10:02:13.464157: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-27 10:02:13.465440: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "From /opt/conda/lib/python3.7/site-packages/tensorflow_transform/tf_utils.py:326: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02886676788330078 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03151583671569824 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03610944747924805 seconds.\n",
      "Feature V1 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V2 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V3 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V4 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V5 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V6 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V7 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V8 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V9 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V10 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V11 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V12 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V13 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V14 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V15 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V16 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V17 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V18 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V19 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V20 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V21 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V22 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V23 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V24 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V25 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V26 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V27 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature V28 has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Amount has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Feature Class has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f5073c2ccb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f5073c2cdd0> ====================\n",
      "==================== <function pack_combiners at 0x7f5073c32320> ====================\n",
      "==================== <function lift_combiners at 0x7f5073c323b0> ====================\n",
      "==================== <function expand_sdf at 0x7f5073c32560> ====================\n",
      "==================== <function expand_gbk at 0x7f5073c325f0> ====================\n",
      "==================== <function sink_flattens at 0x7f5073c32710> ====================\n",
      "==================== <function greedily_fuse at 0x7f5073c327a0> ====================\n",
      "==================== <function read_to_impulse at 0x7f5073c32830> ====================\n",
      "==================== <function impulse_to_input at 0x7f5073c328c0> ====================\n",
      "==================== <function sort_stages at 0x7f5073c32b00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f5073c32c20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f5073c32a70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f5073c32b90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f506ab2b990> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035108327865600586 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.036615610122680664 seconds.\n",
      "2022-06-27 10:02:40.158093: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/2dce4e353b474e649a8b4f5bb6891e3d/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03649616241455078 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03374123573303223 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03632760047912598 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0323185920715332 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.031197071075439453 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03574323654174805 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030604839324951172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02904367446899414 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03583550453186035 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03525137901306152 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.041509151458740234 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028846263885498047 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0313258171081543 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03551149368286133 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.033507585525512695 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0381016731262207 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0276339054107666 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029469013214111328 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03496122360229492 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029111146926879883 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031038761138916016 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03037118911743164 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030755043029785156 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.034507036209106445 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03410816192626953 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.027186870574951172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.025663375854492188 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029875516891479492 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03517317771911621 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03478646278381348 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030281543731689453 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.037505149841308594 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028223752975463867 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03119039535522461 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03557419776916504 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.029707908630371094 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02879643440246582 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02895975112915039 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03584647178649902 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0345911979675293 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03072381019592285 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02739691734313965 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02997422218322754 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.04313230514526367 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03375816345214844 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.040749311447143555 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.036711931228637695 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03654932975769043 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.030846118927001953 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03773307800292969 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.045101165771484375 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.31 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03615593910217285 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029719829559326172 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.029818296432495117 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03270554542541504 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.02901744842529297 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04049324989318848 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.031537771224975586 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03380990028381348 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03375673294067383 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028941869735717773 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03931140899658203 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03526115417480469 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03645610809326172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.028910398483276367 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.029262065887451172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.036154985427856445 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.031690359115600586 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03137326240539551 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03634977340698242 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03351736068725586 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03344416618347168 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.032613515853881836 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03456401824951172 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03773999214172363 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03761935234069824 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03034186363220215 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03000783920288086 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.026218175888061523 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03252291679382324 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02756047248840332 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03874707221984863 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.027092695236206055 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0403439998626709 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029314756393432617 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.029279470443725586 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.032317161560058594 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03466987609863281 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.026700258255004883 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.035471439361572266 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02531886100769043 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0422816276550293 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.025292634963989258 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9/.temp_path/tftransform_tmp/0117e378b60f43aabc9502fe41a3e651/assets\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03168654441833496 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03271746635437012 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03606128692626953 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0337069034576416 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.04513692855834961 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03224539756774902 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.029551029205322266 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0410916805267334 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.033275604248046875 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Artifact type Model is not found in MLMD.\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "E0627 10:03:43.758563826   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Processing /tmp/tmpx2xzqgy5/tfx_user_code_ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0-py3-none-any.whl\n",
      "Installing collected packages: tfx-user-code-ModelTrainer\n",
      "Successfully installed tfx-user-code-ModelTrainer-0.0+0817a36be2a09f88bec8068259a7f79eed49390e46fde3e5e1936c27f15f6fa0\n",
      "Runner started...\n",
      "fn_args: FnArgs(working_dir=None, train_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-train/*'], eval_files=['gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transformed_examples/9/Split-eval/*'], train_steps=None, eval_steps=None, schema_path='src/raw_schema/schema.pbtxt', schema_file='src/raw_schema/schema.pbtxt', transform_graph_path='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', transform_output='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9', data_accessor=DataAccessor(tf_dataset_factory=<function get_tf_dataset_factory_from_artifact.<locals>.dataset_factory at 0x7f5068710b90>, record_batch_factory=<function get_record_batch_factory_from_artifact.<locals>.record_batch_factory at 0x7f5068710cb0>, data_view_decode_fn=None), serving_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving', eval_model_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-TFMA', model_run_dir='gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model_run/10', base_model=None, hyperparameters={'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}, custom_config=None)\n",
      "\n",
      "Hyperparameter:\n",
      "{'num_epochs': 1, 'batch_size': 512, 'learning_rate': 0.001, 'hidden_units': [128, 128]}\n",
      "\n",
      "Runner executing trainer...\n",
      "Loading tft output from gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/DataTransformer/transform_graph/9\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Amount (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V1 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V10 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V11 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V12 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V13 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V14 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V15 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V16 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V17 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V18 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V19 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V2 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V20 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V21 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V22 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V23 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V24 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V25 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V26 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V27 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V28 (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V3 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V4 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V5 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V6 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V7 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V8 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " V9 (InputLayer)                [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 29)           0           ['Amount[0][0]',                 \n",
      "                                                                  'V1[0][0]',                     \n",
      "                                                                  'V10[0][0]',                    \n",
      "                                                                  'V11[0][0]',                    \n",
      "                                                                  'V12[0][0]',                    \n",
      "                                                                  'V13[0][0]',                    \n",
      "                                                                  'V14[0][0]',                    \n",
      "                                                                  'V15[0][0]',                    \n",
      "                                                                  'V16[0][0]',                    \n",
      "                                                                  'V17[0][0]',                    \n",
      "                                                                  'V18[0][0]',                    \n",
      "                                                                  'V19[0][0]',                    \n",
      "                                                                  'V2[0][0]',                     \n",
      "                                                                  'V20[0][0]',                    \n",
      "                                                                  'V21[0][0]',                    \n",
      "                                                                  'V22[0][0]',                    \n",
      "                                                                  'V23[0][0]',                    \n",
      "                                                                  'V24[0][0]',                    \n",
      "                                                                  'V25[0][0]',                    \n",
      "                                                                  'V26[0][0]',                    \n",
      "                                                                  'V27[0][0]',                    \n",
      "                                                                  'V28[0][0]',                    \n",
      "                                                                  'V3[0][0]',                     \n",
      "                                                                  'V4[0][0]',                     \n",
      "                                                                  'V5[0][0]',                     \n",
      "                                                                  'V6[0][0]',                     \n",
      "                                                                  'V7[0][0]',                     \n",
      "                                                                  'V8[0][0]',                     \n",
      "                                                                  'V9[0][0]']                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          3840        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            129         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,481\n",
      "Trainable params: 20,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model training started...\n",
      "      1/Unknown - 1s 931ms/step - loss: 0.6796 - accuracy: 0.7891Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6796 - accuracy: 0.7891\n",
      "Model training completed.\n",
      "Runner executing exporter...\n",
      "struct2tensor is not available.\n",
      "tensorflow_decision_forests is not available.\n",
      "tensorflow_text is not available.\n",
      "Model export started...\n",
      "Function `serve_features_fn` contains input name(s) Amount, V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26, V27, V28, V3, V4, V5, V6, V7, V8, V9 with unsupported characters which will be renamed to amount, v1, v10, v11, v12, v13, v14, v15, v16, v17, v18, v19, v2, v20, v21, v22, v23, v24, v25, v26, v27, v28, v3, v4, v5, v6, v7, v8, v9 in the SavedModel.\n",
      "Assets written to: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/model/10/Format-Serving/assets\n",
      "Model export completed.\n",
      "Runner completed.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelTrainer/.system/stateful_working_dir is not found, not going to delete it.\n",
      "E0627 10:04:03.179313287   16412 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f504332aa90> and <keras.engine.input_layer.InputLayer object at 0x7f5042bb2f10>).\n",
      "Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.030974626541137695 seconds.\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Sequence[typing.MutableMapping[str, typing.Any]]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f504117aa10> and <keras.engine.input_layer.InputLayer object at 0x7f502a6df9d0>).\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.MutableMapping[str, typing.Any]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Type[typing.Union[tensorflow_model_analysis.metrics.metric_types.MetricKey, tensorflow_model_analysis.metrics.metric_types.PlotKey, tensorflow_model_analysis.metrics.metric_types.AttributionsKey]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Using Any for unsupported type: typing.Callable[[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor, typing.Dict[str, typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]], typing.Dict[str, typing.Tuple[typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor], typing.Union[tensorflow.python.framework.ops.Tensor, tensorflow.python.framework.sparse_tensor.SparseTensor, tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor]]]]\n",
      "Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "Default Python SDK image for environment is apache/beam_python3.7_sdk:2.39.0\n",
      "==================== <function annotate_downstream_side_inputs at 0x7f5073c2ccb0> ====================\n",
      "==================== <function fix_side_input_pcoll_coders at 0x7f5073c2cdd0> ====================\n",
      "==================== <function pack_combiners at 0x7f5073c32320> ====================\n",
      "==================== <function lift_combiners at 0x7f5073c323b0> ====================\n",
      "==================== <function expand_sdf at 0x7f5073c32560> ====================\n",
      "==================== <function expand_gbk at 0x7f5073c325f0> ====================\n",
      "==================== <function sink_flattens at 0x7f5073c32710> ====================\n",
      "==================== <function greedily_fuse at 0x7f5073c327a0> ====================\n",
      "==================== <function read_to_impulse at 0x7f5073c32830> ====================\n",
      "==================== <function impulse_to_input at 0x7f5073c328c0> ====================\n",
      "==================== <function sort_stages at 0x7f5073c32b00> ====================\n",
      "==================== <function add_impulse_to_dangling_transforms at 0x7f5073c32c20> ====================\n",
      "==================== <function setup_timer_mapping at 0x7f5073c32a70> ====================\n",
      "==================== <function populate_data_channel_coders at 0x7f5073c32b90> ====================\n",
      "Creating state cache with size 100\n",
      "Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f502aee0e90> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.0287325382232666 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03641629219055176 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f502b6dec10> and <keras.engine.input_layer.InputLayer object at 0x7f502b7a4d90>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f50434c6790> and <keras.engine.input_layer.InputLayer object at 0x7f50434c66d0>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f502b6a4e90> and <keras.engine.input_layer.InputLayer object at 0x7f5070297190>).\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03005075454711914 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f502b06a550> and <keras.engine.input_layer.InputLayer object at 0x7f502a7ec490>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f502b61fd10> and <keras.engine.input_layer.InputLayer object at 0x7f502b1e4e10>).\n",
      "Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7f50403d3dd0> and <keras.engine.input_layer.InputLayer object at 0x7f5040472e50>).\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031069278717041016 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0335688591003418 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.02761697769165039 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.030623674392700195 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.031110048294067383 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03896641731262207 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.0323946475982666 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.03304004669189453 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 0 files in 0.03234982490539551 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "Starting the file information of the input\n",
      "Finished listing 1 files in 0.030157804489135742 seconds.\n",
      "Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "Renamed 1 shards in 0.30 seconds.\n",
      "From /opt/conda/lib/python3.7/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:109: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/ModelEvaluator/.system/stateful_working_dir is not found, not going to delete it.\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/GcsModelPusher/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Model registry location: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry/creditcards-classifier-v02/1656324271/\n",
      "Creating Model\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/188940921537/locations/europe-west4/models/815274677856370688/operations/642695882516463616\n",
      "Create Model backing LRO: projects/188940921537/locations/europe-west4/models/815274677856370688/operations/642695882516463616\n",
      "Model created. Resource name: projects/188940921537/locations/europe-west4/models/815274677856370688\n",
      "Model created. Resource name: projects/188940921537/locations/europe-west4/models/815274677856370688\n",
      "To use this Model in another session:\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/188940921537/locations/europe-west4/models/815274677856370688')\n",
      "model = aiplatform.Model('projects/188940921537/locations/europe-west4/models/815274677856370688')\n",
      "Model uploaded to Vertex AI: projects/188940921537/locations/europe-west4/models/815274677856370688\n",
      "stateful_working_dir /home/jupyter/mlops-with-vertex-ai/gs:/pbalm-cxb-aa-eu/creditcards/e2e_tests/tfx_artifacts/creditcards-classifier-v02-train-pipeline/VertexUploader/.system/stateful_working_dir is not found, not going to delete it.\n",
      "Model output: gs://pbalm-cxb-aa-eu/creditcards/e2e_tests/model_registry/creditcards-classifier-v02\n",
      "\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "../../../opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19\n",
      "  /opt/conda/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "    'nearest': pil_image.NEAREST,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "    'bilinear': pil_image.BILINEAR,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "    'bicubic': pil_image.BICUBIC,\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    if hasattr(pil_image, 'HAMMING'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:29: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "    _PIL_INTERPOLATION_METHODS['hamming'] = pil_image.HAMMING\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    if hasattr(pil_image, 'BOX'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:31: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "    _PIL_INTERPOLATION_METHODS['box'] = pil_image.BOX\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    if hasattr(pil_image, 'LANCZOS'):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34\n",
      "  /opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:34: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "    _PIL_INTERPOLATION_METHODS['lanczos'] = pil_image.LANCZOS\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485\n",
      "  /opt/conda/lib/python3.7/site-packages/jinja2/utils.py:485: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    from collections import MutableMapping\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318\n",
      "  /opt/conda/lib/python3.7/site-packages/jinja2/runtime.py:318: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    from collections import Mapping\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2471: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    temp_location = pcoll.pipeline.options.view_as(\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2473: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    job_name = pcoll.pipeline.options.view_as(GoogleCloudOptions).job_name\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2504: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "    | _PassThroughThenCleanup(files_to_remove_pcoll))\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py::test_e2e_pipeline\n",
      "  /opt/conda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m19 warnings\u001b[0m\u001b[33m in 431.56s (0:07:11)\u001b[0m\u001b[33m ==================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5704bcb",
   "metadata": {},
   "source": [
    "## 2. Run the training pipeline using Vertex Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a2fdb-5b66-4548-ac3c-3cae00e37ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_VERSION='tfx-1.8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7db74",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = DATAFLOW_REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"85000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"15000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"{REGION}-docker.pkg.dev/{PROJECT}/{DATASET_DISPLAY_NAME}/vertex:{IMG_VERSION}\"\n",
    "os.environ[\"ENABLE_CACHE\"] = \"1\"\n",
    "os.environ[\"SUBNETWORK\"] = DATAFLOW_SUBNETWORK\n",
    "os.environ[\"SERVICE_ACCOUNT\"] = DATAFLOW_SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ef31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3164f",
   "metadata": {},
   "source": [
    "### Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087da4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp build/Dockerfile.vertex Dockerfile\n",
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155568ca",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "pipeline_definition = runner.compile_training_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e506ad-e26d-4fd6-b235-e52302061ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "vertex_ai.init(project=PROJECT, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d0d6b-60cf-4bc4-8598-9a0f45acef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/compiled_pipelines/\"\n",
    "!gsutil cp {pipeline_definition_file} {PIPELINES_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb943e",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e625665-e0ac-4978-b820-561018e0adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "    \n",
    "job = pipeline_jobs.PipelineJob(template_path = pipeline_definition_file,\n",
    "                                display_name=DATASET_DISPLAY_NAME,\n",
    "                                parameter_values={\n",
    "                                    'learning_rate': 0.003,\n",
    "                                    'batch_size': 512,\n",
    "                                    'hidden_units': '128,128',\n",
    "                                    'num_epochs': 30,\n",
    "                                })\n",
    "\n",
    "job.run(sync=False, service_account=DATAFLOW_SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888be1fd",
   "metadata": {},
   "source": [
    "### Extracting pipeline runs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
    "pipeline_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b454fe9",
   "metadata": {},
   "source": [
    "## 3. Execute the pipeline deployment CI/CD steps in Cloud Build\n",
    "\n",
    "The CI/CD routine is defined in the [pipeline-deployment.yaml](pipeline-deployment.yaml) file, and consists of the following steps:\n",
    "1. Clone the repository to the build environment.\n",
    "2. Run unit tests.\n",
    "3. Run a local e2e test of the pipeline.\n",
    "4. Build the ML container image for pipeline steps.\n",
    "5. Compile the pipeline.\n",
    "6. Upload the pipeline to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29688d4d",
   "metadata": {},
   "source": [
    "### Build CI/CD container Image for Cloud Build\n",
    "\n",
    "This is the runtime environment where the steps of testing and deploying the pipeline will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc09c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $CICD_IMAGE_URI build/. --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9b2af",
   "metadata": {},
   "source": [
    "### Run CI/CD from pipeline deployment using Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b55593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPO_URL = \"https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai.git\" # Change to your github repo.\n",
    "REPO_URL=\"https://github.com/pbalm/mlops-with-vertex-ai.git\"\n",
    "\n",
    "BRANCH = \"main\"\n",
    "\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 1000\n",
    "CI_TEST_LIMIT = 100\n",
    "CI_UPLOAD_MODEL = 0\n",
    "CI_ACCURACY_THRESHOLD = -0.1 # again setting accuracy threshold to negative\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "VERSION = 'tfx-1.8'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINES_STORE = os.path.join(GCS_LOCATION, \"compiled_pipelines\")\n",
    "\n",
    "TFX_IMAGE_URI = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "#europe-west4-docker.pkg.dev/pbalm-cxb-aa/dataflow/creditcards@sha256:latest\n",
    "TFX_IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT}/{DATASET_DISPLAY_NAME}/vertex:{VERSION}\"\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={DATAFLOW_REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_BQ_TABLE_NAME={BQ_TABLE_NAME},\\\n",
    "_DATASET_DISPLAY_NAME={DATASET_DISPLAY_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_UPLOAD_MODEL={CI_UPLOAD_MODEL},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINES_STORE={PIPELINES_STORE},\\\n",
    "_SUBNETWORK={DATAFLOW_SUBNETWORK},\\\n",
    "_GCS_BUCKET={BUCKET}/cloudbuild,\\\n",
    "_SERVICE_ACCOUNT={DATAFLOW_SERVICE_ACCOUNT}\\\n",
    "\"\"\"\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ea6ba-0d59-439b-9117-e0fbffa1ca21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit --no-source --timeout=60m --config build/pipeline-deployment.yaml --substitutions {SUBSTITUTIONS} --machine-type=e2-highcpu-8"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
